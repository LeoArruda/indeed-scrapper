202008161	Toronto, ON	2020-08-16T23:46:34.000Z	The growing team here at Myticas is looking for an experienced Data Engineer who would be interested in a remote contract opportunity within the Ottawa, Toronto and Montreal regions.

Accountabilities:

Defining and reviewing security design requirements for cloud infrastructure and application components.
Evaluating architecture patterns from security perspective.
Building and implementing security controls to enable enforcement of compliance with Cloud Control Objectives, using custom Azure policies and integrated controls in DevOps processes

Requirements:

Strong Data Engineer w/ DevOps expertise + Azure Cloud Experience
Must know how to code and stand up scripts.
Experience with Data Digestions
Experience writing scripts to automate (infrastructure)
ARM Templating Expertise
Azure Synapse Expertise
Support developing automated DevOps processes and procedures for the following Azure components:
Azure Synapse (Azure DW) & Studio (private preview)
Azure Data Catalog Gen 2 (Babylon – private preview)
Azure Data Lake Storage Gen 2
Azure ML
ML Flow
Azure SQL Analysis Service
Azure Databricks
ADF data pipelines for data loading to AzSQL/Synapse
ADF data pipelines for connecting to on-prem data sources for data

Candidates looking to apply for this role are to send us an updated version of their resume in confidence. Our team will be sure to review all applicants and follow up accordingly at the conclusion of the review process.

Job is also known as: Data Engineer, DevOps Engineer, Azure Cloud Engineer, Azure Engineer, Cloud Engineer

INDMY	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Bt274jo0r_e0cCCqhskoQvohXG3bfDAo8rIdE-Msu3yuVfyF2oiDFKHFOU-vxVulAfe8kPL7riFwoXa1bsfN-szjxO_8qUQWbPsJOnTg0EkutIoK2S4NNLL24JQh9e4DsE9hv907k2_zDbvj0a8WAj5oPSV5J0YhjbTRwBfRwjCQxrc01uZ7J2e0_OHMexeIZTG8YrQagugaJ_D3lIHrWlHcf6PJnH2k8UvA-pc5cZpjw7ddqytNa624y9NXPvrifnTOAT7FAmk3UeLDuUfX1-skYbziNYR4Y7c5p3OsyfnSCh07UWqitBDNEYM6p9cLI4DHIttyCqjWLFYfggBXnYYpSWqsionve4WjKW6yzGyfBSYg7v03a4FQXFEw5Ge855e7c6Un29ITzsnzvdKuEGvQO5IKk7vCwyMZdafaIYiLhT-PX9FM4pxDgT63Kt6e9eG7HuTZTHAA==&p=0&fvj=0&vjs=3	BHJOB15656_15145 - Data Engineer w/ DevOps - Azure Cloud	a32b7c599156a218	Myticas Consulting
202008162	Data Engineer (Intermediate 4-6 years) - Contract - Remote during COVID	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Dkh866153otbRJ8nVMuEXnXPd315JS377__3VyD6HnNQkmcaOHMemmy5pLrNV6j0WaVWEWWabnp4PP6-2u77DLfT_FCIGvCW8CQmQW140Z7EVU32Jnwl42FALsxnbc4n5M6FOTfoW2OoTMOcGG21HZGOoXL7fWT8eaT67VlcClh4sHQcJsHCX_ihTEuHOc6UEzYYKVUb_jfygvE4ZvKl89B_GvxGLfO-e4Sjzx2m3ERv0ivnCOhgBcoB6iuW5gJOF67wyjAj6fnI5LNhBdZKlcNZr9gDWVp2CBrCJbFNa5TXaTjdfrwhPXDUebdRdjm-t23l_ayHtK7zEcrc_ULQNTq2n5ryptjnEOO6tblOw20JtnK10nBjyS14Uo0MzRkxgcjHnwx8EGDhwcGcfjoF2NpYj7tIDAPddz_vg7YOCiz_CRsgmdbM-bBpRW2KMYQJFoIXFDj_oo4RJGVd2lOj8ZtHlsJsOD1aygwi8tFB3b8uuV6ZcbgGM9&p=1&fvj=1&vjs=3	Role: Data Engineer (Intermediate 4-6 years)Structure: Contract (6 months initially)Location: North York, ONHours: Monday - Friday 40 hours (remote during COVID-19)Pay: $50.00 p/h inc.The role: We are looking for Intermediate Data Engineers who have roughly between 4-6 years experience who ideally has experience moving data from Google cloud over to Big query.However, if you are someone who has good experience with ETL and Data Pipeline Design & Data Validation experience and can script in Python and create strong SQL Queries then this will also be a potential match for this role.Ideal Qualifications to be successful in this role: - Degree in Computer Science, Engineering, Information Systems (or equivalent combination of skill and experience)- 4-6 years of experience working with data architecture projects- Excellent SQL coding and experience with a broad array of development tools and platforms including exposure to a big data environment tools/languages, such as SQL, R, SAS, Python, etc.- Experience creating relational database design and data models- Experience with various analytical data platforms and technologies- Experience with using REST API, Cloud (GCP Ideally)- Experience in custom or structured data integration design, implementation, and maintenance- Experience with business intelligence tools such as Qlikview, Tableau and Microstrategy (Nice to have for Data Visualizations)- Previous experience working in retail or eCommerce is highly preferred- Ability to curate data to tell stories and provide business insightsPlease apply with an updated resume and ensure the required skills you are able to speak to for this position are included.For more roles like this please go to www.corgta.com/find-a-job/Job Type: Full-timeSalary: $45.00-$50.00 per hour	CorGTA	2020-08-16T23:46:35.000Z	North York, ON	997673b8effa647f
202008163	Toronto, ON	TES - The Employment Solution	05f7bf2a7458c7fc	2020-08-16T23:46:35.000Z	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DQQnUBuQBSuyaIQhpC59TW7hrTbBg8v-nGtzzV8dunbQHh9VTp-k89gJ8q3B9UEXLHcYbAG67d8jpOK4E3ok8olcOhfyyiChWMI3BPvUFdra3ljwgLS_vFsJXlW6jHt2NpQMzd2p9F6BXGm9ND3feUN42yGy4tVXDAHqnNfhnu70vY-acBciq67Ai6GdSD3PlQ13tg4qFrRiLpqIcz_-jDIa5Nq63B9wZ628cpPKY6dro-rYYuaRQsjRboGg5ghzntSbAnB9ZJ1QNRhNiUeaOyL7c2R4WqqvxXeA3YX0iHxxqG9n63eRdXvRlJQAPR3H3B0HskcFu92t8odPo8Qz2YfC_ifUv0n3nJs5aT9AHKgas7G2cM4YosR62pRkMoR-DKKJvwJo1sAis2WZlVzleg_3Al5gCGbP_iZj94awD8kZbyV4ZrXI89TOCG-iZhzVZ6HnByoD19pPqLYrcrPxpa47CX3Au24RjLYWrQuTMIWq_nR83byXNtb3eOKuxSO40Nn_nf4SwugQHe1fSMFFbDOIsZyH_GUp5MEGdYNtE9o4WkAxcXzvAtwfNyZN1whoXc9RPLNr7z5VFAy5J5LEjrBOL_9mHEecWAMM2Trsu7RlUK81Gz4_Oe3WkCO26f2t9jq8HU877w686HNDrNQ923RPg17weNBQY504sjGAq150H88GjkY40ZGtkWimoifDeV39XfTSJWDOJcaIHeaXc3c8-8p-fksONw-x2fILu8lbit4oXVdsxA95PrBiD3K8n0K34AGFtjE_1K5icfaXrUV_r6rVxXUMlPFsREmjAQCjp_wu1P9Ba_MjaDdtndXPATNQL5GIuAqOAV5RrcvdgM&p=2&fvj=0&vjs=3	Job Title- Data Engineer
Location- Downtown, Toronto
Type - Full Time Permanent
Salary - Negotiable + Benefits



Focus on data architecture, best practices, reliability, security, and complianceImprove and extend ETL, data processing, and analytics processesFacility with PowerBI, including creating dashboards and data sourcesDeveloping high complexity, fast performing SELECT queries.Developing T-SQL procedures, functions, triggers, jobs, scripts, etc.Development of Advanced T-SQL such as temporal tables, PIVOTs, recursive table expressions and more.Modeling and implementing Data Mart solution for Power BI analytics
Managing indexes, statistics, query plans alerts, database activity, and overall performance activity.In-depth experience working with relational databases, such as Microsoft SQL Server or PostgreSQLEnthusiasm for applying good data design, testing, documentation, and support practicesExperience building and optimizing data pipelines, architectures, and data setsKnowledge of message queueing, stream processing, and data stores/warehousesWorking knowledge of AWS products related to data engineeringBachelor's degree in Computer Science, Software Engineering or an equivalent
Excellent communication skills - both written and verbal; ability to speak in Spanish is a bonus

To apply please send an email to sheetalk@tes.net	Data Engineer
202008164	Toronto, ON	There is never a typical day at Accenture, but that’s why we love it here! This is an extraordinary chance to begin a rewarding career at Accenture Technology. Immersed in a digitally compassionate and innovation-led environment, here is where you can help top clients shift to the New using leading-edge technologies on the most ground-breaking projects imaginable.

Interested in building end-to-end marketing solutions for clients? Bring your talent and join Data which operates in the Interactive, Mobility and Analytics space. You will have opportunities to get involved in digital marketing, eCommerce and end-to-end mobility capabilities to help clients to improve productivity and more!

WORK YOU’LL DO
Work across the Service Delivery Lifecycle to analyze, design, build, test, implement and/or maintain multiple system components or applications for Accenture or our clients
Responsible for the maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases
Support our database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects
Determines methods and procedures on new assignments with guidance
Manages small teams and/or work efforts (if in an individual contributor role) at a client or within Accenture

WHO WE´RE LOOKING FOR?
Minimum 5 years of experience as a Data Engineer
Must have experience with one of the Cloud Technologies (Azure or AWS)
Azure cloud includes Spark, Python, Databricks, Synapse, Snowflake, Data Factory and ADLS
AWS cloud includes Glue, EC2, EMR, Athena, redshift, Snowflake, S3, Spark, Python and Databricks
Experience with Big Data technologies like MapReduce, Pig, Hive, HBase, Sqoop, Flume, YARN, Kafka, Storm and etc.
2+ years of experience with at least one SQL language such as T-SQL or PL/SQL
2+ years of work experience with ETL and data modeling
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets
Experience in both batch and stream processing technologies
Experience with object-oriented/object function scripting languages: Java, C++, Scala
Machine learning experience with Spark or similar
Professional Skills Qualifications:
Proven success in contributing to a team-oriented environment.
Proven ability to work creatively in a problem-solving environment.
Desire to work in an information systems environment.
Demonstrated teamwork and collaboration in professional setting; either military or civilian.
WHAT´S IN IT FOR YOU?
Competitive benefits, including a fair and balanced parental leave policy.
Fantastic opportunities to develop your career across industries with local and global clients.
Performance achievement and career mentorship: our performance management process focuses on your strengths, progress and career possibilities.
Opportunities to get involved in corporate citizenship initiatives, from volunteering to charity work.
To learn more about Accenture, and how you will be challenged and inspired from Day 1, please visit our website at accenture.ca/careers.
It is currently our objective to assign our people to work near where they live. However, given the nature of our business and our need to serve our clients, our employees must be available to travel when needed.

Accenture does not discriminate on the basis of race, religion, color, sex, age, non-disqualifying physical or mental disability, national origin, sexual orientation, gender identity or expression, or any other basis covered by local law. Accenture is committed to providing employment opportunities to current or former members of the armed forces.

We are committed to employment equity. We encourage all people, including women, visible minorities, persons with disabilities and persons of aboriginal descent to apply.

Accenture is a leading global professional services company, providing a broad range of services and solutions in strategy, consulting, digital, technology and operations. Combining unmatched experience and specialized skills across more than 40 industries and all business functions — underpinned by the world’s largest delivery network — Accenture works at the intersection of business and technology to help clients improve their performance and create sustainable value for their stakeholders. With 469,000 people serving clients in more than 120 countries, Accenture drives innovation to improve the way the world works and lives. Visit us at www.accenture.com	fbcc383eeed7b6dc	https://ca.indeed.com/rc/clk?jk=fbcc383eeed7b6dc&fccid=a4e4e2eaf26690c9&vjs=3	2020-08-16T23:46:36.000Z	Accenture	Data Engineer
202008165	Toronto, ON	2020-08-16T23:46:37.000Z	Data Engineer, GEMINI (Contract)	25f6a9d8e47d869e	Vector Institute	https://ca.indeed.com/rc/clk?jk=25f6a9d8e47d869e&fccid=1a9a6d236cf89e9e&vjs=3	POSITION OVERVIEW


The Vector Institute is seeking a Data Engineer, GEMINI to join our team in Toronto. This role will predominantly work with the General Medicine Inpatient Initiative (GEMINI) team at St. Michael’s Hospital, in addition to providing support to the Vector community.


The Data Engineer, GEMINI will primarily work in R to lead a team to automate and optimize GEMINI’s data pipeline workflow including extracting, transforming and loading data, conducting quality and validation checks, and standardizing data from multiple data sources. The ideal candidate will have excellent programming skills, a strong understanding of data pipelines and analytic methods, an aptitude for data visualization, and strong leadership and communication skills. You will be joining a dynamic and mission-driven team of clinicians, scientists, and quality improvement experts.
EMPLOYMENT TYPE


Contract (18 months)


ABOUT THE VECTOR INSTITUTE


A thriving, independent not-for-profit, the Vector Institute strives to advance the Artificial Intelligence ecosystem in Ontario, developing and attracting the world’s best machine learning and deep learning experts, and creating an unrivalled convergence of research, investment, entrepreneurialism, and economic growth. Located in the MaRS Discovery District in downtown Toronto, we are part of a dynamic and vibrant community of research, academia, health science and commerce.


ABOUT GEMINI


Co-led by Drs. Fahad Razak and Amol Verma, GEMINI has developed methods to extract and standardize data from electronic health records to harness the tremendous potential of data generated through routine patient care of General Internal Medicine hospital inpatients for research and quality improvement purposes. GEMINI is a unique data platform in the Canadian healthcare landscape and currently exists at 7 hospitals, with data collected on 345,000+ patient visits, including billions of data points. The GEMINI data platform has recently been funded to expand to the 30 largest hospitals in Ontario to support data analytics that inform the COVID-19 pandemic response. GEMINI supports a network of nearly 100 collaborating scientists and more than 40 students, including clinicians, computer scientists, biostatisticians, epidemiologists, social scientists, and engineers.
RESPONSIBILITIES


Develop scripts to extract, transform and load data from multiple data sources into GEMINI’s platform;

Gather requirements and conduct analyses to design, develop and maintain secure data pipelines;
Automate and optimize GEMINI’s data pipeline workflows;
Provide leadership and guidance to data pipeline team;
Maintain and update on-going quality assurance of data workflows;
Engages with team and collaborators to understand needs and requirements;
Documents data pipeline architecture;
Routinely tests and monitors system for failures, errors, breaches;
Troubleshoot problems;
Provides teaching and training as needed;
Create and update Standard Operating Procedures and other documentation files, as needed;
Establish methods to improve and automate data workflow of rapidly changing COVID-19-related data;
Support GEMINI’s data platform as required; and,
Perform other functions as required (e.g., providing scientific advice and support to other members Vector’s teams and corporate projects led by Vector).


SUCCESS MEASURES


GEMINI’s data pipeline workflow is automated and optimized through the development of algorithms and procedures.
High quality coding practices are maintained.
Significant contributions are made towards GEMINI’s data platform.


PROFILE OF IDEAL CANDIDATE


A degree in Engineering, Computer Science and/or related discipline; graduate degree preferred
At least 5 years’ relevant professional experience
Fully knowledgeable in designing and testing of data pipelines required
Extensive experience in SQL as well as at least one of either R or Python required
Familiarity of key databases such as: PostgreSQL, MySQL, Oracle, etc. is preferred
Knowledge of Linux commands and Shell scripts required
Strong analytical, technical design and problem-solving skills required
Strong working knowledge of Microsoft Office products (e.g. Outlook, Word, Excel, PowerPoint) required
Experience with GitHub preferred
Good judgement and understanding of what issues to escalate, resolve on your own, making suggestions for possible resolution required
Ability to learn new technology expediently required
Demonstrated success working within interdisciplinary teams
Experience with the Ontario healthcare system is an asset
Excellent attention to detail and proven ability to learn new skills
Experience working independently and as part of a team
Excellent organizational skills to manage multiple tasks in a timely manner
Demonstrated flexibility and have the ability to adapt and manage changing priorities


Please address applications (cover letter and resume) to Kailyn Burke, HR Generalist, using the link provided. Review of applications will begin August 19, 2020. We thank all applicants for their interest in this exciting opportunity and will be in touch with those whose qualifications most closely match with our needs. Please note that candidates may be required to demonstrate proficiency in R.

The Vector Institute is committed to employment equity and diversity in the workplace and welcomes applications from women, racialized persons/visible minorities, Indigenous peoples, persons with disabilities, and LGBTQ+ persons. All qualified candidates are encouraged to apply.


Further, we are committed to fostering an environment of inclusivity and accessibility. If you require an accommodation at any point throughout the recruitment and selection process, please

contact hr@vectorinstitute.ai and we will happily work with you to meet your needs.
202008166	Fluid Hose & Coupling	Junior Data Engineer	7cc1b932ac3ee1bf	Mississauga, ON	Job summaryResponsible for the completion of long-term IT engineering projects. Performs engineering design evaluations and works to complete projects within budget and scheduling restraints. Develops, implements, and monitors information systems policies and controls to ensure data accuracy, security, and regulatory compliance.Responsibilities· Responsible for the analysis, improvement, cleaning, and manipulation of legacy relational databases at the company.· Responsible for the architectural design and development of a modernized database, integrated with legacy systems.· Responsible for the full-stack development of web applications which enhance customer and employee relationships with company data.· Responsible for general development and maintenance of the company's IT systems, including web server maintenance and website updates.· Works with the business’s leadership team, in order to aid in the implementation of database requirements and troubleshoot any existent issues.· Defines and builds the data pipelines that will enable faster, better, data-informed decision-making within the business.· Manages Salesforce database base as required by sales leadershipEducation and Skill Requirement· Computer Programming degree or equivalent. Equivalence may be demonstrated from past projects, results, etc· Computer Programming/Cloud Computing/Data Science/Data Engineer/Database Management/IT Security and Infrastructure· Ability to learn new technologies unguided/with minimal intervention (Adaptability)· Complete knowledge of Web programming, design, e-commerce, SQL and relational DB Design. Understands data structures and hierarchies (1-2+ years of experience with SQL, or any other database management system)· Clear understanding of System Architecture and ERP systems· Detail oriented, organized with strong analytical and critical thinking skills· Excellent communication skills combined with patience in the process· Excellent Excel and MS Office skills with a deep understanding of Excel macros and programming. May be required to demonstrate this in a remote interview by example· Quick learner and familiar with CRUD Operations· Resourcefulness to keep moving forward (does not stall at a challenge)· Preference will be given to those who understand App design and machine learningJob Types: Full-time, PermanentSalary: $40,000.00-$40,001.00 per yearBenefits:Dental CareExtended Health CareOn-site ParkingSchedule:8 Hour ShiftMonday to FridayEducation:Bachelor's Degree (Required)Work remotely:No	2020-08-16T23:46:38.000Z	https://ca.indeed.com/company/FLUID-HOSE-&-COUPLING-INC/jobs/Junior-Data-Engineer-7cc1b932ac3ee1bf?fccid=5156f9b5283a3233&vjs=3
202008167	Toronto, ON	https://ca.indeed.com/rc/clk?jk=6e7b18ed4b257445&fccid=e981b35e4c0452a3&vjs=3	6e7b18ed4b257445	CI Investments Inc	2020-08-16T23:46:38.000Z	ABOUT US
CI Investments Inc. is one of the country’s largest investment fund companies. CI is known for its innovation and ability to adapt quickly to the changing needs of Canadian investors. It provides employees with a fast-paced and challenging work environment with opportunities for advancement. CI is part of CI Financial, a diverse group of financial services firms.
POSITION: Data Engineer
LOCATION: Toronto (M5J 0A3)
STATUS: Contract (6 months with extension possibility)
JOB OVERVIEW
We are currently seeking a Data Engineer to join our Client Reporting and Data Management team. The successful candidate will work closely with our data science team on the development of our centralized predictive analytics function. In this role, you will assist with solving high-value business problems by extracting and manipulating large, complex datasets for use by data scientists. The role will be a six-month contract position, with an option to extend based on performance.
WHAT YOU WILL DO
Collaborate with business analysts, data scientists, software engineers, and solution architects to develop data pipelines to feed our data marketplace
Extract, analyze & interpret large, complex datasets for use in predictive modelling
Utilize AWS tools to develop automated, productionized data pipelines
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Develop and support ETL code for data warehouse and data marts to support the reporting and data analytic systems.
WHAT YOU WILL BRING
At least two years of work experience in quantitative analysis
Post-secondary degree in a quantitative discipline
Experience with large-scale, AWS big data storage such as S3 and EBS
Experience creating ETL jobs using AWS Glue or Talend
Experience with AWS Data pipeline tools like Cloudwatch and Stepfunctions
Experience working with data preparation tools like Talend
Experience in the Financial Services Industry is an asset
Strong knowledge with programming methodologies (version control, testing, QA) and agile development methodologies.
In-depth knowledge of AWS tools required to develop automated, productionized data pipelines
In depth knowledge of and experience with relational, SQL and NoSQL databases
Fluency with SQL and Python
Experience working with large, complex datasets
Excellent communication, writing and interpersonal skills
WHAT YOU CAN EXPECT FROM US
Our dedication to the Employee Experience at CI is aimed at supporting, empowering and inspiring our talented team through:
Recognition & Compensation
Training & Development
Health & Well-being
Communication & Feedback
If you are a passionate, committed and dynamic individual, please submit your resume in confidence by clicking “Apply”.
Only qualified candidates selected for an interview will be contacted.
CI Financial Corp. and all of our affiliates (“CI”) are committed to fair and accessible employment practices and we are committed to providing accommodations for persons with disabilities. If you require accommodations in order to apply for any job opportunities, or require this posting in an additional format, please contact us at accessible.recruitment@ci.com, or call 416-364-1145 ext. 4747. If you are contacted by CI regarding a job opportunity or testing and require accommodation in any stage of the recruitment process, please use the above contact information. We will work with all applicants to determine appropriate accommodation for individual accessibility needs.	Data Engineer
202008168	Toronto, ON	NLB Services Inc	Title: Data EngineerLocation: Toronto, CADuration: 6+ monthsExpertise in the design, creation, management, and business use of large datasets, across AWS Data and Analytics product.Excellent business and interpersonal skills to be able to work with business owners to understand data requirements, and to build the required data pipelines.Crafting, implementing, and operating stable, scalable, low cost solutions data pipelines to ingest real-time and event-based data using AWS technologies.5+ years of work experience with Data Pipelines, Data Modelling, and Data Architecture. ·Expert-level skills in writing and optimizing SQL.Proficiency in python scripting languages and integrating it with lambda-based processing in AWSExperience operating very large data warehouses or data lakes.Experience with building data pipelines and applications to stream and process datasets at low latencies. ·Demonstrate efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data. ·Sound knowledge of distributed systems and data architecture (lambda)- design and implement batch and stream data processing pipelines using KinesisGood understanding of different DB technologies like Dynamo DB (NoSQL),Aurora DB ( RDBMS), Redshift (In-Memory DBs), across AWS stack.Job Types: Full-time, ContractSalary: $70.00-$75.00 per hourExperience:AWS: 1 year (Preferred)Work remotely:Temporarily due to COVID-19	2020-08-16T23:46:39.000Z	904ae8a46b296ad2	https://ca.indeed.com/company/NLB-Services-Inc/jobs/Data-Engineer-904ae8a46b296ad2?fccid=0b209b5526418580&vjs=3	Data Engineer
202008169	Toronto, ON	3+ years of experience as a Data Engineer or in a similar roleExperience with data modeling, data warehousing, and building ETL pipelinesExperience in SQL

Amazon.com was recently voted #2 most admired company in the US, #1 most innovative, and # 1 in Customer Service. We are investing heavily in building an excellent advertising business, and are responsible for defining, and delivering a collection of self-service performance advertising products – “always-on analytics” that is fully scalable and reliable. Our products are strategically important to our leadership, finance, economists, analysts, and BI partners to drive long-term growth. We mine billions of ad impressions and millions of clicks daily and are breaking fresh ground to create world-class products. We are highly motivated, collaborative, and fun loving with an entrepreneurial spirit and bias for action. With a broad mandate to experiment and innovate, we are growing at an unprecedented rate with a seemingly endless range of new opportunities.
The Advertising Analytics and Data Management team is looking for an exceptional Data Engineer who is passionate about data and the insights that large amounts of data can provide, who thinks/acts globally, and who has the ability to contribute to major novel innovations in the industry. The role will focus on working with a team of data engineers, business and tech savvy professionals to lay down scalable data architecture to ingest large amounts of structured and unstructured datasets and work with stakeholders to drive business decisions based on these datasets.

The ideal candidate will possess both a data engineering background and a strong business acumen that enables him/her to think strategically and add value to the customer experience. He/She will experience a wide range of problem solving situations, requiring extensive use of data collection and analysis techniques such as data mining and machine learning.

The successful candidate will work with multiple global site leaders, Business Analysts, Software Developers, Database Engineers, Product Management in addition to stakeholders in sales, finance, marketing and service teams to create a coherent customer view. They will:

Develop and improve the current data architecture using AWS Redshift, AWS S3, AWS Aurora (Postgres) and Hadoop/EMR.Improve upon the data ingestion models, ETL jobs, and alarming to maintain data integrity and data availability.Stay up-to-date with advances in data persistence and big data technologies and run pilots to design the data architecture to scale with the increased data sets of advertiser experience.Partner with analysts across teams such as product management, operations, sales, finance, marketing and engineering to build and verify hypothesis to improve the business performance.Manage weekly business reports via dashboards and paper the analyses of daily, weekly, and monthly reporting of performance via Key Performance Indicators.

AWS ExperienceAdvertising domain knowledge is a plus
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.

#sspajobs	AMZN CAN Fulfillment Svcs, ULC	https://ca.indeed.com/rc/clk?jk=59c48782b60cacd0&fccid=fe2d21eef233e94a&vjs=3	59c48782b60cacd0	2020-08-16T23:46:40.000Z	Data Engineer
2020081610	Toronto, ON	Data Engineer - Datamart Azure	https://ca.indeed.com/company/ARISOFT-INC./jobs/Data-Engineer-76d86fb24f1bdc31?fccid=1dbce7609f8e0b52&vjs=3	Data EngineerRequirements: · 8+ years of hands-on development experience in multiple projects, with progressively increasing responsibility and ETL background, and an understanding of type 2 dimension, and prior experience with data marts.· 5+ years of hands-on experience working with data warehousing like applications and big data.· Experience leveraging big data technologies (One or more of Hadoop, Python, Spark) is required.· Exposure to Microsoft Azure (or other cloud) platforms is preferred· Experience working with various data exchange formats (JSON, CSV, XML etc.)· Solid understanding of relational and dimensional database design and knowledge of logical and physical data models is preferred· Experience with SDLC and/or Agile methodologies for project development, and participation in all phases of project development, is required· Excellent knowledge of SQL and Linux shell scripting· Experience in deploying and managing SQL and NoSQL databases is preferred· Experience with job scheduling (TIDAL, CAWLA, Oozie) and file transfer (e.g. SFTP)· Excellent diagnostic, analytical and problem-solving skills are preferred· Experience with continuous delivery tools (Jenkins, Bamboo, Circle CI), and an understanding of the principles and pragmatics for build pipelines, artefact repositories, zero-downtime deployment, etc. is preferred· Experience building real-time data pipelines using Kafka or spark streaming is preferredKey Accountabilities: · Perform technical systems and data flow development in a variety of projects for complex front, middle and back office applications, with a focus on the reporting and analytics environment; this environment is built on a Microsoft Azure cloud and is underpinned by a Hortonworks Hadoop stack· Perform technical systems and data flow design for small-to-medium sized projects· Work with multiple project execution and deployment teams (e.g. Development Architecture, Release Management, Production Support)· Work closely with source system SMEs to produce source to target mappings· Translate business requirements to technical specifications· Work closely with the technical leads and architecture teams, and align solutions that meet the departmental architectural vision· Able to handle multiple priorities seamlesslyJob Types: Full-time, ContractSalary: $89,471.00-$170,126.00 per yearExperience:total IT: 10 years (Required)dartmart: 2 years (Required)Data engineer: 10 years (Required)Azure: 3 years (Required)Big data: 3 years (Required)ETL: 5 years (Required)Work remotely:Temporarily due to COVID-19	76d86fb24f1bdc31	ARISOFT INC.	2020-08-16T23:46:40.000Z
2020081611	Toronto, ON	874eae48579a7736	Rackspace	Big Data Engineer	As a full spectrum AWS integrator, we assist hundreds of companies to realize the value, efficiency, and productivity of the cloud. We take customers on their journey to enable, operate, and innovate using cloud technologies – from migration strategy to operational excellence and immersive transformation.

If you like a challenge, you’ll love it here, because we’re solving complex business problems every day, building and promoting great technology solutions that impact our customers’ success. The best part is, we’re committed to you and your growth, both professionally and personally.

Overview

Our Big Data Engineers are experienced technologists with technical depth and breadth, along with strong interpersonal skills. In this role, you will work directly with customers and our team to help enable innovation through continuous, hands-on, deployment across technology stacks. You will work to build data pipelines and by developing data engineering code ( as well as writing complex data queries and algorithms.

If you get a thrill working with cutting-edge technology and love to help solve customers’ problems, we’d love to hear from you. It’s time to rethink the possible. Are you ready?
What You’ll Be Doing:
Build complex ETL code
Build complex SQL queries using MongoDB, Oracle, SQL Server, MariaDB, MySQL
Work on Data and Analytics Tools in the Cloud
Develop code using Python, Scala, R languages
Work with technologies such as Spark, Hadoop, Kafka, etc.
Build complex Data Engineering workflows
Create complex data solutions and build data pipelines
Establish credibility and build impactful relationships with our customers to enable them to be cloud advocates
Capture and share industry best practices amongst the community
Attend and present valuable information at Industry Events
Traveling up to 50% of the time
Qualifications & Experience:
3+ years design & implementation experience with distributed applications
2+ years of experience in database architectures and data pipeline development
Demonstrated knowledge of software development tools and methodologies
Presentation skills with a high degree of comfort speaking with executives, IT management, and developers
Excellent communication skills with an ability to right level conversations
Technical degree required; Computer Science or Math background desired
Demonstrated ability to adapt to new technologies and learn quickly


About Rackspace Technology
We are the multicloud solutions experts. We combine our expertise with the world’s leading technologies — across applications, data and security — to deliver end-to-end solutions. We have a proven record of advising customers based on their business challenges, designing solutions that scale, building and managing those solutions, and optimizing returns into the future. Named a best place to work, year after year according to Fortune, Forbes and Glassdoor, we attract and develop world-class talent. Join us on our mission to embrace technology, empower customers and deliver the future.


More on Rackspace Technology
Though we’re all different, Rackers thrive through our connection to a central goal: to be a valued member of a winning team on an inspiring mission. We bring our whole selves to work every day. And we embrace the notion that unique perspectives fuel innovation and enable us to best serve our customers and communities around the globe. We welcome you to apply today and want you to know that we are committed to offering equal employment opportunity without regard to age, color, disability, gender reassignment or identity or expression, genetic information, marital or civil partner status, pregnancy or maternity status, military or veteran status, nationality, ethnic or national origin, race, religion or belief, sexual orientation, or any legally protected characteristic. If you have a disability or special need that requires accommodation, please let us know.	https://ca.indeed.com/rc/clk?jk=874eae48579a7736&fccid=b60c9324aeb7df96&vjs=3	2020-08-16T23:46:41.000Z
2020081612	Toronto, ON	2020-08-16T23:46:42.000Z	Company Description

The Company
Hitachi Solutions is an impact-driven global leader in consulting dedicated to delivering competitive, end-to-end solutions based on the Microsoft Cloud. Our deeply connected teams are unified by our values and our commitment to helping clients succeed and compete with the largest global enterprises. We are a division of the 38th largest company in the world and carry the strength of a vast network of interconnected Hitachi companies all while remaining nimble, agile, and ready to pivot at a moment’s notice.
Hitachi Solutions started as three founding partners and transformed into nearly 2,000 consultants, developers, and support personnel all around the globe. With 36 Microsoft Partner of the Year awards, Hitachi Solutions has established itself as a leading partner in the ever-growing landscape of technology consulting.
The Culture
Our team is a mix of curious, fun, and get it done. We celebrate collaborative thought leadership and individual talents so that our ideas are translated into real-world results.
Each day, our team members show up to work as a blank slate with the sole purpose of coloring their canvases with knowledge. We are a complex group of resilient learners and encourage one another to go beyond the limits of conventional expectations. As a member of the dynamic Hitachi Solutions team, you will be challenged, pushed, and unconditionally supported in your efforts to drive the business forward.
** THIS ROLE IS OPEN TO ANY CANDIDATES FROM WITHIN CANADA**

Job Description

As an Analytics Data Engineer for Hitachi’s Azure Cloud Enablement Team you will be responsible to deliver high quality modern data solutions while being part of a dynamic and fast-growing team consisting of endless opportunities.
The successful candidate will be a self-motivated, passionate individual who strives to be the best at what he/she does and creates a trail of ecstatic Customers for life. This person will love to learn, contribute to the team and wants to be part of something great.
Knowledge and Experience
Hands-on experience with the Azure Data Platform (Data Factory, Data Lake, Data Warehouse, Blob Storage, SQL DB, Analysis Services)
Data quality (profiling, cleansing, enriching)
Data Modeling – including design from conceptual to logical to physical data models
Considered to be an expert in T-SQL
Hands-on experience with MPP database technologies such as Azure SQL DW, Teradata Netezza, etc.
Experience with multiple components listed, required:
Power BI including DAX
Database migration from legacy systems to new solutions
DevOps
Interpreted languages (i.e. python, C-sharp, Java, Scala, etc.)
Databricks
LogicApps
PowerApps
HDInsight
D365FO / CE experience as it pertains to data extraction
Knowledge of Cap Theorum and Distributed Database Management Systems, iis considered an asset.
Opportunity for a career path into a Data Scientist role if desired

Qualifications

Required skills / qualifications
Proven ability to engage customers to understand customer challenges and needs to develop technical solutions
3+ years of hands on experience working with the Azure Platform and its relevant components
Proven experience of architecting Azure services into a solution platform on Microsoft Azure for Analytics
Minimum 5 years hands on development experience with ELT/ETL using MS SQL Server, Oracle or similar RDBMS Platform
Familiarity with data visualization tools (e.g. PowerBI, Tableau etc.)
Experience or desire to coach, mentor and provide leadership to team members
Post-secondary degree/diploma in Business, Computer Science or a related discipline;
Strong communication skills, both written and verbal
Prepared for domestic and US travel as required
Preferred considered an asset, NOT required:
Project management experience
Databricks and Spark SQL
Previous Consulting experience
Additional Information

Opportunity Benefits:
Medical and Dental Benefit Package (including Long Term and Short Term Disability)Base salary plus targeted bonus package
This position can be based anywhere in Canada, though travel might be required.	Hitachi Solutions	24716d17b181ff38	Intermediate Data Engineer	https://ca.indeed.com/rc/clk?jk=24716d17b181ff38&fccid=28f79c18789111e8&vjs=3
2020081613	Toronto, ON	2020-08-16T23:46:42.000Z	https://ca.indeed.com/company/Smarttechlink-technologies/jobs/Aws-Data-Engineer-91168fc331947e74?fccid=7be406286ca6842e&vjs=3	SmartTechlink Sollutions Inc.	91168fc331947e74	Job Description: AWS Data engineer with 3-5 years work experience using Python, PySpark, AWS EMR and Airflow. The desired candidate should have strong development skills and experience on datalake implementation including data extraction and building data pipeline.Job Type: Full-timePay: $94,832.00-$120,000.00 per yearSchedule:Monday to FridayExperience:AWS Data: 3 years (Preferred)	AWS Data Engineer
2020081614	goeasy	2020-08-16T23:46:43.000Z	If you are looking to join one of Canada’s fastest growing companies, goeasy Ltd. is the place for you! Recognized as one of Canada’s Most Admired Corporate Cultures, one of Canada’s Top 50 Fintech’s and one of North America’s Most Engaged Workplaces, we want the best and brightest to join our team.

We are a publicly traded company on the TSX with over 4000% shareholder return since 2001, goeasy operates two main business units. easyfinancial is our consumer lending business that offers secured and unsecured installment loans of up to $35,000 and easyhome is Canada’s largest merchandise lease-to-own company. It is our mission to provide everyday Canadians the chance for a better tomorrow, today by giving them access to the credit they need and by offering them a second chance when they have been turned down by banks and traditional lenders. With a retail network of nearly 400 locations across Canada and over 1900 employees, we are able to build lasting relationships with our customers as we help them rebuild their credit and graduate towards prime rates and a brighter financial future.

The Data Engineer on the Data Science and Business Insights team will build, integrate data from various resources, manage data in goeasy operational data store and enterprise data warehouse. This position will develop ETL (Extract, Transform and Load) with various tools on large datasets to ensure data is easily accessible, works smoothly, as well as maintain and expand the data warehouse for reporting and analysis. The Data Engineer will also work closely with the data architect on the design and architecture of our enterprise data warehouse.

Responsibilities:

Develop data set processes for data modeling, mining and production
Develop and maintain ETL processes using SSIS, Scripting and data replication technologies
Participate in development of datamarts for reports and data visualization solutions
Research opportunities for data acquisition and new uses for existing data
Integrate new data management technologies and software engineering tools into existing structures
Support the translation of business requirements for data acquisition/manipulation and provide detailed specifications that can be passed downstream for use
Develop detailed technical specifications and operational support documentation in collaboration with Business Systems Analysts, BI Engineers and Architects.
Identify and communicate technical problems, process and solutions
Create Ad-Hoc queries and reports as needed along with providing on-going analytical support for these requests
Assist in the collection and documentation of user’s requirements
Ensure that existing business processes dependent on the ODS/EDW are monitored and respond quickly to bug fixes, enhancement requests and production ETL related issues.
Dealing with the database users on a daily basis to ensure that problems are dealt with promptly and that appropriate fixes are made to resolve any problems.
Recommend ways to improve data reliability, efficiency and quality
Ensure systems meet business requirements and industry practices
Work effectively with the Business Intelligence and Data Solutions Architects, Data and BI Engineers to ensure that all approved development and deployment procedures are followed.

Qualifications:

Bachelor’s Degree in Computer Science, MIS, Computer Engineering or other Information Technology related degree
4+ years working with SQL Server or comparable relational database system
3+ years of extensive ETL development experience with SSIS and/or ADF
4+ years of experience troubleshooting within a Data Warehouse environment
Expert domain knowledge & experience in Data warehousing, encompassing data model design, dimensional modeling, naming conventions, cross-cutting concerns, common integration technologies, patterns & standards and emerging technologies.
Expert Knowledge of SQL skills to build, debug, and optimize (developing procedures, functions, SQL queries, etc.) and working with large data sets and to perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
2+ years SQL Server Database administration experience
Cloud experience (Azure) is highly preferred
Exposure and experience with Python, R, Hadoop, Azure, other Big data and advanced analytics
Knowledge of AI and ML developments/solutions/implementations
Experience with multiple programming languages (PowerShell scripting, C#, others) with basic scripting skills.
High level of technical aptitude

Inclusion and Equal Opportunity Employment

goeasy is an equal opportunity employer. In addition, goeasy is committed to providing accommodations for applicants upon request at any stage of the recruitment process in accordance with all legislative requirements throughout Canada. Please let us know if you require an accommodation due to a disability during any aspect of the recruitment process and we will work with you to address your needs.

Additional Information:

All candidates considered for hire must successfully pass a criminal background check, credit check, and validation of their work experience to qualify for hire. We thank all interested applicants, however we will only be contacting those for interview who possess the skills and qualifications outlined above.

Why should you work for goeasy?

To learn more about our great company please click the links below:

PAID1234	Mississauga, ON	e8f01b72a171ac29	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DbNJQGwjC-NEA80-qlVyEGmSv34Xx0eW4t4u9rXSv_7_tlvMU2e-Gw_wrZ47s1zPCnrySU2yuEL07wtSyx7XOwYh5VPcVL7CxRhOYwHzR-CdmyBrwbeqommOn6y9FD_uKIYpUCb9-PC_XgwGzBDrPy7SD7JQndrQckDdpmPdvVOjydr4_QgV8g8QpKPaK0U8K_2zSIO4mZgh40OdhRsdeeTy60qlDtW9GJ_KsKIrxCBL2e_PvZWux-DalakNU0qZIJTnklqd-XWljIeeRuH0RlEz0foKS0NdZbP_0zHm9HUalr6pr1fx5JcHDRmcFFRMFT3q_rakCnR_sJ4x77xpYwqgdt__14NwedM06kFzpVrpg5tJEOIAkeNCFFCDYl6no-7SdKJnE98H2qqmk4JFiOasEc6ycsn6KtS-PFd2-uFvKXdmKF17gOR1GU_t_ugASQyIT-8FGglu7WJwmvdDOzGbazDaqeSdfBov2F9T4ekw==&p=13&fvj=0&vjs=3	Data Engineer
2020081615	Toronto, ON	Our client, a boutique-size consulting firm, prides itself on being at the forefront of innovation in the Big Data space. Founded in 2010, they provide thought leadership and implementation excellence within the ever-growing data and analytics world.
They take pride in having some of the most highly trained and experienced consultants in the industry which translates into optimal value for their clients. They were one of the first companies to provide analytics and data as a service, via the cloud, as early as 2010. They strive to make sure their customers are well-positioned with the best technologies/tools in the industry, constantly evaluating new and existing technology partnerships. Some of the more prominent companies they have partnered with include; Snowflake, DataRobot, MicroStrategy, Informatica, Amazon AWS & Microsoft. They continue to invest in their most valuable resource, their people. They do this through extensive training both on the job and through various educational programs.
Due to growth, they are seeking to hire an Intermediate - Senior Integration Consultant.

Desired Skills and Experience
: University/College degree in Computer Science, Mathematics, Data Science and/or Relevant Degree
: 5+ years hands-on development, configuration, scripting and administration experience with Data Integration platforms. (i.e. Informatica, Talend, DataStage, SSIS)
: BI Experience (MicroStrategy, Looker, Tableau, PowerBI) considered a nice to have
: Extensive theoretical and practical knowledge of data warehousing principles/concepts and practical development experience in all areas of the data warehousing life cycle
: Experience with Data Management, ETL, Cloud Data (AWS), Data Integration
: Knowledge of OLAP-related principles and concepts
: Strong grasp of data modeling techniques and concepts (Normalized/Denormalized, Conceptual/Logical/Physical, Star, Snowflake, Data Vault)
: Strong knowledge and experience with relational databases such as Snowflake, SQL Server, Oracle (Advanced knowledge of reading and writing SQL, Performance analysis and tuning)
: Knowledge and experience with key Big Data technologies (Hive, Presto, Spark, Kafka, NoSQL databases, Semi-structured data access patterns (Json, Parquet, XML, etc.))
: Strong Python scripting skills
: Excellent communication skills
: Great problem-solving skills
: Leadership and good client management skills

Day to Day Activities Would Include
: Conduct relevant customer interviews to determine key business requirements and objectives
: Build appropriate analytical data models based on outcomes of user interviews
: Analyze and profile data systems to build source to target data mappings
: Review ETL performance and conducts performance tuning as required on mappings/workflows or SQL
: Administration and support of data integration infrastructure
: 2nd level on-call support of ETL services as required

You will be responsible for attaining the following goals:
: Attaining a minimum of 1 new accreditation/certification per year
: Spending 80% or more of their time on billable work
: Completing 90% or more of their agile delivery tasks on time
: Demonstrating competency in 1 new relevant technology every year	Copperstone Connect	54e0fcc38385e3a2	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Ceb7pqR8iGmLuXIZQklF6pqed3xddONNM6Jjumj-ndyGK0kULQUmQ2REqfJ5KAzFVeK_es1WVg0s1sLgKW09UsvVRp0-dSizFQr1oNyK_VIqdjla00VU9GNVvu2x4v9p3hsL7NeF-6hBksbAaSgKazsaYTODGMKQQeVuYcxoOuxvFKuD0HRLD9ybgbcXTn1LOF9UoybJhIQ5kun-AmUHRxsjns7cnZW2VRGpFnx2eiV8TXbmRbmuXvAGv0Mom_xmlzorT-f1oJetAlcOIpjGvoBNfkWpaZ52G2gpJNTHnhZd3RIu_hWw0D-kg7qIit7i8IEIUblMJEc1cz4ntqvczdByA10OV_Q_SCHaU0idjkoUcKmB6B6lfUg_enxVWxM2RZKVOyjvpkMPKkN3VBO-1K2tzUKDbpgQ2N1Bckn9896PSYj-e5kzBVYYpeGmTJhAB-LLA9nOZ_BlhCPBJU3NWbUuBtMhZLiFAL-57tZtkdUw2FabFg5mtXL6eMfrvUCvbGZAdDU3wJP9k5g5sEnx-B96p2dG_O3X19ceGB53VK_hIoptziXaexfg99nQ9KO6fpQMtJMSFoYcY8cKU53GbTLG6C-BNAkazFvOlbOBE1PAzKGAakb1y3I8j4KntfmnP3v1CcloIRboHSnU1ql3W1IFH2gBd-Xqj6dV-ojhqCIg==&p=14&fvj=0&vjs=3	Data Engineer/Integration Consultant	2020-08-16T23:46:44.000Z
2020081616	Toronto, ON	2020-08-16T23:46:45.000Z	The growing team here at Myticas is looking for an experienced Data Engineer who would be interested in a remote contract opportunity within the Ottawa, Toronto and Montreal regions.

Accountabilities:

Defining and reviewing security design requirements for cloud infrastructure and application components.
Evaluating architecture patterns from security perspective.
Building and implementing security controls to enable enforcement of compliance with Cloud Control Objectives, using custom Azure policies and integrated controls in DevOps processes

Requirements:

Strong Data Engineer w/ DevOps expertise + Azure Cloud Experience
Must know how to code and stand up scripts.
Experience with Data Digestions
Experience writing scripts to automate (infrastructure)
ARM Templating Expertise
Azure Synapse Expertise
Support developing automated DevOps processes and procedures for the following Azure components:
Azure Synapse (Azure DW) & Studio (private preview)
Azure Data Catalog Gen 2 (Babylon – private preview)
Azure Data Lake Storage Gen 2
Azure ML
ML Flow
Azure SQL Analysis Service
Azure Databricks
ADF data pipelines for data loading to AzSQL/Synapse
ADF data pipelines for connecting to on-prem data sources for data

Candidates looking to apply for this role are to send us an updated version of their resume in confidence. Our team will be sure to review all applicants and follow up accordingly at the conclusion of the review process.

Job is also known as: Data Engineer, DevOps Engineer, Azure Cloud Engineer, Azure Engineer, Cloud Engineer

INDMY	BHJOB15656_15145 - Data Engineer w/ DevOps - Azure Cloud	a32b7c599156a218	Myticas Consulting	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Bt274jo0r_e0cCCqhskoQvohXG3bfDAo8rIdE-Msu3yuVfyF2oiDFKHFOU-vxVulAfe8kPL7riFwoXa1bsfN-szjxO_8qUQWbPsJOnTg0EkutIoK2S4NNLL24JQh9e4DsE9hv907k2_zDbvj0a8WAj5oPSV5J0YhgrbSF1z_nzlAWq6WEE44HHqpAFoYlHEsubH0krceduZUvnD-0GfFIbvYDaoLkZZ1WeBzmRViyknaDLmgRTppMYbQVXnieigPJgmEpNv3-i8mQ3rhQxZMM8gz9FIhpJOaL0D2BA9Sa5cKx2E0-XDDtdm9AnFqM3DwcGNkGkmdqUw6KqtXsPxQ5EGPyvoTkfZ3tULyNKZ10NpTepp-sJKdbuWLjsif86nPzBg0ujeptl4iaPzNCwWM8lv9OmUpo9fmh4KUP82nIQ_lZ7K5JKbL-rmoV0GHQjkWIhVMyVp_Y0RA==&p=0&fvj=0&vjs=3
2020081617	Data Engineer (Intermediate 4-6 years) - Contract - Remote during COVID	Role: Data Engineer (Intermediate 4-6 years)Structure: Contract (6 months initially)Location: North York, ONHours: Monday - Friday 40 hours (remote during COVID-19)Pay: $50.00 p/h inc.The role: We are looking for Intermediate Data Engineers who have roughly between 4-6 years experience who ideally has experience moving data from Google cloud over to Big query.However, if you are someone who has good experience with ETL and Data Pipeline Design & Data Validation experience and can script in Python and create strong SQL Queries then this will also be a potential match for this role.Ideal Qualifications to be successful in this role: - Degree in Computer Science, Engineering, Information Systems (or equivalent combination of skill and experience)- 4-6 years of experience working with data architecture projects- Excellent SQL coding and experience with a broad array of development tools and platforms including exposure to a big data environment tools/languages, such as SQL, R, SAS, Python, etc.- Experience creating relational database design and data models- Experience with various analytical data platforms and technologies- Experience with using REST API, Cloud (GCP Ideally)- Experience in custom or structured data integration design, implementation, and maintenance- Experience with business intelligence tools such as Qlikview, Tableau and Microstrategy (Nice to have for Data Visualizations)- Previous experience working in retail or eCommerce is highly preferred- Ability to curate data to tell stories and provide business insightsPlease apply with an updated resume and ensure the required skills you are able to speak to for this position are included.For more roles like this please go to www.corgta.com/find-a-job/Job Type: Full-timeSalary: $45.00-$50.00 per hour	CorGTA	North York, ON	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Dkh866153otbRJ8nVMuEXnXPd315JS377__3VyD6HnNQkmcaOHMemmy5pLrNV6j0WaVWEWWabnp4PP6-2u77DLfT_FCIGvCW8CQmQW140Z7EVU32Jnwl42FALsxnbc4n5M6FOTfoW2OoTMOcGG21HZGOoXL7fWT8eaT67VlcClh4sHQcJsHCX_xl3a_pMoBRPB-aJxbPiJqmWcqBop-mQyXkzENdU99j87dvuJ4emyyHz51-jRR7LsnTWM6erPo18LT9ER71h4KwgAyyJAqPeF5LkmX7Y0EiOkO1Zt6fuKIE3_DFZ_-xKVDS7L3Kk4YJ7L9CmZ5Z7NrKQ7ktUJrNrNhZAikU1a9HBexG6pjUSuVVJlXGTvgg-QXg9V6NwIdZNJ4dE1WRWjG-_FiH6bYIwakrC1PHYI4szaJPmqutw5-rtKxsQog6vkOGwUXACdnO2WhrXDir83Me4-FjGv49FiproOOGFP55WLPoVZCuxT9zaFffSqjP5a&p=1&fvj=1&vjs=3	997673b8effa647f	2020-08-16T23:46:46.000Z
2020081618	Toronto, ON	FreshBooks	Staff Data Engineer (Remote)	980a5785f8688836	https://ca.indeed.com/rc/clk?jk=980a5785f8688836&fccid=785af18d53962443&vjs=3	2020-08-16T23:46:47.000Z	FreshBooks has an ambitious vision. We launched in 2003 but we're just getting started and there's a lot left to do. We're a high-performing team working towards a common goal: building an elite online accounting application to help small businesses better handle their finances. Known for extraordinary product and customer service experiences and based in Toronto, Canada, FreshBooks serves paying customers in over 120 countries.

The Opportunity - Staff Data Engineer (Remote)

We're looking for a strong technical leader who has gravitas to influence and has deep knowledge of building and designing scalable data platforms. An effective communicator, a mentor who can think on their feet and be able to come up with practical, simple solutions to complex problems. As a Staff Data Engineer, you are someone who can redefine data engineering capability and constantly push boundaries. You will co-own FreshBooks' data platform to ensure scalability and elasticity.

If this appeals to you, please come and chat with us to learn more about how you can become a part of the intrinsically motivated teams of data engineers!

What you'll do in your first twelve months at FreshBooks:

Collaborate with analytics, engineering and business teams working on the product for our customers.
Be the technical lead and advocate best practices, contribution to roadmaps, developing the domain breadth and/or depth, stewardship.
Contribute towards defining technical vision and challenge across different levels of business including Senior Executives and Senior Leadership group.
Help in growing the technical expertise of the team as they continue to work and touch services outside their realm.
Demonstrate experience coaching and mentoring data engineering teams, and growing the overall technical maturity of our Data organization.
Evolve our data platform architecture to scale with our rapidly growing customer community.
Develop a deep understanding of multiple parts of our stack as well as the processes and technologies relevant to our tech space.
Be capable of supporting the data platform from end to end.
Raise the bar for our entire data engineering team through best practices, automation, documentation, and hiring.
Level up our operational excellence, and drive our team to maintain it, so that common regressions are root caused.
Support operational excellence and make measurable improvements to our support processes.

What you bring:

At least 10 years of combined experience in software and data engineering (Agile or Lean environment)
Strong programming skills in Python or similar language with deep refactoring skills.
Five or more years with cloud based data platforms and technologies such as AWS or Google Cloud Technologies, Elastic MapReduce, S3, EC2, and Kinesis.
Experience in architecting and building large-scale batch or stream processing data pipelines.
Experience architecting scalable ETLs with inputs from multiple data sources.
Experience with data warehousing technologies such as Amazon RedShift or Google BigQuery.
Experience in Airflow or other data infrastructure job scheduling software.
Experience writing complex SQL queries.
Ability to troubleshoot and determine root causes of issues.
Experience working with large codebases, writing robust and testable code
Experience ensuring security and governance of data.
A passion for keeping up to date in current technologies and future trends
A deep understanding of test-driven (and behavioural test driven) development, and of building substantially complete test code, and not just for the happy path
Familiarity with continuous integration (or better, continuous delivery) and automated build pipelines.
The ability to balance desire to ship code quickly to our customers with the responsibility of making good technical decisions.
A long-standing habit of continuous learning, and of applying new technologies, architectures, and methodologies to improve the code and Engineering organization.

What you might bring:

We're looking for a variety of talented technical leadership and know that a mix of skills and experience is useful. Even having a couple of the skills from the list below would be a strong asset.

Expertise in the core areas of business of FreshBooks (accounting, payments, small business solutions)
A background in DevOps and service ownership, and a clear understanding of bounded contexts and how they map onto microservices.
Strong pair programming both as a mechanism for producing better code, and for teaching skills.
Experience with Docker, Kubernetes, Ansible, Terraform, or other similar tools.
Experience with Redis / Elasticsearch & RabbitMQ.

Why Join Us

We're a motivated bunch, with our eye's laser-focused on shipping extraordinary experiences to businesses. You will be surrounded by hardworking team members who share a common vision for what an amazing software company could be and have the opportunity to help build an elite one, right here in downtown Toronto.

Apply Now

Have we got your attention? Submit your application today and a member of our recruitment team will be in touch with you shortly!

FreshBooks is an equal opportunity employer. We do not discriminate based on gender, religion, race, mental disability, sexual orientation, age, or any other status. All applicants are considered based on their qualifications and merits. At FreshBooks, we inspire an environment of mutual respect and we believe diversity and inclusion are crucial to our success.

Here at FreshBooks, we welcome and encourage applications from people with disabilities. Should you require any accommodations during the recruitment process, please advise your recruiter on how we can meet your needs to ensure a fair and equitable selection process in a confidential manner.
2020081619	Toronto, ON	Job DescriptionSDK is looking for Big Data Engineers that will work on the collecting, storing, processing, and analyzing of huge sets of data. The Data Engineer must also have exceptional analytical skills, showing fluency in the use of tools such as MySQL and strong Python, Shell, Java, PHP, and T-SQL programming skills. He must also be technologically adept, demonstrating strong computer skills. The candidate must additionally be capable of developing databases using SSIS packages, T-SQL, MSSQL, and MySQL scripts.The candidate will also have an ability to design, build, and maintain the business’s ETL pipeline and data warehouse. The candidate will also demonstrate expertise in data modeling and query performance tuning on SQL Server, MySQL, Redshift, Postgres or similar platforms.Experience:ETL: 5 years (Preferred)Software Development: 5 years (Preferred)Data Integration: 5 year (Preferred)Spark Programming (Azure Databricks preferable)Python, Java & SQLKnowledge of Azure Cloud (Data Platform Technologies)Experience and commitment to development and testing best practices.Manage high volume, high traffic GDPR solutions buildSCALA a nice to haveSDK is looking for Big Data Engineers that will work on the collecting, storing, processing, and analyzing of huge sets of data. The Data Engineer must also have exceptional analytical skills, showing fluency in the use of tools such as MySQL and strong Python, Shell, Java, PHP, and T-SQL programming skills. He must also be technologically adept, demonstrating strong computer skills. The candidate must additionally be capable of developing databases using SSIS packages, T-SQL, MSSQL, and MySQL scripts.The candidate will also have an ability to design, build, and maintain the business’s ETL pipeline and data warehouse. The candidate will also demonstrate expertise in data modeling and query performance tuning on SQL Server, MySQL, Redshift, Postgres or similar platforms.Base Qualifications3+ years of demonstrated data engineering experience or development experience3+ years of experience with Big Data Technologies like Hadoop or Hive3+ years' experience in custom ETL design, implementation and maintenanceProficient designing and implementing data models and data integrationExperienced deploying Azure SQL Database, Azure Data Factory and well-acquainted with other Azure services including Azure Data Lake and Azure MLExperience implementing REST API calls and authenticationExperienced working with agile project management methodologiesPreferred QualificationsAt SDK we believe “perfection” is a process. We hire for fit and invest in training, so our people continue to be the best for themselves, SDK, and SDK's customers.EducationComputer Science Degree/DiplomaMicrosoft Certified: Azure Data Engineer Associate:Job Types: Looking for full-time employees only. No Contractors. Must be eligible to work in Canada.Job Types: Full-time, ContractExperience:Data Engineering: 5 years (Preferred)Location:Toronto (Required)Work remotely:Temporarily due to COVID-19	2020-08-16T23:46:48.000Z	https://ca.indeed.com/company/SDK/jobs/Data-Engineer-1e5daca6232e90a6?fccid=185dce58b577e4a6&vjs=3	1e5daca6232e90a6	SDK	Data Engineer
2020081620	Toronto, ON	Data Engineer (Cloud)	StackPros Inc.	2020-08-16T23:46:48.000Z	https://ca.indeed.com/company/StackPros-Inc./jobs/Data-Engineer-fa77394a330e96c4?fccid=efee8d7cab418992&vjs=3	StackPros Inc is seeking a candidate for a full-time role within our Data Systems Team in Toronto, Ontario.The *Cloud Data Engineer* will play a key role at StackPros, required to help create and maintain industry-leading quality and efficiency of service and software delivery.StackPros will rely on the Data Engineer to support the Data Systems team, in both data engineering and data science-related workflows. The Data Engineer will be expected to meet and exceed StackPros' quality standards, while helping the organization rapidly expand complex Machine Learning and related applications.*Key Responsibilities*:*Data Engineering-Specific Responsibilities*Participate in continuous delivery pipeline to fully automate deployment of the highly available cloud platform that supports multiple projectsDesign and develop ETL workflows and datasets to be used in data visualization toolsWrite complex SQL queries with multiple joins to automate and manipulate data extractsPerform end to end Data Validation to maintain accuracy of data setsBuild tools for deployment, monitoring and operationsTroubleshoot and resolve issues in the development, test and production environmentsDevelop re-useable processes that can be leveraged and standardized for multiple instancesPrepare technical specifications and documentation for projectsStay up-to-date on relevant technologies, plug into user groups, understand trends and opportunities to ensure we are using the best possible techniques and toolsUnderstand, implement, and automate security controls, governance processes, and compliance validationDesign, manage, and maintain tools to automate operational processes*Data Science-Specific Responsibilities*Perform exploratory data analysis to identify patterns from historical data, generate and test hypotheses, and provide product owners with actionable insightsDesign experiments for product initiatives and perform statistical analysis of the results with recommendations for next steps and future experimentsCreate and design dashboards by using different data visualization tools to present reports and insights, and support business decision makingHelp the StackPros Data Systems team adopt and evolve Predictive Modeling, Machine Learning and Deep Learning processes to deliver to clients in the future*Company-Wide Responsibilities: *Maintain and exceed client satisfaction with StackPros Inc.’s deliverables, day-to-day work and overall value as a partnerCultivate opportunities for company growth, always seek areas where StackPros Inc.’s role could be expandedAdapt to ever-changing client needs and expectationsMaintain dedication toward achieving excellence in StackPros Inc.’s delivery against client needs, and overall success as an organizationBe an enthusiastic, positive and generally awesome team mate, mentor & constantly curious learner*Qualifications: *4+ years experience in Data EngineeringUnderstanding of digital ecosystems including online data collection, cloud systems and analytics tools (Google Stack, Facebook, AWS, Salesforce, Adobe Suite etc.) a strong assetStrong technical understanding of a range of marketing concepts such as cookie-based data collection, setting and leveraging audience segments, attribution modelling, AB/N & multivariateExcellent written & verbal communication skills are essential; candidate should be comfortable presenting and participating in group discussions of concepts with internal and external stakeholdersCandidate must exhibit an analytical, detail-oriented approach to problem solvingExperience with Jira / Atlassian project management tools is an asset*WHAT’S IN IT FOR YOU?*100% employer-paid benefits packageMonthly yoga and meditation classes onsiteRegular Lunch and Learns from your Team MatesStanding desksEntertainment and Games area, including pool tableFully-loaded kitchen: snacks/fruit/drinks/beerFun Employee Events and ActivitiesParticipation in Community EngagementJob Type: Full-time	fa77394a330e96c4
2020081621	Toronto, ON	Scotiabank	4cd09805ecea1dd2	Requisition ID: 88577

Join the Global Community of Scotiabankers to help customers become better off.

JOB SUMMARY
The Bank’s Internal Audit Department plays a key role in the risk management process of the Bank. Our mandate is to provide independent and objective assurance over the design and operation of the Bank’s internal controls and to advise senior management on improvements to the Bank’s operations.

The role of the Data Engineer is to assist in the development of the data infrastructure and leverage existing analytical tools and methods to help Audit professionals to execute audits through enhanced data sampling, advanced analytics, continuous monitoring and visualization. As an active member of the team, they will grow in capability, coverage and knowledge to move the team forward.

The successful candidate will have a solid knowledge of R or Python, working knowledge of data analytics and statistical methods, as well as solid working knowledge of data acquisition and transformation (SQL, SAS). Experience with Mainframe commands and procedural programming, including Mainframe SAS, CLIST and ACF2, is strongly preferred. The ideal candidate will be able to communicate with Auditors to execute on projects independently and increase engagement.
KEY ACCOUNTABILITIES
Identify, acquire and use large volumes of data to provide incremental business value through data analytics methods and algorithms that help the Internal Audit group execute Audits with greater efficiency, accuracy and coverage
Assist in the development of the data infrastructure / architectureDesign and build a data pipeline that cleans, transforms and aggregates unstructured data in organized databases / data sourcesDevelop and write complex queries (that are suitable and scalable) for ‘big data’, build models, and ensure data is accessible to Data Scientists and Analysts, and the data sets are working smoothly.Develop data infrastructure necessary to support audits and continuous monitoring
Work with large datasets and distributed computing tools for analysis, data mining and modelingCollaborate with business lines and other stakeholders to enhance Internal Audit's use of data and analyticsPrepare detailed documentation to outline data sources, models and algorithms used and developed, as well as results obtainedEnhance and acquire skills as needed to move the team forward
FUNCTIONAL COMPETENCIES
Degree in Statistics, Computer Science or related fieldExperience with data ingestion, cleansing, transformation, and integrationExtensive experience with statistical, data processing and analytical tools like R, Python and SASExperience on mainframe with CLIST, skeletons, and panels.Working knowledge of ACF2Experience with SAS on the mainframe including FSEDIT.Strong analytical skillsSQL skills for querying relational databases (e.g., SQL Server, DB2, MySQL)Nice to have: implementing visualization tools like Microsoft Power BI or TableauExcellent written and oral communication skills to communicate with Auditors and present clear resultsAt least 5 years experience with similar tools / functions
Location(s): Canada : Ontario : Toronto
As Canada's International Bank, we are a diverse and global team. We speak more than 100 languages with backgrounds from more than 120 countries. Our employees are committed to a superior customer experience and use the Bank’s six guiding sales practice principles to ensure they act with honesty and integrity.

At Scotiabank, we value the unique skills and experiences each individual brings to the Bank, and are committed to creating and maintaining an inclusive and accessible environment for everyone. If you require accommodation (including, but not limited to, an accessible interview site, alternate format documents, ASL Interpreter, or Assistive Technology) during the recruitment and selection process, please let our Recruitment team know. If you require technical assistance, please click here. Candidates must apply directly online to be considered for this role. We thank all applicants for their interest in a career at Scotiabank; however, only those candidates who are selected for an interview will be contacted.	2020-08-16T23:46:49.000Z	https://ca.indeed.com/rc/clk?jk=4cd09805ecea1dd2&fccid=3002307a9e5b4706&vjs=3	Data Engineer
2020081622	55e49e6c1c9af157	Toronto, ON	What is the Opportunity?

The DNA (Data & Analytics) group is responsible for enabling RBC to become a data-driven organization. As part of this mission, DNA works with various lines of business (Personal & Commercial Banking, Wealth Management, Insurance, Capital Markets, etc…) to create and build data-driven solutions to serve our clients better. You will be part of a strong team of developers building out our reusable, core data services to deliver value to business partners through data, insights and AI.
The DNA Data Services team will build data-driven products and services/API’s, tackle challenging and interesting data-related problems using RBC's massive internal datasets (client relationships, user behavior across channels, transactions, etc….) and strategically partner with the business to enable client interactions to be informed by Artificial Intelligence (AI).

What will you do?

 Build large-scale real-time data pipelines using the latest technologies.
 Apply design thinking and an agile mindset in working with other engineers, data scientists and business stakeholders to continuously experiment, iterate and deliver on new initiatives.
 Leverage best practices in continuous integration and delivery.
 Help drive transformation by continuously looking for ways to automate existing processes and testing and optimize data quality.
 Explore new capabilities and technologies to drive innovation

What do you need to succeed?

Must-have
 Bachelor’s degree in computer science, software engineering, or equivalent
 5+ years development experience in Java
 2+ years’ experience with streaming or messaging technologies (Kafka, etc…)
 2+ years’ experience in Big Data environments (Spark, Hadoop, etc…)
 Experience building operational REST APIs
 Strong foundational knowledge of relational databases (MySQL, SQL Server, etc…) and NoSQL stores (Elasticsearch, Neo4j, MongoDB, etc…)

Nice-to-have
Knowledge of public cloud environments (AWS, Azure)
 A passion for simplifying and automating work, making things better, continuous learning, solving open-ended problems, improving efficiency and helping others

What’s in it for you?

 We thrive on the challenge to be our best, progressive thinking to keep growing, and working together to deliver trusted advice to help our clients thrive and communities prosper. We care about each other, reaching our potential, making a difference to our communities, and achieving success that is mutual.
A comprehensive Total Rewards Program including bonuses and flexible benefits, competitive compensation, commissions, and stock where applicable
Leaders who support your development through coaching and managing opportunities
Ability to make a difference and lasting impact
Work in a dynamic, collaborative, progressive, and high-performing team
A world-class training program in financial services
Flexible work/life balance options
Opportunities to do challenging work
Opportunities to take on progressively greater accountabilities
Opportunities to building close relationships with clients
Access to a variety of job opportunities across business and geographies	https://ca.indeed.com/rc/clk?jk=55e49e6c1c9af157&fccid=537b899e30af3338&vjs=3	2020-08-16T23:46:50.000Z	Senior Data Engineer	RBC
2020081623	Toronto, ON	d40c380c285457aa	https://ca.indeed.com/rc/clk?jk=d40c380c285457aa&fccid=ae47c2169a389ac0&vjs=3	2020-08-16T23:46:51.000Z	Gensquared	Data Engineer	Gensquared prides itself on being at the forefront of innovation in the Big Data space.

Founded in 2010, Gensquared provides thought leadership and implementation excellence within the ever-growing data and analytics world. The volume of data is expected to grow to 5x what it is today, and Gensquared helps its customers to be well-positioned for success to use this data to their advantage. Companies that use data have been proven to outperform their peers by as much as 85% (McKinsey Group 2017).

We take pride in having some of the most highly trained and experienced consultants in the industry which translates into optimal value for our clients. We were one of the first companies to provide analytics and data as a service, via the cloud, as early as 2010. We strive to make sure our customers are well-positioned with the best technologies/tools in the industry, constantly evaluating new and existing technology partnerships. Some of the more prominent companies we have partnered with include; Snowflake, DataRobot, MicroStrategy, Informatica, Amazon AWS & Microsoft. We continue to invest in our most valuable resource, our people. We do this through extensive training both on the job and through various educational programs.

Our consultants are family and valuing each other is one of our most important core values. We are looking to grow our family as we search for a Data Integration Consultant.

This individual will be responsible for attaining the following goals:

Attaining a minimum of 1 new accreditation/certification per year
Spending 80% or more of their time on billable work
Completing 90% or more of their agile delivery tasks on time
Demonstrating competency in 1 new relevant technology every year


Day to day activities for this role would include:

Conduct relevant customer interviews to determine key business requirements and objectives
Build appropriate analytical data models based on outcomes of user interviews
Analyze and profile data systems to build source to target data mappings
Build required ETL to populate target designed data warehouse and/or data lake
Review ETL performance and conducts performance tuning as required on mappings/workflows or SQL
Administration and support of data integration infrastructure
2nd level on-call support of ETL services as required

Desired Skills and Experience

University/College degree in Computer Science, Mathematics, Data Science and/or Relevant Degree
5+ years hands-on development, configuration, scripting and administration experience with Data Integration platforms. (i.e. Informatica, Talend, DataStage, SSIS)
BI Experience (MicroStrategy, Looker, Tableau, PowerBI) considered a nice to have
Extensive theoretical and practical knowledge of data warehousing principles/concepts and practical development experience in all areas of the data warehousing life cycle
Experience with Data Management, ETL, Cloud Data (AWS), Data Integration
Knowledge of OLAP-related principles and concepts
Strong grasp of data modeling techniques and concepts (Normalized/Denormalized, Conceptual/Logical/Physical, Star, Snowflake, Data Vault)
Strong knowledge and experience with relational databases such as Snowflake, SQL Server, Oracle (Advanced knowledge of reading and writing SQL, Performance analysis and tuning)
Knowledge and experience with key Big Data technologies (Hive, Presto, Spark, Kafka, NoSQL databases, Semi-structured data access patterns (Json, Parquet, XML, etc.))
Strong Python scripting skills
Excellent communication skills
Great problem-solving skills
Leadership and good client management skills
2020081624	Toronto, ON	Must have:
Strong Spark, Kafka and are familiar with Flink/Druid/Ignite/Presto/Athena

You are proficient in Java/Scala/Python/Spark	2020-08-16T23:46:51.000Z	5929006533b711f6	https://ca.indeed.com/rc/clk?jk=5929006533b711f6&fccid=580f6b9a5ada66c2&vjs=3	Data Engineer- Permanent, downtown Toronto	IT Connex
2020081625	Data Engineer Mgr - Platforms	At Rogers, we connect Canadians to a world of possibilities and the memorable moments that matter most in their lives. Every day we wake up with one purpose in mind. To bring loved ones together from across the globe. To connect people to each other and the world around them. To help an entrepreneur realize their dream. A sports fan celebrate a special moment.

Because we believe connections unite us, possibilities fuels us, and moments define us.

Rogers is seeking a Database and Platform Administrator as part of the Enterprise Data and Analytics Team who will be operating in a senior role managing a team of Big Data, Cloud and other Database Administrators. The Database and Platform Administrator will serve as a key member of the Data Engineering team and will be responsible for the design, deployment, and operations of all of the database infrastructure in Development, Pre-Production and Production environments.

Responsibilities:
Responsible for Management of all database infrastructure including: transactional databases, data warehouses, and cloud infrastructure as well as data replication and extractions from these database infrastructures.
Design of database structures and replication, failover, backup, and extraction functions.
Product evaluation and selection for all database infrastructure requirements.
Creation of operations documentation and scripts for all database infrastructures.
Deployment and tuning of monitoring for critical database availability and performance thresholds and alarms.
Ensure database servers are backed up and meet the Enterprise Recovery Time Objectives
Tuning of all databases in Development, Pre-Production, and Production environments.
Create performance baseline metrics and monitor performance problems.
Application database setup in cloud environment. Setup/configuration of database backup/recovery, database monitoring, database performance tuning, Security, problem diagnosis
Database design and DDL deployments to cloud environment and working closely with application teams
Evaluating and testing new database features, documenting and presenting to team

What You Need to Have:
8+ years of experience as an Oracle DBA supporting prod/QA/Dev environment
Experience migrating the On-Prem Databases (Oracle/Big Data) to Cloud (Azure and AWS)
Experience of all key DBA roles in Cloud (backup, recovery, alert monitoring, performance monitoring, security administration, performance tuning, other database maintenance – statistics, re-indexing, etc.
Experience in automation and scheduling of database backups and other database maintenance
Knowledge of availability zones, HA, DR solutions in the Cloud will be nice to have
Experience with Cloud portals to create and manage databases and resources

Schedule: Full time
Shift: Day
Length of Contract: No Selection
Work Location: 8200 Dixie Road (101), Brampton, ON
Travel Requirements: Up to 25%
Posting Category/Function: Technology & Information Technology
Requisition ID: 201652

Together, we'll make more possible, and these six shared values guide and define our work:

Our people are at the heart of our success
Our customers come first. They inspire everything we do
We do what’s right, each and every day
We believe in the power of new ideas
We work as one team, with one vision
We give back to our communities and protect our environment

What makes us different makes us stronger. Rogers has a strong commitment to diversity and inclusion. Everyone who applies for a job will be considered. We recognize the business value in creating a workplace where each team member has the tools to reach their full potential. At Rogers, we value the insights and innovation that diverse teams bring to work. We work with our candidates with disabilities throughout the recruitment process to ensure that they have what they need to be at their best. Please reach out to our recruiters and hiring managers to begin a conversation about how we can ensure that you deliver your best work. You matter to us! For any questions, please visit the Rogers FAQ.

Posting Notes: Information Technology & Engineering	Brampton, ON	4326ff472bad16d1	2020-08-16T23:46:52.000Z	https://ca.indeed.com/rc/clk?jk=4326ff472bad16d1&fccid=b5583ca8df6a68c3&vjs=3	Rogers Communications
2020081626	Toronto, ON	2020-08-16T23:46:53.000Z	Company Description

The Company
Hitachi Solutions is an impact-driven global leader in consulting dedicated to delivering competitive, end-to-end solutions based on the Microsoft Cloud. Our deeply connected teams are unified by our values and our commitment to helping clients succeed and compete with the largest global enterprises. We are a division of the 38th largest company in the world and carry the strength of a vast network of interconnected Hitachi companies all while remaining nimble, agile, and ready to pivot at a moment’s notice.
Hitachi Solutions started as three founding partners and transformed into nearly 2,000 consultants, developers, and support personnel all around the globe. With 36 Microsoft Partner of the Year awards, Hitachi Solutions has established itself as a leading partner in the ever-growing landscape of technology consulting.
The Culture
Our team is a mix of curious, fun, and get it done. We celebrate collaborative thought leadership and individual talents so that our ideas are translated into real-world results.
Each day, our team members show up to work as a blank slate with the sole purpose of coloring their canvases with knowledge. We are a complex group of resilient learners and encourage one another to go beyond the limits of conventional expectations. As a member of the dynamic Hitachi Solutions team, you will be challenged, pushed, and unconditionally supported in your efforts to drive the business forward.
** THIS ROLE IS OPEN TO ANY CANDIDATES FROM WITHIN CANADA**

Job Description

As an Analytics Data Engineer for Hitachi’s Azure Cloud Enablement Team you will be responsible to deliver high quality modern data solutions while being part of a dynamic and fast-growing team consisting of endless opportunities.
The successful candidate will be a self-motivated, passionate individual who strives to be the best at what he/she does and creates a trail of ecstatic Customers for life. This person will love to learn, contribute to the team and wants to be part of something great.
Knowledge and Experience
Hands-on experience with the Azure Data Platform (Data Factory, Data Lake, Data Warehouse, Blob Storage, SQL DB, Analysis Services)
Data quality (profiling, cleansing, enriching)
Data Modeling – including design from conceptual to logical to physical data models
Considered to be an expert in T-SQL
Hands-on experience with MPP database technologies such as Azure SQL DW, Teradata Netezza, etc.
Experience with multiple components listed, required:
Power BI including DAX
Database migration from legacy systems to new solutions
DevOps
Interpreted languages (i.e. python, C-sharp, Java, Scala, etc.)
Databricks
LogicApps
PowerApps
HDInsight
D365FO / CE experience as it pertains to data extraction
Knowledge of Cap Theorum and Distributed Database Management Systems, iis considered an asset.
Opportunity for a career path into a Data Scientist role if desired

Qualifications

Required skills / qualifications
Proven ability to engage customers to understand customer challenges and needs to develop technical solutions
3+ years of hands on experience working with the Azure Platform and its relevant components
Proven experience of architecting Azure services into a solution platform on Microsoft Azure for Analytics
Minimum 5 years hands on development experience with ELT/ETL using MS SQL Server, Oracle or similar RDBMS Platform
Familiarity with data visualization tools (e.g. PowerBI, Tableau etc.)
Experience or desire to coach, mentor and provide leadership to team members
Post-secondary degree/diploma in Business, Computer Science or a related discipline;
Strong communication skills, both written and verbal
Prepared for domestic and US travel as required
Preferred considered an asset, NOT required:
Project management experience
Databricks and Spark SQL
Previous Consulting experience
Additional Information

Opportunity Benefits:
Medical and Dental Benefit Package (including Long Term and Short Term Disability)Base salary plus targeted bonus package
This position can be based anywhere in Canada, though travel might be required.	Hitachi Solutions	24716d17b181ff38	Intermediate Data Engineer	https://ca.indeed.com/rc/clk?jk=24716d17b181ff38&fccid=28f79c18789111e8&vjs=3
2020081627	Toronto, ON	2020-08-16T23:46:53.000Z	Avanade	7b094d3be2a72d1a	https://ca.indeed.com/rc/clk?jk=7b094d3be2a72d1a&fccid=5386281035076fdf&vjs=3	Do you enjoy making sure that information is accessible and easy to use? So do we.

You’re a data designer who knows how to find, store and present a range of information from different sources so that everyone can access what they need quickly and simply, and use it effectively.

About you

You draw on your considerable experience in bringing data and statistics to life to solve sometimes complex problems, and you’re comfortable looking after several projects at once. You’re able to make your own decisions while at the same time supporting more junior team members.

About the job

As a Consultant, Data Engineering, you know the importance of data to business. You design and set up projects that bring together information from a variety of sources, to enable analysis and decision-making. You make sure that data is accessible and easy to use, so that it can be used for routine and ad-hoc analysis.

Day to day, you will:
*

Use your knowledge to plan and deliver data warehouse and storage
*

Take part in designing and running bespoke data services for individual projects
*

Stay up to date with business best practice in using and retrieving data
*

Design, develop, adapt and maintain data warehouse architecture and relational databases that support data mining
*

Customize storage and extraction, metadata, and information repositories
*

Create and use effective metrics and monitoring processes
*

Help to develop business intelligence tools
*

Support deal teams by providing subject knowledge and solutions for client proposals
*

Author reports that include key performance indicators, show where current operations can be improved, and identify the causes of any problems
*

Create and maintain report forms and formats, information dashboards, data generators and canned reports, as well as other information portals and resources
*

Travel as required.

Your skills

You’re got great experience in data and analysis, and how to source, store and share information. You’re a problem solver who’s happy to work autonomously and to share their knowledge and skills, as well as guiding other team members.

Your skills and experience include:
*

Database, storage, collection and aggregation models, techniques and technologies and how to apply them in business
*

Employing statistical and data visualization tools and techniques
*

Experience in structured problem solving
*

Great project and people management
*

Using SharePoint, PowerPivot, SRRS, Excel - including pivot tables and macros
*

Working with SQL.

You’re likely to have a Bachelor’s degree in Applied Mathematics, Statistics or another relevant field, or an equivalent combination of education and experience. You also have three to five years of relevant professional experience.	Consultant, Data Engineer
2020081628	Toronto, ON	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DQQnUBuQBSuyaIQhpC59TW7hrTbBg8v-nGtzzV8dunbQHh9VTp-k89gJ8q3B9UEXLHcYbAG67d8jpOK4E3ok8olcOhfyyiChWMI3BPvUFdra3ljwgLS_vFsJXlW6jHt2NpQMzd2p9F6BXGm9ND3feUN42yGy4tVXBU7xPsBm2zUmQg-AgzTAlu8_DBay9mmUSjN1fpAtJ-9RfKljUEag4f3MADzY-d7Az8z1UOXxYQylOmQyytDx4Te0_QqLlz1PuyuO3DbUaad57HPu4mY1mh43rJX0jMl8WfSNMLHdpn84mdMi4n3Y6eAXrCWKO4XcDq4MQcXm5S8FpqDQ0YrOBm4FmzMlwemmvkibn1DF1iH3QxePB3VXSnL_K5_ht-jRmjX6i5H0ux3OH2a9IB86IZxc-ILobWWZqS88cpMVKhakUjIIca6cWaie7p4ENuQWoSaiUMsMsWcWshwvy4bzJ-IOb9Q8WPfWwC66g7569dhoqjyTOBMJPgq7DwGeL8XT_9lYrjDSPGc7rJUNnoCh0683kzX83lP1oE5K60M8qrkjAJvStQEV4IT14AtWvlpT4_B523rCvXrLIav4NHxda_LhKzpqik-BfZ8UnF9oTKkeSIh4msoKbFOYVsTLRnhm-an3MzCQTdh-Z8gdd-QvvdX6sNtFfL_G-0d_Jz9X-4MhxVMRsXVO4Ope9kTuM2m2dqrGh8VabwA1PnxslCPooMOKLzC1jwT54hr_WoYUprHO2vrLmhqji0_sTKOFZs5OlR9HQUjGQVO7CkOvcrDe0FsaLz23blNaHzOIbZomuelf8TPN_-AZPhhQKQfGUzgjOR1mvHF3kq751txitPtIDaFIYq755FYJo=&p=12&fvj=0&vjs=3	TES - The Employment Solution	05f7bf2a7458c7fc	2020-08-16T23:46:54.000Z	Job Title- Data Engineer
Location- Downtown, Toronto
Type - Full Time Permanent
Salary - Negotiable + Benefits



Focus on data architecture, best practices, reliability, security, and complianceImprove and extend ETL, data processing, and analytics processesFacility with PowerBI, including creating dashboards and data sourcesDeveloping high complexity, fast performing SELECT queries.Developing T-SQL procedures, functions, triggers, jobs, scripts, etc.Development of Advanced T-SQL such as temporal tables, PIVOTs, recursive table expressions and more.Modeling and implementing Data Mart solution for Power BI analytics
Managing indexes, statistics, query plans alerts, database activity, and overall performance activity.In-depth experience working with relational databases, such as Microsoft SQL Server or PostgreSQLEnthusiasm for applying good data design, testing, documentation, and support practicesExperience building and optimizing data pipelines, architectures, and data setsKnowledge of message queueing, stream processing, and data stores/warehousesWorking knowledge of AWS products related to data engineeringBachelor's degree in Computer Science, Software Engineering or an equivalent
Excellent communication skills - both written and verbal; ability to speak in Spanish is a bonus

To apply please send an email to sheetalk@tes.net	Data Engineer
2020081629	2020-08-16T23:46:55.000Z	goeasy	If you are looking to join one of Canada’s fastest growing companies, goeasy Ltd. is the place for you! Recognized as one of Canada’s Most Admired Corporate Cultures, one of Canada’s Top 50 Fintech’s and one of North America’s Most Engaged Workplaces, we want the best and brightest to join our team.

We are a publicly traded company on the TSX with over 4000% shareholder return since 2001, goeasy operates two main business units. easyfinancial is our consumer lending business that offers secured and unsecured installment loans of up to $35,000 and easyhome is Canada’s largest merchandise lease-to-own company. It is our mission to provide everyday Canadians the chance for a better tomorrow, today by giving them access to the credit they need and by offering them a second chance when they have been turned down by banks and traditional lenders. With a retail network of nearly 400 locations across Canada and over 1900 employees, we are able to build lasting relationships with our customers as we help them rebuild their credit and graduate towards prime rates and a brighter financial future.

The Data Engineer on the Data Science and Business Insights team will build, integrate data from various resources, manage data in goeasy operational data store and enterprise data warehouse. This position will develop ETL (Extract, Transform and Load) with various tools on large datasets to ensure data is easily accessible, works smoothly, as well as maintain and expand the data warehouse for reporting and analysis. The Data Engineer will also work closely with the data architect on the design and architecture of our enterprise data warehouse.

Responsibilities:

Develop data set processes for data modeling, mining and production
Develop and maintain ETL processes using SSIS, Scripting and data replication technologies
Participate in development of datamarts for reports and data visualization solutions
Research opportunities for data acquisition and new uses for existing data
Integrate new data management technologies and software engineering tools into existing structures
Support the translation of business requirements for data acquisition/manipulation and provide detailed specifications that can be passed downstream for use
Develop detailed technical specifications and operational support documentation in collaboration with Business Systems Analysts, BI Engineers and Architects.
Identify and communicate technical problems, process and solutions
Create Ad-Hoc queries and reports as needed along with providing on-going analytical support for these requests
Assist in the collection and documentation of user’s requirements
Ensure that existing business processes dependent on the ODS/EDW are monitored and respond quickly to bug fixes, enhancement requests and production ETL related issues.
Dealing with the database users on a daily basis to ensure that problems are dealt with promptly and that appropriate fixes are made to resolve any problems.
Recommend ways to improve data reliability, efficiency and quality
Ensure systems meet business requirements and industry practices
Work effectively with the Business Intelligence and Data Solutions Architects, Data and BI Engineers to ensure that all approved development and deployment procedures are followed.

Qualifications:

Bachelor’s Degree in Computer Science, MIS, Computer Engineering or other Information Technology related degree
4+ years working with SQL Server or comparable relational database system
3+ years of extensive ETL development experience with SSIS and/or ADF
4+ years of experience troubleshooting within a Data Warehouse environment
Expert domain knowledge & experience in Data warehousing, encompassing data model design, dimensional modeling, naming conventions, cross-cutting concerns, common integration technologies, patterns & standards and emerging technologies.
Expert Knowledge of SQL skills to build, debug, and optimize (developing procedures, functions, SQL queries, etc.) and working with large data sets and to perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
2+ years SQL Server Database administration experience
Cloud experience (Azure) is highly preferred
Exposure and experience with Python, R, Hadoop, Azure, other Big data and advanced analytics
Knowledge of AI and ML developments/solutions/implementations
Experience with multiple programming languages (PowerShell scripting, C#, others) with basic scripting skills.
High level of technical aptitude

Inclusion and Equal Opportunity Employment

goeasy is an equal opportunity employer. In addition, goeasy is committed to providing accommodations for applicants upon request at any stage of the recruitment process in accordance with all legislative requirements throughout Canada. Please let us know if you require an accommodation due to a disability during any aspect of the recruitment process and we will work with you to address your needs.

Additional Information:

All candidates considered for hire must successfully pass a criminal background check, credit check, and validation of their work experience to qualify for hire. We thank all interested applicants, however we will only be contacting those for interview who possess the skills and qualifications outlined above.

Why should you work for goeasy?

To learn more about our great company please click the links below:

PAID1234	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DbNJQGwjC-NEA80-qlVyEGmSv34Xx0eW4t4u9rXSv_7_tlvMU2e-Gw_wrZ47s1zPCnrySU2yuEL07wtSyx7XOwYh5VPcVL7CxRhOYwHzR-CdmyBrwbeqommOn6y9FD_uKIYpUCb9-PC_XgwGzBDrPy7SD7JQndrQc_GJzDDokXuNbsI68yBiT0knA9aINNAIojsN4NT8ebDi8JYxasSNuKitM-Z7ybC3bwhKgeXKocDUZuB6AjxCycu2JBdvo6vk7iUvsLxJrxoOpmCfwx8nz_YMoBFjz4GwBjYD5H1GMYAnuJxTODcUf_4b7Zy2eveDvp5t3k4AAQaqLTaSwJMYIqAf6IF47GT35yzg4mGLYwk21jtPtxGct2yQUnGtGhkEm8hPhdZtfQed3_vioKZKwhr9SVpttUg-Hx3UOrMlLPhQ994z-iM92Sqg45S75k0qHcl6fw5BsQKFkhswfWySxQAc1_Vw_l6pBdeug20YOo9g==&p=13&fvj=0&vjs=3	Mississauga, ON	e8f01b72a171ac29	Data Engineer
2020081630	Toronto, ON	Our client, a boutique-size consulting firm, prides itself on being at the forefront of innovation in the Big Data space. Founded in 2010, they provide thought leadership and implementation excellence within the ever-growing data and analytics world.
They take pride in having some of the most highly trained and experienced consultants in the industry which translates into optimal value for their clients. They were one of the first companies to provide analytics and data as a service, via the cloud, as early as 2010. They strive to make sure their customers are well-positioned with the best technologies/tools in the industry, constantly evaluating new and existing technology partnerships. Some of the more prominent companies they have partnered with include; Snowflake, DataRobot, MicroStrategy, Informatica, Amazon AWS & Microsoft. They continue to invest in their most valuable resource, their people. They do this through extensive training both on the job and through various educational programs.
Due to growth, they are seeking to hire an Intermediate - Senior Integration Consultant.

Desired Skills and Experience
: University/College degree in Computer Science, Mathematics, Data Science and/or Relevant Degree
: 5+ years hands-on development, configuration, scripting and administration experience with Data Integration platforms. (i.e. Informatica, Talend, DataStage, SSIS)
: BI Experience (MicroStrategy, Looker, Tableau, PowerBI) considered a nice to have
: Extensive theoretical and practical knowledge of data warehousing principles/concepts and practical development experience in all areas of the data warehousing life cycle
: Experience with Data Management, ETL, Cloud Data (AWS), Data Integration
: Knowledge of OLAP-related principles and concepts
: Strong grasp of data modeling techniques and concepts (Normalized/Denormalized, Conceptual/Logical/Physical, Star, Snowflake, Data Vault)
: Strong knowledge and experience with relational databases such as Snowflake, SQL Server, Oracle (Advanced knowledge of reading and writing SQL, Performance analysis and tuning)
: Knowledge and experience with key Big Data technologies (Hive, Presto, Spark, Kafka, NoSQL databases, Semi-structured data access patterns (Json, Parquet, XML, etc.))
: Strong Python scripting skills
: Excellent communication skills
: Great problem-solving skills
: Leadership and good client management skills

Day to Day Activities Would Include
: Conduct relevant customer interviews to determine key business requirements and objectives
: Build appropriate analytical data models based on outcomes of user interviews
: Analyze and profile data systems to build source to target data mappings
: Review ETL performance and conducts performance tuning as required on mappings/workflows or SQL
: Administration and support of data integration infrastructure
: 2nd level on-call support of ETL services as required

You will be responsible for attaining the following goals:
: Attaining a minimum of 1 new accreditation/certification per year
: Spending 80% or more of their time on billable work
: Completing 90% or more of their agile delivery tasks on time
: Demonstrating competency in 1 new relevant technology every year	Copperstone Connect	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Ceb7pqR8iGmLuXIZQklF6pqed3xddONNM6Jjumj-ndyGK0kULQUmQ2REqfJ5KAzFVeK_es1WVg0s1sLgKW09UsvVRp0-dSizFQr1oNyK_VIqdjla00VU9GNVvu2x4v9p3hsL7NeF-6hBksbAaSgKazsaYTODGMKQQeVuYcxoOuxvFKuD0HRLD9ybgbcXTn1LOg85A399nevZh7QIe94JtI5oxmQkH7vIOgfxBBCL751qXORBmJrpeycAXoZgzLTAQwwkLIsuUVCRmzoi-UDqE-qAvaXLCJTXMPClX1iBLMdvJZwnRoufyn9KbRrPODAynzGHydlLcySeUh8csUPkp3B-yCy0yGHrDwbDHYM7fgJ9a6Grk8Ieu_5g54G_mT-AvG48EaOclOA5gxNDXS1KZyTycay4bcrCwYYQ_bsYFUpJxZLCisnXHQ4_fyooC8nvqu2otyieONXC3p273mp7CqeOmlcKnkPW1CsE1qAVIgpzUm1ikS4pXn04X0Cm40gdL6cVMBrlkKydzY2eew7V-ktfyfiP_YgN5Y2BcICs3X9RxHte7gE8D0e1xJEq7_9LH2gmoP9UWhDPQMJJD1qB0SUCsc7iYTXgFmuhsBe-gZ9IGhWcrBjEctvzi3n8EsQ-1OPdweAFziO5jYzm2ykA13CqGBIPNa0GraxwaU1Raijw==&p=14&fvj=0&vjs=3	54e0fcc38385e3a2	Data Engineer/Integration Consultant	2020-08-16T23:46:56.000Z
2020081631	Toronto, ON	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Bt274jo0r_e0cCCqhskoQvohXG3bfDAo8rIdE-Msu3yuVfyF2oiDFKHFOU-vxVulAfe8kPL7riFwoXa1bsfN-szjxO_8qUQWbPsJOnTg0EkutIoK2S4NNLL24JQh9e4DsE9hv907k2_zDbvj0a8WAj5oPSV5J0YhgrbSF1z_nzlOcOgBACdtIuenuiJ1Ue15k8DtZnSseghatiSG5kqdAO5hbT33VXvB8ypB3dS2Mij4Hx1OIC0C-BEysLJ-oim3mFsu-cQDNyBGQRmtv7qkaSCM-Sk7MGuYbPkYtX15yFtsCHM0N6Taxe2B_DwwKZvw5hxg3hTXBQpBkh-N0WTikbviBqX66CK6WeZw-6wi7fasdfTTK8CqdX--yGhUxo_MWjDCWAD6T4cRyjFq3CPJAOfl4bnzzaSdWqeWVUtT-attTgSWon5WTin7IZnKGbe1MpoR3L4Mu5nA==&p=0&fvj=0&vjs=3	The growing team here at Myticas is looking for an experienced Data Engineer who would be interested in a remote contract opportunity within the Ottawa, Toronto and Montreal regions.

Accountabilities:

Defining and reviewing security design requirements for cloud infrastructure and application components.
Evaluating architecture patterns from security perspective.
Building and implementing security controls to enable enforcement of compliance with Cloud Control Objectives, using custom Azure policies and integrated controls in DevOps processes

Requirements:

Strong Data Engineer w/ DevOps expertise + Azure Cloud Experience
Must know how to code and stand up scripts.
Experience with Data Digestions
Experience writing scripts to automate (infrastructure)
ARM Templating Expertise
Azure Synapse Expertise
Support developing automated DevOps processes and procedures for the following Azure components:
Azure Synapse (Azure DW) & Studio (private preview)
Azure Data Catalog Gen 2 (Babylon – private preview)
Azure Data Lake Storage Gen 2
Azure ML
ML Flow
Azure SQL Analysis Service
Azure Databricks
ADF data pipelines for data loading to AzSQL/Synapse
ADF data pipelines for connecting to on-prem data sources for data

Candidates looking to apply for this role are to send us an updated version of their resume in confidence. Our team will be sure to review all applicants and follow up accordingly at the conclusion of the review process.

Job is also known as: Data Engineer, DevOps Engineer, Azure Cloud Engineer, Azure Engineer, Cloud Engineer

INDMY	BHJOB15656_15145 - Data Engineer w/ DevOps - Azure Cloud	a32b7c599156a218	Myticas Consulting	2020-08-16T23:46:57.000Z
2020081632	Toronto, ON	TES - The Employment Solution	05f7bf2a7458c7fc	2020-08-16T23:46:58.000Z	Job Title- Data Engineer
Location- Downtown, Toronto
Type - Full Time Permanent
Salary - Negotiable + Benefits



Focus on data architecture, best practices, reliability, security, and complianceImprove and extend ETL, data processing, and analytics processesFacility with PowerBI, including creating dashboards and data sourcesDeveloping high complexity, fast performing SELECT queries.Developing T-SQL procedures, functions, triggers, jobs, scripts, etc.Development of Advanced T-SQL such as temporal tables, PIVOTs, recursive table expressions and more.Modeling and implementing Data Mart solution for Power BI analytics
Managing indexes, statistics, query plans alerts, database activity, and overall performance activity.In-depth experience working with relational databases, such as Microsoft SQL Server or PostgreSQLEnthusiasm for applying good data design, testing, documentation, and support practicesExperience building and optimizing data pipelines, architectures, and data setsKnowledge of message queueing, stream processing, and data stores/warehousesWorking knowledge of AWS products related to data engineeringBachelor's degree in Computer Science, Software Engineering or an equivalent
Excellent communication skills - both written and verbal; ability to speak in Spanish is a bonus

To apply please send an email to sheetalk@tes.net	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DQQnUBuQBSuyaIQhpC59TW7hrTbBg8v-nGtzzV8dunbQHh9VTp-k89gJ8q3B9UEXLHcYbAG67d8jpOK4E3ok8olcOhfyyiChWMI3BPvUFdra3ljwgLS_vFsJXlW6jHt2NpQMzd2p9F6BXGm9ND3feUN42yGy4tVXBU7xPsBm2zUh2tjgNyKT1VASrQOaDE7JhcaMidbT1VuvIHfjygFzAQGBKfaVEZRoI4j6gAhyFlwUG1GsJjYGWKsXmuG0sYLFVX0pwkZGJLwtnXGwEitrKsEcmGNhvEVrVvE0WQJLHK4APV5rbO8EZSo0OLqM-CLOX4CP-aicgC6H4xHYDGDU1roRjZE2gqN-SPz94nDqrTXTABwBebm4LqdMrX0z0vYIvvf99wv17T29_PrmQFgYHpZTK6TlttGP8CxvVgdHQls48pIe_vlo55v6oxeFfIltymkTPbtA7onKmWAYJFYtKfVWLs9yLEXUi5pXtyUlNnXxcyJCJh_hjfEybMR3e1GZvok2kgrUqtK2ncksRholR7bn6XrAcNCqX0SM4G7GBkp4Rxm9JhzAV1HjBKXb6ekTWMiAbR_N6vxRcfsTkg_pNcXgFFwJ6NOB9sWlp3rToR6erMBRIqbk8IZgdgqHooNhxkX2X7qhLK-O8B9daFImqdcN6qSBDhIgI2F6bHtZA_F-j9YM38DDbkotl94UgkiuYy2nmuQ3uqApJHEu0nPhI3SSf_LXs3BV0x5q6VJZkJ1rCaMayXtYAaO-IxrjOE2ZhhHSZS8DDx5DjiIRYL5_lNpj54ffyxM5GqCqnB82jOsPRJ8Tjw_WoFt5mpzVg4Um4Fj1v6_Jd1rLFPMKEOYK9GlwiAEGwQGoY=&p=1&fvj=0&vjs=3	Data Engineer
2020081633	Toronto, ON	Data/Software Engineer Co-op	67bbfacf7e5a2ca0	https://ca.indeed.com/rc/clk?jk=67bbfacf7e5a2ca0&fccid=e4a22b3d28be02c0&vjs=3	The Company

Smart Nora is a wellness tech company based in Toronto that has improved sleep and relationships for tens of thousands of couples around the world. Our debut product is the world's most comfortable snoring solution and has been listed on Oprah’s Favorite Things, as well as Good Morning America, TIME, TODAY and BBC to name a few.

The Team

We are an ambitious, tight-knit team with an open work environment and a self-directed approach. We typically work out of our office at King+Spadina or remotely in a weekly cadence of Monday kick-offs, Wednesday tea times, and Friday wrap-ups — taking a lot of pride in our individual work and holding one another accountable for producing great work.

Note: Currently we are all working from home throughout the COVID-19 pandemic.

The Product

Smart Nora is an over-the-counter, contact-free snoring solution relevant to the 40% of adults who snore. Smart Nora is loved by customers for its comfortable contact-free design that enables users to sleep without any attachments to their face or body.

The Role

We are looking for a Software / Data Engineer. This is a co-op placement, full-time from September - December ideally for a 4th-5th year student. We are flexible with an earlier starting date or longer term.

Location

Remote

What you would do

Support the current FW project conducted with our third party project partners by implementing structured testing and documentation
Build supporting tools in Python, Node, or other scripting languages to validate firmware and hardware
Actively participate in Mobile App development project with our third party project partners
Support acoustic performance optimization including microphone and codec gain settings, assisted by automated tests
Be part of the Product team developing the next generation Smart Nora device
Wrangle data and decipher meaning from quantitative tests and analytics
Write clear documentation

Our requirements

Comprehensive understanding of Software Development processes (SDLC)
Comprehensive understanding of Object Oriented Programming (OOP) principles
Experience with Pandas, R, Tableau, or other data processing/visualization tools
Experience with GitHub (send us your profile!)
Exposure to cloud (AWS) and APIs is a plus
Experience in audio processing is a plus
Experience in machine learning is a plus
Majoring in Software Engineering, Computer Science, Industrial & Systems Engineering, or related field.

How to Apply

Please apply using our online application form to include your resume and links to your LinkedIn, GitHub (if have), and Kaggle (if have) profiles.

Accommodations are available on request for candidates throughout the application process. Please let us know your needs so that we may accommodate. Email careers@smartnora.com if you would like to discuss this role before applying.	Smart Nora	2020-08-16T23:46:59.000Z
2020081634	Toronto, ON	Company Description

Square builds common business tools in unconventional ways so more people can start, run, and grow their businesses. When Square started, it was difficult and expensive (or just plain impossible) for some businesses to take credit cards. Square made credit card payments possible for all by turning a mobile phone into a credit card reader. Since then Square has been building an entire business toolkit of both hardware and software products including Square Capital, Square Terminal, Square Payroll, and more. We’re working to find new and better ways to help businesses succeed on their own terms—and we’re looking for people like you to help shape tomorrow at Square.

Job Description

Square began with a simple yet revolutionary piece of hardware—the Square Reader. We have expanded that vision to give our merchants access to the latest secure payment technologies (contactless and chip cards) and easy-to-use Point of Sale products. Today our hardware carries over $30 billion in transactions annually. Measuring the performance of our hardware, and the journey in getting the hardware to our merchants, is essential in ensuring that our merchants’ experience with Square is as seamless as possible.
We need your help to collect data about our hardware, organize that data, and make it available to analysts across Square. As a member of the Hardware data engineering team, you will define, develop, and manage a variety of data infrastructure components and pipelines so that our analytics teams and collaborators have trusted data to inform decisions and insights.
You will:
Join the small but mighty Hardware data engineering team that partners with internal Hardware data consumers to understand their needs and to source the right data sets to work on
Work in a remote environment that allows you to build scalable Hardware data pipelines and tools to ingest data from internal/external sources to our cloud data stack (Snowflake / AWS)
Develop data structures to support flexible analysis of this data, including creating data models, structuring optimized ETLs, designing validation scripts
Troubleshoot technical issues with platforms, data discrepancies, alerts, etc.
Be a voice between the Hardware team and Square’s data community. Help the hardware team’s data producers embrace best-practices, represent the hardware team’s voice in data infrastructure planning discussions.
Report into a data engineering manager on the Platform Infrastructure Engineering team

Qualifications

You have:
3+ years building and supporting reporting data systems built on columnar oriented RDBMS systems (e.g. Snowflake, BigQuery, Redshift, Vertica, etc.)
Strong experience building data pipelines from heterogeneous data sources (e.g. Event streams, Flat Files, RDBMS, REST APIs, SFTP, etc.) to support real-time operational and analytical workloads
Technical accomplishments working with SQL, ETL, and Apache Airflow; and knowledge of at least one mature programming language (Ruby (and Rails), Python, Java, Go, or similar)
Experience with Linux/OSX command line, version control software (git), and general software development
Experience with cloud based data tools and services (e.g. AWS Lambda/S3/Transfer, GCP Cloud Functions/GCS, Fivetran, etc.)
Additional Information

At Square, our purpose is to empower – within and outside of our walls. In order to build the best tools for the businesses and customers we support all over the world, we have to start at home with a workforce as diverse and empowered as our sellers. To this end, we take great care to evaluate all employees and job applicants equally, based on merit, competence, and qualifications. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, disability status, or any other characteristic protected by law. We encourage candidates from all backgrounds to apply. Applicants in need of special assistance or accommodation during the interview process or in accessing our website may contact us by sending an email to assistance(at)squareup.com. We will treat your request as confidentially as possible. In your email, please include your name and preferred method of contact, and we will respond as soon as possible.

Perks

At Square, we want you to be well and thrive. Our global benefits package includes:

Healthcare coverage
Retirement Plans
Employee Stock Purchase Program
Wellness perks
Paid parental leave
Flexible time off
Learning and Development resources	9b3e8aaf625b3563	Data Engineer - Hardware	https://ca.indeed.com/rc/clk?jk=9b3e8aaf625b3563&fccid=09abad886b83c501&vjs=3	Square	2020-08-16T23:47:00.000Z
2020081635	Toronto, ON	2020-08-16T23:47:01.000Z	https://ca.indeed.com/rc/clk?jk=02c0e1b67825ddaf&fccid=0b1a3a8f9692c945&vjs=3	02c0e1b67825ddaf	About KOHO
KOHO is a quickly scaling FinTech company backed by leading investors and advisors from around the world. We started KOHO because we believe in doing two things:

Democratizing access to the best financial products and giving everyone a great financial foundation.

Since our journey began 5 years ago, we’ve raised more the $60M, grown the KOHO Collective to over 100 employees and created accounts for more than 250,000 Canadians.

About the Role
As our newest Data Engineer, you would bring your database design sense and meticulous standards for clean data to the table. More than just understanding the strategic importance of data in a financial context, you’re eager to roll up your sleeves and build the infrastructure to support it. We value potential as much as we value experience, so if you think you’d be a great fit we’d love to hear from you.
Responsibilities
Maintaining clean and consistent access to all our data sources
Providing a solid foundation for calculating key business metrics
Maintaining data infrastructure to keep up with the product roadmap
Understanding data lineage and governance for a variety of data sources
Communicating updates and changes to the broader data team as well as contributing to and maintaining data-related documentation
Desired Skills & Experience
Minimum of three years of experience working with data
Able to design efficient and scalable cloud architectures
Experience writing complex SQL Queries
Knowledge of ETL patterns, including testing and maintenance
Familiar with relational / non-relational database approaches and knowing which to apply where and when
Understanding of hot/warm/cold data concepts and when each is appropriate
Understanding of event-driven and stream-based processing patterns
Ability to think holistically about uses of data, designing for ease of data access
Working knowledge of one of Python, Java
Experience with Kubernetes cluster management is an asset
Experience with data processing frameworks such as Apache Spark
Nice-to-Have Skills
Experience with Google Cloud Platform (BigQuery, Dataflow, PubSub)
Knowledge of GoLang
Joining the (lovely!) KOHO Team

We invest time and resources into making sure KOHO is as good as the people we hire. Our culture is one of collaboration, creativity, and diverse perspective. Here are some of the reasons we attract the best people:
Balance Your Life - Unlimited PTO, generous vacation, flexible WFH, and a lifestyle spending accountLevel Up - Access to an onsite certified performance coachReach Your Goals - Salary assessments twice a year, annual training allowanceThe KOHO Culture - We have won 5 "Great Place to Work ®" awards since 2019

The Fine Print

We are an equal opportunity employer and value diversity and uniqueness at our company.

KOHO is trusted with highly sensitive information. Upon joining the team, you may be asked to undergo security screening including a criminal record check.	Koho	Data Engineer
2020081636	Toronto, ON	https://ca.indeed.com/rc/clk?jk=ad526aacc6864e5d&fccid=1747adf6142beb48&vjs=3	2020-08-16T23:47:02.000Z	Data Engineer - Infosphere MDM	Are you looking for unlimited opportunities to develop and succeed? With work that challenges and makes a difference, within a flexible and supportive environment, we can help our customers achieve their dreams and aspirations.
Job Description
Are you looking for unlimited opportunities to develop and succeed? With work that challenges and makes a difference and a flexible and supportive environment, we can help our customers achieve their dreams and aspirations. We are currently seeking a Data Engineer to join one of our fast growing teams.
A day to day breakdown of the role would be as follows: 60% Hands On Development and Analysis, 20% Business partner interaction, 20% Agile Team Collaboration.
Additional responsibilities include:
Designs and implements data architectures in production environments
You will develop solutions that process data real-time from a variety of sources and make data available to multiple partners ranging from other IT applications to business teams to end customers.
You will work jointly with the Front-End team to ensure the definition of API endpoints is well understood and their requirements are supported
Translates business needs into data architecture solutions
Develops data landscape modernization architectures and roadmaps
Review and analyze the effectiveness and efficiency of existing systems and develop strategies for improving or further utilizing these systems.
Creates, reviews, updates and presents systems models, specifications, diagrams and charts to provide direction to system programmers and manages third party vendor (managed services) relationships.
Develops standards and processes for coding, deployment, testing, and governance.
You would be a good for this this role if:
You work jointly with the Front-End team to ensure the definition of API endpoints is well understood and their requirements are supported
You value collaborative development, this can be read in the quality and readability of your code and in your thorough code reviews
You are obsessed with accurate, reliable customer data
You think about QA and testability before you implement anything
You are mindful your work doesn’t stop when something works on your local computer: you work collaboratively with DevOps, think about migration, deployment, test coverage and documentation
You are promoting a culture of self-serve data analytics by minimizing technical barriers to data access and understanding
Qualifications for this role include:
BSc in Computer Science, Statistics, Informatics, Information System, Mathematics or equivalent quantitative field preferred
Well versed in software development methodologies, testing, release management, and maintenance. take pride in optimizing and cleaning code, and leave a program in a better shape than you found it
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL). A SQL veteran,
Good understanding of Master Data Management and IBM InfoSphere.
Good knowledge of DevOps and CI practices, ability to spec and setup the right environments and deployment procedures, proficiency with Docker and Jenkins.
Excellent troubleshooting skills. Experience performing root cause analysis on internal and external data and processes to answer specific business questions and find opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Understanding of relational and warehousing database technology working with at least one of the major databases platforms, preferably DB2, SQL Server
Practical experience with big data processing frameworks and techniques such as Azure, Hortonworks, Spark, Hive, PCF,
Solid working knowledge of data processing tools using SQL, Spark, Python or similar open source and commercial technologies
Knowledge of Java/Scala especially in relation to big data open source software preferred.
ETL experience is a requirement.
Agile project methodologies
Collaborative attitude, willingness to work with team members; able to coach, participate in code reviews, share skills and methods
Constantly learns from both success and failure
Good organizational and problem-solving abilities that enable you to manage through creative abrasion
Good verbal and written communication; effectively articulates technical vision, possibilities, and outcomes
Experiments with emerging technologies and understanding how they will impact what comes next.
What about Perks?
Manulife has lots of perks including, but not limited to:
Competitive compensation
Retirement Savings Accounts including a RPP (Pension Plan), RRSP (Retirement Savings Plan), and TFSA (Tax Free Savings account)
Manulife Share Ownership Program with employer matching
Customizable Benefits Package including Health, Dental, Vision, and 100% of Mental Health expenses
Financial support for ongoing training, learning, and education
Monthly Innovation Days (Hackathons)
Wearing jeans to work every day
An abundance of career paths and opportunities to advance
This is a full-time permanent role located in Toronto, Ontario.
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status

If you are ready to unleash your potential it’s time to start your career with Manulife/John Hancock.
About Manulife
Manulife Financial Corporation is a leading international financial services group that helps people make their decisions easier and lives better. With our global headquarters in Toronto, Canada, we operate as Manulife across our offices in Canada, Asia, and Europe, and primarily as John Hancock in the United States. We provide financial advice, insurance, and wealth and asset management solutions for individuals, groups and institutions. At the end of 2019, we had more than 35,000 employees, over 98,000 agents, and thousands of distribution partners, serving almost 30 million customers. As of March 31, 2020, we had $1.2 trillion (US$0.8 trillion) in assets under management and administration, and in the previous 12 months we made $30.4 billion in payments to our customers. Our principal operations are in Asia, Canada and the United States where we have served customers for more than 155 years. We trade as 'MFC' on the Toronto, New York, and the Philippine stock exchanges and under '945' in Hong Kong.
Manulife is an equal opportunity employer. We strive to attract, develop and retain a workforce that is as diverse as the customers we serve and to foster an inclusive work environment that embraces the strength of cultures and individuals. We are committed to fair recruitment, retention and advancement and we administer all of our practices and programs based on qualification and performance and without discrimination on any protected ground.
It is our priority to remove barriers to provide equal access to employment. A Human Resources representative will consult with applicants contacted to participate at any stage of the recruitment process who request any accommodation. Information received regarding the accommodation needs of applicants will be addressed confidentially.	ad526aacc6864e5d	Manulife
2020081637	Toronto, ON	Req Id: 255052

At Bell, we do more than build world-class networks, develop innovative services and create original multiplatform media content – we’re revolutionizing how Canadians communicate.
If you’re ready to bring game-changing ideas to life and join a community that values bold ideas, professional growth and employee wellness, we want you on the Bell team.
Bell Residential and Small Business provides the most innovative, industry-leading Internet, TV and Smart Home products and services to Canadians. Whether we’re tackling a new challenge, implementing a big idea, or selling an innovative, new feature our team puts in the effort to be Canada’s #1 communications company.

The BRS Business Intelligence team collaborates with partners across the organization and provides strategic guidance supported with data. We work with the latest and greatest BI technologies like Teradata, SAS, Hadoop and Microstrategy. We enable better results through an integrated market view of customers and competition.

Reporting to the Senior Manager of Data Science, the Senior Data Engineer is responsible for the management and optimization of our data repositories used for our marketing campaigns, reporting and analytics. They will work as part of a team of highly competent BI professionals who like to get things done.

In an ever-changing and dynamic environment, the candidate should be flexible, have strong time management skills and be capable of balancing many projects simultaneously with short deadlines for completion. The Senior Engineer will work in a cross-functional team environment and will be asked to coach other Engineers and provide technical support and guidance, ensuring industry best practices are employed.

Primary Responsibilities:
The Senior Developer will be responsible for processes and data residing in our Teradata SAS and Hadoop environments. They will execute the following tasks:
Meet with Reporting, Marketing and Analytics teams to understand business needs and relate these to data requirements;
Design solutions, support ETL Developers and Business Analysts concerning the implementation of data architecture and ensure timely and successful project completion;
Lead and manage projects with minimal guidance to determine impact on existing data and implement new data structure as required;
Analyze, conceptualize, build and optimize complex data systems;
Coach and collaborate with team members to enhance expertise;
Support ad-hoc requests and investigations and suggest/implement solution;
Take ownership and accountability for deliverables and committed deadlines, communicate proactively and escalate when required;
Develop and modify ETL processes taking into consideration long-term implications;
Respond to ad-hoc requests in a timely manner;
Investigate data/process anomalies; propose and implement solutions;
Help maintain the integrity and security of the company database;

Competencies / Skills:
A minimum of 8 years experience in ETL development in Teradata and/or PL/SQL;
SAS experience
Experience in data warehousing which include data mapping and transformation, data architecture, data source analysis and data profiling;
SQL programming experience: creating complex queries, tables, views, and procedures;
Advanced knowledge of the following software: Enterprise Guide, Excel, PowerPoint, Word;
Hadoop knowledge is an asset;
Knowledge of the telecommunications business is an asset;
Demonstrable history of cross-team collaboration;
Advanced technical and non-technical communications skills;
Self motivated. Ability to research situations, identify the key issues, and then gain approval to address the opportunity from upper management.
Extremely rigorous with regards to the data and information provided;
Ability to work on their own, with a minimum of supervision within a dynamic environment;
Ability to work and manage multiple projects simultaneously;
Ability to diagnose business problem, document, resolve, plan and deliver solution in short time;
Well organized;
Invested in continual self-development to stay current with evolving technological landscape;
Customer driven.

Bilingualism is an asset (English and French); adequate knowledge of French is required for positions in Quebec.

Additional Information:
Position Type: Management
Job Status: Regular - Full Time
Job Location: Canada : Ontario : Don Mills || Canada : Ontario : Toronto || Canada : Quebec : Montreal || Canada : Quebec : Verdun
Application Deadline: 08/21/2020

Please apply directly online to be considered for this role. Applications through email will not be accepted.

At Bell, we don’t just accept difference - we celebrate it. We’re committed to fostering an inclusive, equitable, and accessible workplace where every team member feels valued, respected, and supported, and has the opportunity to reach their full potential. We welcome and encourage applications from people with disabilities.

Accommodations are available on request for candidates taking part in all aspects of the selection process. For a confidential inquiry, simply email your recruiter directly or recruitment@bell.ca to make arrangements. If you have questions regarding accessible employment at Bell please email our Diversity & Inclusion Team at inclusion@bell.ca.

Created: Canada, ON, TorontoLI-SP1

Read more about why Bell is considered one of Canada's Top 100 Employers.	2020-08-16T23:47:03.000Z	https://ca.indeed.com/rc/clk?jk=7ee27d40cde621d1&fccid=6d9b8e75dfffb17e&vjs=3	Senior Data Engineer	7ee27d40cde621d1	BELL RESIDENTIAL SERVICES
2020081638	Do you have what it takes to win?
Like a championship team, a leading global sports brand is built with a solid foundation of players at all levels who have an unending desire and dedication not only to succeed, but also to win. At Peak Achievement Athletics, our championship team is deeply committed to developing the most innovative sports equipment in the industry and we are always looking to strengthen our roster with talented players.
Want to join our team as a Data Engineer?
The Data Engineer is responsible for developing and delivering a large-scale database platform that can efficiently enable our analysts to transform data into intelligence. The system design will dramatically reduce the time spent on data preparation tasks and have the inherent ability to scale with business growth and complexity. The Data Engineer will become intimately familiar with the architecture of Bauer Hockey systems & enterprise data structures, and be responsible for diving deep into code while simultaneously developing UI solutions for the Sales Operations super users. The Data Engineer will liaise with multiple technical teams and business teams across international geographies. The database management system will operate on a global scale driving automation and scaling for the wider organization. The role of the Data Engineer will include incorporating data management best practices into the scoping, design and development of the database. The Data Engineer will also be responsible for effectively organizing testing, implementation, support, and the development of user and technical documentation such as guidelines or instructions as necessary.
Qualifications:
Bachelor's degree in Computer science or a related field (MBA a plus) with 3+ years of practical work experience or the equivalent combination of education and experience.
Comprehensive knowledge of database technologies including, but not limited to Google Cloud, AWS, SQL, Hadoop, SAP HANA, and Alteryx.
Hands-on experience developing platforms that translate big data into business insight.
Strong knowledge of data structures and operating systems.
Knowledge of database maintenance and administration techniques.
Experience with Machine Learning languages is a plus.
Desire to work in a high-paced environment.
Strong problem-solving skills and the ability to work independently.
Strong written and verbal communication skills.

Interested yet? Good. Us too. We're pretty sure you'll want to know we offer one of the most generous benefits packages around. Things like a 401(k) retirement plan, casual work environment, and a host of other perks we don't have room to mention here.

We're interested in learning more about you and appreciate you taking the time to apply online at www.bauer.com.
Only those persons chosen for an interview will be contacted.
Peak Achievement Athletics is committed to employing a diverse workforce.	Mississauga, ON	Easton Diamond Sports	https://ca.indeed.com/rc/clk?jk=dee5e212401d9e7d&fccid=a73cf0f26ba2d750&vjs=3	Data Engineer	2020-08-16T23:47:04.000Z	dee5e212401d9e7d
2020081639	Toronto, ON	https://ca.indeed.com/rc/clk?jk=e8f693d8ceb0d39b&fccid=f9ed656b63c620fc&vjs=3	e8f693d8ceb0d39b	The Property Registry	Big Data Engineer	2020-08-16T23:47:04.000Z	Teranet is seeking an experienced Big Data Engineer to support its Data Analytics program. In this role, the successful candidate will be responsible for the ongoing monitoring and maintenance of Teranet’s data lake and the in-house developed and third-party software tools used to maintain it. In addition, the Big Data Engineer will work with consumers of the Teranet data lake to develop data views and data feeds according to their business needs and requirements. As part of this effort, the Big Data Engineer will be responsible for publishing data sets and views to the Teranet Data Analytics visualization platform so the data are accessible to downstream consumers. The Big Data Engineer will also collaborate with Teranet’s security team to ensure proper data access controls are in place, data is properly secured, and access activities are auditable.
Key responsibilities include:
Maintain and monitor Teranet’s data lake feeds
Manage, maintain and oversee Teranet’s data lake Big Data platform (Cloudera)
Prepare the data views for downstream data lake consumers (e.g. Data Scientists and Analysts)
Optimization of Teranet’s Big Data environment, applications and data views
Design, build and test data queries for data views and feeds for downstream data lake consumers
Design, build and integrate additional data sources into the Teranet data lake
Design, build and test analysis/data models to support downstream data lake consumers
You are someone who:
Continuously seeks to improve your knowledge of data analytics technologies and best practices
Strives to understand business drivers and strategy in order to understand business requirements
Takes a collaborative approach to assignments and works well with others
Is a good communicator with strong written and oral communication skills
Clearly documents your work so that it can be readily understood by others
Takes ownership and accountability for your assignments and responsibilities
Takes pride in delivering detail-oriented, thoughtful, thorough, and quality results
Key qualifications:
Bachelor’s degree in Computer, a quantitative field, or equivalent practical experience
3-4 years working with Hadoop related technologies (Spark, Hive, MapReduce, Sqoop, Impala)
Advanced software development experience with Scala, Java, SQL and Python
Familiarity with visualization and statistical modeling tools and languages (Tableau, R)
2020081640	Toronto, ON	Our client located in downtown Toronto is looking for a Big Data Engineer who will be responsible for developing new system stacks and tools for big data ingestion, processing, and analytics. This position is perfect for a developer whose passion is to apply cutting-edge technologies to solve complex business and engineering problems. This individual will work on a team of talented engineers responsible for the full life-cycle of production systems, software, tools, and flows.

Responsibilities

Design, develop, and maintain the software and systems that make up the data platform that runs our entire business
Participate in multi-disciplinary projects
Partner with the Data Science and Engineering teams who use our platform to by diagnosing, predicting and correcting scaling problems
Contribute to our teams growing set of development platforms, tools, and processes

Required Skills

Hands-on experience with Big data technologies (HBASE, HDFS, SPARK, and/or HADOOP)
Demonstrated proficiency with Spark, Scala, Python
Experience in building stream processing systems using spark streaming or Storm.
Experience in integration of data from multiple sources
Experience with NoSQL cluster databases, such as HBase, Cassandra, Druid
Experience with various messaging systems such as KAFKA, or RabbitMQ
Proficient understanding of distributed computing principles.

Preferred Skills

Experience with other highly scalable, low latency big data systems is a plus
Experience with Hortonworks / Cloudera distributions is a plus
Experience with Cloud technology and containerization (docker / kubernetes) is a plus

Who you are

Able to learn and apply new technologies quickly
Proven excellent problem-solving abilities
Able to work both independently and as part of a team
Able to multi-task in a dynamic environment
Have Excellent verbal and communication skills

Qualifications

Software development experience using mainstream languages such as Java, Scala and/or C++
Experience architecting, deploying and operating mission-critical big data clusters.	2020-08-16T23:47:05.000Z	d5e196c24ff9d6f9	https://ca.indeed.com/rc/clk?jk=d5e196c24ff9d6f9&fccid=d5eebd59dbfe9145&vjs=3	Technology Development Corp	Data Engineer
2020081641	Software Engineer, Data	Toronto, ON	https://ca.indeed.com/rc/clk?jk=a677b4cff4d9abf1&fccid=9e88642c8c797c72&vjs=3	a677b4cff4d9abf1	Fathom Health	We’re on a mission to understand and structure the world’s medical data, starting by making sense of the terabytes of clinician notes contained within the electronic health records of the world’s largest health systems.

We’re seeking exceptional Data Engineers to work on data products that drive the core of our business-a backend expert able to unify data, and build systems that scale from both an operational and an organizational perspective.
As a Data Engineer you will:
Develop data infrastructure to ingest, sanitize and normalize a broad range of medical data, such as electronics health records, journals, established medical ontologies, crowd-sourced labelling and other human inputs.
Build performant and expressive interfaces to the data
Build infrastructure to help us not only scale up data ingest, but large-scale cloud-based machine learning

We’re looking for teammates who bring:
Experience building data pipelines from disparate sources
Hands-on experience building and scaling up compute clusters
Excitement about learning how to build and support machine learning pipelines that scale not just computationally, but in ways that are flexible, iterative, and geared for collaboration.
A solid understanding of databases and large-scale data processing frameworks like Hadoop or Spark. You’ve not only worked with a variety of technologies, but know how to pick the right tool for the job.
A unique combination of creative and analytic skills capable of designing a system capable of pulling together, training, and testing dozens of data sources under a unified ontology.

Bonus points if you have experience with:
Developing systems to do or support machine learning, including experience working with NLP toolkits like Stanford CoreNLP, OpenNLP, and/or Python’s NLTK.
Expertise with wrangling healthcare data and/or HIPAA.
Experience with managing large-scale data labelling and acquisition, through tools such as through Amazon Turk or DeepDive.	2020-08-16T23:47:06.000Z
2020081642	Toronto, ON	CATO - Big Data Engineer	https://ca.indeed.com/rc/clk?jk=20480654089beb5a&fccid=c2a63affe8751868&vjs=3	CAPCO	2020-08-16T23:47:06.000Z	Big Data Engineer
LOCATION: TORONTO

Capco – The Future. Now.

Capco is a distinctly and positively different place to work. Much more than consultants, we are active participants in the global financial services industry. Our passionate business and technology professionals enjoy a unique environment where they are actively encouraged to apply intellect, innovation, experience and teamwork. We are dedicated to fully supporting our world class clients as they respond to challenges and opportunities in: Banking, Capital Markets, Finance Risk & Compliance, Insurance, and Wealth and Investment Management. Experience Capco for yourself at capco.com

Let’s Talk About You

You want to Own Your Career. You’re serious about rising as far and as fast as your work and achievements can take you. And you’re ready to write the next chapter of your career story: a challenging and rewarding role as a Capco Big Data Engineer.


Let’s Get Down To Business

Capco is looking for talented, innovative, and creative people to join our development team to work on a number of projects and applications with a Data focus within the Digital practice.

Fitting that description, you will also need to be personally motivated to work in a team where clients become colleagues too.

Responsibilities

Produces high quality complex, deliverables with minimal input from stakeholders
Manage full software lifecycle for medium complexity projects from requirements, to design, to implementation, to testing
Develop and maintain back end solutions using cutting edge technologies and products
Work with Scrum Masters and product owners to priorities and deliver solutions using an Agile environment
Build reusable code and libraries for future use and follow emerging technologies
Mentor and train junior developers

Education/Experience

Bachelor’s degree (preference given to Computer Science, Engineering, Gaming and STEM-based majors) or equivalent experience
Five (5) or more years of experience as a Full-stack Data Engineer/developer on Data driven projects
Strong understanding of the full development lifecycle including requirements, architecture, design, development and testing
Strong development experience with Scala/Spark
Experience working with REST APIs/Springboot.
Familiarity working with Java and Hive.
Ability to balance competing priorities in a very dynamic and fast-paced environment
Excellent detail-oriented, problem solving skills and the ability to quickly learn and apply new concepts, principles and solutions
Must have excellent communication skills (verbal and written)

Show Us What You’ve Got

It will be very useful if you have some or all of the following skills:

Understanding of big data and distributed programming concepts
Experience working with ASW, GCloud, Docker, Kubernetes
Experience working with Microservices, CQRS, EventSourcing
Experience working with Spring, Akka, Spark
Experience working with Reactive Streams (Rx, Akka, Reactor)
Strong organizational and communication skills
Experience working in an Agile environment
Experience working with code versioning tools
Experience working with build, packaging and continuous integration tools and frameworks


Professional experience is important. But it’s paramount you share our belief in disruptive innovation that puts clients ahead in a tough market. From day one, your key skill will be to perceive new and better ways of doing things to give your clients an unfair advantage.


Now Take the Next Step

If you’re looking forward to progressing your career with us, then we’re looking forward to receiving your application.

Capco is well known for its thought leadership and client-centric model that distinguishes it from other consulting firms. Capco’s strong technology and digital knowledge base, it’s global experience of the Financial Service enables us to deliver projects from strategy through to delivery. We are committed to providing new areas of expertise from which our clients will greatly benefit. We have:

Access to industry-focused talent globally
Ability to leverage best-of-breed, innovative products and solutions for complex architecture and large-scale transformation
Extended global geographic market reach
Ability to capitalize on our client footprint and deep domain expertise within financial services

For more information about Capco, visit www.Capco.com.

Capco is an equal opportunity employer. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, marital status, genetic information, national origin, disability, veteran status, and other protected characteristics.	20480654089beb5a
2020081643	goeasy	If you are looking to join one of Canada’s fastest growing companies, goeasy Ltd. is the place for you! Recognized as one of Canada’s Most Admired Corporate Cultures, one of Canada’s Top 50 Fintech’s and one of North America’s Most Engaged Workplaces, we want the best and brightest to join our team.

We are a publicly traded company on the TSX with over 4000% shareholder return since 2001, goeasy operates two main business units. easyfinancial is our consumer lending business that offers secured and unsecured installment loans of up to $35,000 and easyhome is Canada’s largest merchandise lease-to-own company. It is our mission to provide everyday Canadians the chance for a better tomorrow, today by giving them access to the credit they need and by offering them a second chance when they have been turned down by banks and traditional lenders. With a retail network of nearly 400 locations across Canada and over 1900 employees, we are able to build lasting relationships with our customers as we help them rebuild their credit and graduate towards prime rates and a brighter financial future.

The Data Engineer on the Data Science and Business Insights team will build, integrate data from various resources, manage data in goeasy operational data store and enterprise data warehouse. This position will develop ETL (Extract, Transform and Load) with various tools on large datasets to ensure data is easily accessible, works smoothly, as well as maintain and expand the data warehouse for reporting and analysis. The Data Engineer will also work closely with the data architect on the design and architecture of our enterprise data warehouse.

Responsibilities:

Develop data set processes for data modeling, mining and production
Develop and maintain ETL processes using SSIS, Scripting and data replication technologies
Participate in development of datamarts for reports and data visualization solutions
Research opportunities for data acquisition and new uses for existing data
Integrate new data management technologies and software engineering tools into existing structures
Support the translation of business requirements for data acquisition/manipulation and provide detailed specifications that can be passed downstream for use
Develop detailed technical specifications and operational support documentation in collaboration with Business Systems Analysts, BI Engineers and Architects.
Identify and communicate technical problems, process and solutions
Create Ad-Hoc queries and reports as needed along with providing on-going analytical support for these requests
Assist in the collection and documentation of user’s requirements
Ensure that existing business processes dependent on the ODS/EDW are monitored and respond quickly to bug fixes, enhancement requests and production ETL related issues.
Dealing with the database users on a daily basis to ensure that problems are dealt with promptly and that appropriate fixes are made to resolve any problems.
Recommend ways to improve data reliability, efficiency and quality
Ensure systems meet business requirements and industry practices
Work effectively with the Business Intelligence and Data Solutions Architects, Data and BI Engineers to ensure that all approved development and deployment procedures are followed.

Qualifications:

Bachelor’s Degree in Computer Science, MIS, Computer Engineering or other Information Technology related degree
4+ years working with SQL Server or comparable relational database system
3+ years of extensive ETL development experience with SSIS and/or ADF
4+ years of experience troubleshooting within a Data Warehouse environment
Expert domain knowledge & experience in Data warehousing, encompassing data model design, dimensional modeling, naming conventions, cross-cutting concerns, common integration technologies, patterns & standards and emerging technologies.
Expert Knowledge of SQL skills to build, debug, and optimize (developing procedures, functions, SQL queries, etc.) and working with large data sets and to perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
2+ years SQL Server Database administration experience
Cloud experience (Azure) is highly preferred
Exposure and experience with Python, R, Hadoop, Azure, other Big data and advanced analytics
Knowledge of AI and ML developments/solutions/implementations
Experience with multiple programming languages (PowerShell scripting, C#, others) with basic scripting skills.
High level of technical aptitude

Inclusion and Equal Opportunity Employment

goeasy is an equal opportunity employer. In addition, goeasy is committed to providing accommodations for applicants upon request at any stage of the recruitment process in accordance with all legislative requirements throughout Canada. Please let us know if you require an accommodation due to a disability during any aspect of the recruitment process and we will work with you to address your needs.

Additional Information:

All candidates considered for hire must successfully pass a criminal background check, credit check, and validation of their work experience to qualify for hire. We thank all interested applicants, however we will only be contacting those for interview who possess the skills and qualifications outlined above.

Why should you work for goeasy?

To learn more about our great company please click the links below:

PAID1234	2020-08-16T23:47:07.000Z	Mississauga, ON	e8f01b72a171ac29	Data Engineer	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DbNJQGwjC-NEA80-qlVyEGmSv34Xx0eW4t4u9rXSv_7_tlvMU2e-Gw_wrZ47s1zPCnrySU2yuEL07wtSyx7XOwYh5VPcVL7CxRhOYwHzR-CdmyBrwbeqommOn6y9FD_uKIYpUCb9-PC_XgwGzBDrPy7SD7JQndrQc_GJzDDokXuIVipM_GznS0V_N6MHN_2nrOb4iduw0eaeNejhlwPijwGp-t829cxlcdC2dmFUnVC5xPKTQe3nk7bl9A1nmSSW71-qvbp6jAkrFQZsc0K85xbmvmLyhHrmmKPGb7LvSkChBSSqx16AJj6qQ_WnjwBVgcaftldEV8Bac6uDCBOVLEd3gHuTnFjGNsKAY3rOJZin4ltxW1RbUAc931ZQ--D3Z8n4o_WVZnqGvx-zja1M4K-Cr_T1Oo9dp1cM0FjvTUxFtZDPcxQGp9TXHxazFcXZISFMXU2xCy2gA-ytywY7trqzePjpzX75eooSxkj87kXA==&p=12&fvj=0&vjs=3
2020081644	Data Engineer (Intermediate 4-6 years) - Contract - Remote during COVID	Role: Data Engineer (Intermediate 4-6 years)Structure: Contract (6 months initially)Location: North York, ONHours: Monday - Friday 40 hours (remote during COVID-19)Pay: $50.00 p/h inc.The role: We are looking for Intermediate Data Engineers who have roughly between 4-6 years experience who ideally has experience moving data from Google cloud over to Big query.However, if you are someone who has good experience with ETL and Data Pipeline Design & Data Validation experience and can script in Python and create strong SQL Queries then this will also be a potential match for this role.Ideal Qualifications to be successful in this role: - Degree in Computer Science, Engineering, Information Systems (or equivalent combination of skill and experience)- 4-6 years of experience working with data architecture projects- Excellent SQL coding and experience with a broad array of development tools and platforms including exposure to a big data environment tools/languages, such as SQL, R, SAS, Python, etc.- Experience creating relational database design and data models- Experience with various analytical data platforms and technologies- Experience with using REST API, Cloud (GCP Ideally)- Experience in custom or structured data integration design, implementation, and maintenance- Experience with business intelligence tools such as Qlikview, Tableau and Microstrategy (Nice to have for Data Visualizations)- Previous experience working in retail or eCommerce is highly preferred- Ability to curate data to tell stories and provide business insightsPlease apply with an updated resume and ensure the required skills you are able to speak to for this position are included.For more roles like this please go to www.corgta.com/find-a-job/Job Type: Full-timeSalary: $45.00-$50.00 per hour	CorGTA	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Dkh866153otbRJ8nVMuEXnXPd315JS377__3VyD6HnNQkmcaOHMemmy5pLrNV6j0WaVWEWWabnp4PP6-2u77DLfT_FCIGvCW8CQmQW140Z7EVU32Jnwl42FALsxnbc4n5M6FOTfoW2OoTMOcGG21HZGOoXL7fWT8eaT67VlcClh4sHQcJsHCX_YFQRPO0Ll4I5ZY26R-f99XRBl_I5GCxJIeaN0-BquFKGbdhGQQPDJQXQohld5HQQbwx3u_fiSyh94njMr4gBqV0c6BaH5SV1oNbfeKv_q7FeQOmjr7SGV7LwfyVxPTx95NC7twTqi5T1NC6ZY5gHFeur8pDLRB_59immQZl20q6W3f-FUzVx0Xz3jTy0s23XuMcaXZR6Xd05lU1kKi7BkFii8NfSThzJIJyK8fylZ6Up85bB70NByf5DsXZZYkbed0xBk4Qwhy5IA8gHujE-LQTfUqAbQJIW4B4niXfSiu6Vr5mjgQ2LUU18tLr0X0QT&p=13&fvj=1&vjs=3	North York, ON	997673b8effa647f	2020-08-16T23:47:08.000Z
2020081645	Toronto, ON	Our client, a boutique-size consulting firm, prides itself on being at the forefront of innovation in the Big Data space. Founded in 2010, they provide thought leadership and implementation excellence within the ever-growing data and analytics world.
They take pride in having some of the most highly trained and experienced consultants in the industry which translates into optimal value for their clients. They were one of the first companies to provide analytics and data as a service, via the cloud, as early as 2010. They strive to make sure their customers are well-positioned with the best technologies/tools in the industry, constantly evaluating new and existing technology partnerships. Some of the more prominent companies they have partnered with include; Snowflake, DataRobot, MicroStrategy, Informatica, Amazon AWS & Microsoft. They continue to invest in their most valuable resource, their people. They do this through extensive training both on the job and through various educational programs.
Due to growth, they are seeking to hire an Intermediate - Senior Integration Consultant.

Desired Skills and Experience
: University/College degree in Computer Science, Mathematics, Data Science and/or Relevant Degree
: 5+ years hands-on development, configuration, scripting and administration experience with Data Integration platforms. (i.e. Informatica, Talend, DataStage, SSIS)
: BI Experience (MicroStrategy, Looker, Tableau, PowerBI) considered a nice to have
: Extensive theoretical and practical knowledge of data warehousing principles/concepts and practical development experience in all areas of the data warehousing life cycle
: Experience with Data Management, ETL, Cloud Data (AWS), Data Integration
: Knowledge of OLAP-related principles and concepts
: Strong grasp of data modeling techniques and concepts (Normalized/Denormalized, Conceptual/Logical/Physical, Star, Snowflake, Data Vault)
: Strong knowledge and experience with relational databases such as Snowflake, SQL Server, Oracle (Advanced knowledge of reading and writing SQL, Performance analysis and tuning)
: Knowledge and experience with key Big Data technologies (Hive, Presto, Spark, Kafka, NoSQL databases, Semi-structured data access patterns (Json, Parquet, XML, etc.))
: Strong Python scripting skills
: Excellent communication skills
: Great problem-solving skills
: Leadership and good client management skills

Day to Day Activities Would Include
: Conduct relevant customer interviews to determine key business requirements and objectives
: Build appropriate analytical data models based on outcomes of user interviews
: Analyze and profile data systems to build source to target data mappings
: Review ETL performance and conducts performance tuning as required on mappings/workflows or SQL
: Administration and support of data integration infrastructure
: 2nd level on-call support of ETL services as required

You will be responsible for attaining the following goals:
: Attaining a minimum of 1 new accreditation/certification per year
: Spending 80% or more of their time on billable work
: Completing 90% or more of their agile delivery tasks on time
: Demonstrating competency in 1 new relevant technology every year	Copperstone Connect	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Ceb7pqR8iGmLuXIZQklF6pqed3xddONNM6Jjumj-ndyGK0kULQUmQ2REqfJ5KAzFVeK_es1WVg0s1sLgKW09UsvVRp0-dSizFQr1oNyK_VIqdjla00VU9GNVvu2x4v9p3hsL7NeF-6hBksbAaSgKazsaYTODGMKQQeVuYcxoOuxvFKuD0HRLD9ybgbcXTn1LNyIPJPItw4q7uGizlrVAQgVmOBHIjD67JlLj0rRplqACKV0usL1WP86yD7gz_6YojJ8sD9froVtCC-wit7HI49shJNlJ5Ko1b1djDIgBTRrcBDevuo_onrF-qZXuX5rxxWvbEG8VznYkQpcRlQriO8gNzrnlbTDymMohOz4UynEnU6DdnUEXpuHJp-yw5Bm1htuZ6H177l6QOIYMFCaKm-lTwUMIrxqqLWUxH0xd4adVn4GiFXlIxeuTeWuHFt1O8hjaonI4Hk2RX3NPZhZdb7dg5ucMdJRLj5lAvyfhSWGGnpdP_66yQEGEfBkGDCZX9T1-w3NorUcp_9-XBpd2nlsEOlqynkxvReRzIuuPkf8_Od7Qk7ssfgMSpc5s7meEzqXaRbnMbIb-BojbYOptsBpOQvU2ZT1OJwDp1r5qXN6sJJwCKc7bZA0UKVpWt-DXc7Lxzpb5oQJ-RbNKxKzbNLg77oq1yQ9D7QY22N2lvc1w==&p=14&fvj=0&vjs=3	54e0fcc38385e3a2	Data Engineer/Integration Consultant	2020-08-16T23:47:09.000Z
2020081646	Data Engineer (Intermediate 4-6 years) - Contract - Remote during COVID	Role: Data Engineer (Intermediate 4-6 years)Structure: Contract (6 months initially)Location: North York, ONHours: Monday - Friday 40 hours (remote during COVID-19)Pay: $50.00 p/h inc.The role: We are looking for Intermediate Data Engineers who have roughly between 4-6 years experience who ideally has experience moving data from Google cloud over to Big query.However, if you are someone who has good experience with ETL and Data Pipeline Design & Data Validation experience and can script in Python and create strong SQL Queries then this will also be a potential match for this role.Ideal Qualifications to be successful in this role: - Degree in Computer Science, Engineering, Information Systems (or equivalent combination of skill and experience)- 4-6 years of experience working with data architecture projects- Excellent SQL coding and experience with a broad array of development tools and platforms including exposure to a big data environment tools/languages, such as SQL, R, SAS, Python, etc.- Experience creating relational database design and data models- Experience with various analytical data platforms and technologies- Experience with using REST API, Cloud (GCP Ideally)- Experience in custom or structured data integration design, implementation, and maintenance- Experience with business intelligence tools such as Qlikview, Tableau and Microstrategy (Nice to have for Data Visualizations)- Previous experience working in retail or eCommerce is highly preferred- Ability to curate data to tell stories and provide business insightsPlease apply with an updated resume and ensure the required skills you are able to speak to for this position are included.For more roles like this please go to www.corgta.com/find-a-job/Job Type: Full-timeSalary: $45.00-$50.00 per hour	CorGTA	2020-08-16T23:47:10.000Z	North York, ON	997673b8effa647f	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Dkh866153otbRJ8nVMuEXnXPd315JS377__3VyD6HnNQkmcaOHMemmy5pLrNV6j0WaVWEWWabnp4PP6-2u77DLfT_FCIGvCW8CQmQW140Z7EVU32Jnwl42FALsxnbc4n5M6FOTfoW2OoTMOcGG21HZGOoXL7fWT8eaT67VlcClh4sHQcJsHCX_-V9G0OyffIXO3TNiF1tfd5wX9jwBjJTSoQjXQ8ugZGObGUfCXXsQxMTyKn8EZgqlefFO6sauyX_Y1Wg9yGkZIoFDw_zzCAvJhxFJe1uSvgm_RtheqvS81eEZqtYTGSix3iUBBk1z5CT6NLBrax-2ddwQd8Q35aWrJoHOAU-VZNi6IGXjBIHuCAYWn6Qjn5fkiv_jld77o6v4E6HeZsiu08CAChatmGKatuq8AYLKS2R9yURI8Fwc_zGEDcsocTviqq66jszkITA5d8CmFXUhJxUmb6yYVaBm-zO_NXYK_VxKB30Aq1_LOX4H4_VMqltM&p=0&fvj=1&vjs=3
2020081647	Toronto, ON	TES - The Employment Solution	05f7bf2a7458c7fc	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DQQnUBuQBSuyaIQhpC59TW7hrTbBg8v-nGtzzV8dunbQHh9VTp-k89gJ8q3B9UEXLHcYbAG67d8jpOK4E3ok8olcOhfyyiChWMI3BPvUFdra3ljwgLS_vFsJXlW6jHt2NpQMzd2p9F6BXGm9ND3feUN42yGy4tVXBU7xPsBm2zUq3Qa9fZc5xMfzLqEiJy-Qp2qvz3BhdmQKPGH0VCq1uReO12AW8Y9gYJJ3s_Mce_U7DX92ieVLpygTBVLcQL65cLrAiueDOzEm0KutD3MdT87tdmkE0UhPXPWmyXVGj47Jbo1GW_88SLrE3OtMrg28rhyWmTXEvQg0yud_lkaa2lioZNHTyFrz5KQLhhc56sz7Ueph4QA-36HRhCd3hlEm7ysa7DtoiaBfDZgqcs5tSRZqfgymHPLIHU9CAadlXtOf9I9gsygXUqUxqfLmr5H6dZCPXegOsU5Cm8LpMk4R5sIL_H8UsokQLDjh285vdjTd08cubzStZ-b9jo-Db54_efX5lXT0PHxAvpJod4vvApi9a-5p3zxfp_4IalB6IBZUkea0ivRnZYYzeNKN_7t_3EqyXXnEzGQw3pBGuCufRtw1gaoywyqXSgFIRyI1srGl8xNqRlHhq8XOsj4E6QHBPZFCyFCBskgTpvbcqWCkrqjVF9yfwcR-517Uh8UYOuspl05rnEK1U5TDPmXQOOnPNGjTyRer_pdkGfbHGjvs8FOo__DN85lZPi8KuHwqI3uPyZF9BA0UbkxDMyxr0sNAlZYqeneCnko2FA1J7dlKICvYOZhnUIcWK_yq9nOGxbHYlVNpUSvbAlExJ-gmXfcdr1zELlLrVzclDpLfzUytEg&p=1&fvj=0&vjs=3	Job Title- Data Engineer
Location- Downtown, Toronto
Type - Full Time Permanent
Salary - Negotiable + Benefits



Focus on data architecture, best practices, reliability, security, and complianceImprove and extend ETL, data processing, and analytics processesFacility with PowerBI, including creating dashboards and data sourcesDeveloping high complexity, fast performing SELECT queries.Developing T-SQL procedures, functions, triggers, jobs, scripts, etc.Development of Advanced T-SQL such as temporal tables, PIVOTs, recursive table expressions and more.Modeling and implementing Data Mart solution for Power BI analytics
Managing indexes, statistics, query plans alerts, database activity, and overall performance activity.In-depth experience working with relational databases, such as Microsoft SQL Server or PostgreSQLEnthusiasm for applying good data design, testing, documentation, and support practicesExperience building and optimizing data pipelines, architectures, and data setsKnowledge of message queueing, stream processing, and data stores/warehousesWorking knowledge of AWS products related to data engineeringBachelor's degree in Computer Science, Software Engineering or an equivalent
Excellent communication skills - both written and verbal; ability to speak in Spanish is a bonus

To apply please send an email to sheetalk@tes.net	2020-08-16T23:47:11.000Z	Data Engineer
2020081648	Toronto, ON	Company Description

The Company
Hitachi Solutions is an impact-driven global leader in consulting dedicated to delivering competitive, end-to-end solutions based on the Microsoft Cloud. Our deeply connected teams are unified by our values and our commitment to helping clients succeed and compete with the largest global enterprises. We are a division of the 38th largest company in the world and carry the strength of a vast network of interconnected Hitachi companies all while remaining nimble, agile, and ready to pivot at a moment’s notice.
Hitachi Solutions started as three founding partners and transformed into nearly 2,000 consultants, developers, and support personnel all around the globe. With 36 Microsoft Partner of the Year awards, Hitachi Solutions has established itself as a leading partner in the ever-growing landscape of technology consulting.
The Culture
Our team is a mix of curious, fun, and get it done. We celebrate collaborative thought leadership and individual talents so that our ideas are translated into real-world results.
Each day, our team members show up to work as a blank slate with the sole purpose of coloring their canvases with knowledge. We are a complex group of resilient learners and encourage one another to go beyond the limits of conventional expectations. As a member of the dynamic Hitachi Solutions team, you will be challenged, pushed, and unconditionally supported in your efforts to drive the business forward.
** THIS ROLE IS OPEN TO ANY CANDIDATES FROM WITHIN CANADA**

Job Description

As an Analytics Data Engineer for Hitachi’s Azure Cloud Enablement Team you will be responsible to deliver high quality modern data solutions while being part of a dynamic and fast-growing team consisting of endless opportunities.
The successful candidate will be a self-motivated, passionate individual who strives to be the best at what he/she does and creates a trail of ecstatic Customers for life. This person will love to learn, contribute to the team and wants to be part of something great.
Knowledge and Experience
Hands-on experience with the Azure Data Platform (Data Factory, Data Lake, Data Warehouse, Blob Storage, SQL DB, Analysis Services)
Data quality (profiling, cleansing, enriching)
Data Modeling – including design from conceptual to logical to physical data models
Considered to be an expert in T-SQL
Hands-on experience with MPP database technologies such as Azure SQL DW, Teradata Netezza, etc.
Experience with multiple components listed, required:
Power BI including DAX
Database migration from legacy systems to new solutions
DevOps
Interpreted languages (i.e. python, C-sharp, Java, Scala, etc.)
Databricks
LogicApps
PowerApps
HDInsight
D365FO / CE experience as it pertains to data extraction
Knowledge of Cap Theorum and Distributed Database Management Systems, iis considered an asset.
Opportunity for a career path into a Data Scientist role if desired

Qualifications

Required skills / qualifications
Proven ability to engage customers to understand customer challenges and needs to develop technical solutions
3+ years of hands on experience working with the Azure Platform and its relevant components
Proven experience of architecting Azure services into a solution platform on Microsoft Azure for Analytics
Minimum 5 years hands on development experience with ELT/ETL using MS SQL Server, Oracle or similar RDBMS Platform
Familiarity with data visualization tools (e.g. PowerBI, Tableau etc.)
Experience or desire to coach, mentor and provide leadership to team members
Post-secondary degree/diploma in Business, Computer Science or a related discipline;
Strong communication skills, both written and verbal
Prepared for domestic and US travel as required
Preferred considered an asset, NOT required:
Project management experience
Databricks and Spark SQL
Previous Consulting experience
Additional Information

Opportunity Benefits:
Medical and Dental Benefit Package (including Long Term and Short Term Disability)Base salary plus targeted bonus package
This position can be based anywhere in Canada, though travel might be required.	Hitachi Solutions	24716d17b181ff38	Intermediate Data Engineer	https://ca.indeed.com/rc/clk?jk=24716d17b181ff38&fccid=28f79c18789111e8&vjs=3	2020-08-16T23:47:12.000Z
2020081649	Toronto, ON	00df245dd038b055	2020-08-16T23:47:13.000Z	Fiix	Why Fiix?
Fiix has a big goal – to create a more sustainable world. Sounds lofty right? Our mission is to make every maintenance team successful by enabling the adoption of a CMMS and we’re off to a great start. Teams that are part of the world’s most well known brands (like Toyota, Siemens, and Sara Lee) manage their maintenance activities and achieve greater results with Fiix. But we’re not stopping there. Our team is growing by leaps and bounds and we’re conquering new challenges every day. We’re looking for big thinkers with small egos to join us on our journey to create a more sustainable world.

Why we do it?
We’re a team of market disrupting, like-minded individuals. We all do things our own way, but we come together each and every day to create and deliver the long awaited answer to an antiquated industry – and we have a lot of fun while we’re at it.

We’re looking for a Data Engineer to help take Fiix’s explosive growth to a whole new level. We are looking for an experienced Data Engineer to build our data layer to support the delivery of machine learning-driven products. This is a unique opportunity to join a team of creative and passionate individuals committed to bringing AI to the predictive maintenance world.

What You Will Do:
Work with software engineering to design, build, maintain and optimize our data management and analytics pipeline
Perform data analysis, quality assessments, cleaning, imputation and data aggregation tasks
Perform feature engineering and selection to support machine learning activities
Build data processing pipelines and automate data pipelines in production environments
Work with engineers and data scientists to deploy analytics capabilities and machine learning models in production
Work with engineering and data teams to ingest and structure high throughput IoT data
Develop processes and frameworks to ensure data and model quality
Perform code reviews and testing to ensure software quality is high and requirements are met
Diagnose and repair data issues and assist customers with technical problems
What We're Looking for:
3+ years experience in a high growth software development environment developing data-driven products
Experience working on ETL, data warehousing, data modeling, data architecting, data analysis
Experience with at least some of: 1) Data streaming with Kinesis, Kafka or similar 2) ETL orchestration frameworks such as Airflow, Luigi or similar 3) Data warehouses such as Snowflake or similar
Development skills in Python, MySQL and other relational databases, NoSQL databases such as DynamoDB, Redis or similar
Experience with AWS or other cloud providerEducation background:
Bachelor’s Degree or higher in Computer Science or a related field
Equity Statement

At Fiix, we recognize that people come with a wealth of experience and talent beyond just the technical requirements of a job. If your experience is close to what you see listed here, please still consider applying. Diversity of experience and skills combined with passion is a key to innovation and excellence. Therefore, we encourage people from all backgrounds to apply to our positions. Please let us know if you require accommodations during the interview process.	https://ca.indeed.com/rc/clk?jk=00df245dd038b055&fccid=87852d99a20def3f&vjs=3	Data Engineer - Applied AI
2020081650	Toronto, ON	FreshBooks	e493781e9354119e	2020-08-16T23:47:13.000Z	Senior Data Engineer (Remote)	https://ca.indeed.com/rc/clk?jk=e493781e9354119e&fccid=785af18d53962443&vjs=3	FreshBooks has a big vision. We launched in 2003 but we're just getting started and there's a lot left to do. We're a high performing team working towards a common goal: building an elite online accounting application to help small businesses better handle their finances. Known for extraordinary customer service and based in Toronto, Canada, FreshBooks serves paying customers in over 120 countries.

The Opportunity - Senior Data Engineer

FreshBooks is seeking a Senior Data Engineer to join our team. You will help build new features and update existing ones in our current data pipeline infrastructure. If you're committed to great work and are constantly looking for ways to improve the systems you're responsible for, we'd love to chat with you!

What you'll do:

Collaborate with data engineers and full-stack developers on cross-functional agile teams working on features for our stakeholders.
Work closely with our analytics, data science, product and other internal business teams to ensure their data needs are met.
Participate and share your ideas in technical design and architecture discussions.
Ship your code with our continuous integration process.
Provide coaching to data engineers and share and learn from your peers.
Develop your craft and build your expertise in data engineering.

What you bring:

Enthusiasm for data engineering!
Experience creating and maintaining data pipelines
Experience with AWS, or another major cloud provider such as Google Cloud Platform.
Experience with Redshift, Big Query, or similar cloud storage technologies.
Strong programming skills in Python or similar language
A strong practitioner of test-driven (and behavioural test-driven) development
Experience with Git workflows, continuous integration and automated build pipelines.
Experience working in an Agile environment.

What you might bring:

A track record of staying at the forefront of data engineering technology.
Experience with BI tools: Periscope, Looker.
Experience with Spark, Kafka, Flink, Dataflow, or other streaming technologies.
Experience with Docker, Kubernetes, Terraform, and other DevOps and infrastructure as code technologies.
A limitless imagination for where data could go and what we can do with it to make our customers and our people awesome!

Why Join Us

We're an ambitious bunch, with our eyes laser-focused on shipping extraordinary experiences to small business owners. You'll be surrounded by talented team members who share a common vision for what an amazing software company could be, and have the opportunity to help build a world-class one, right here in Toronto, Canada.

Apply now

Have we got your attention? Submit your application today and a member of our recruitment team will be in touch with you shortly!

FreshBooks is an equal opportunity employer that embraces the differences in all of our employees. We celebrate diversity and are committed to creating an inclusive environment for all FreshBookers. All applicants are evaluated based on their experience and qualifications in relation to this position.

FreshBooks provides employment accommodation during the recruitment process. Should you require any accommodation, please indicate this on your application and we will work with you to meet your accessibility needs. For any questions, suggestions or required documents regarding accessibility in a different format, please contact us at phone 416-780-2700 and/or accessibility@freshbooks.com
2020081651	Toronto, ON	Data/Software Engineer Co-op	67bbfacf7e5a2ca0	https://ca.indeed.com/rc/clk?jk=67bbfacf7e5a2ca0&fccid=e4a22b3d28be02c0&vjs=3	The Company

Smart Nora is a wellness tech company based in Toronto that has improved sleep and relationships for tens of thousands of couples around the world. Our debut product is the world's most comfortable snoring solution and has been listed on Oprah’s Favorite Things, as well as Good Morning America, TIME, TODAY and BBC to name a few.

The Team

We are an ambitious, tight-knit team with an open work environment and a self-directed approach. We typically work out of our office at King+Spadina or remotely in a weekly cadence of Monday kick-offs, Wednesday tea times, and Friday wrap-ups — taking a lot of pride in our individual work and holding one another accountable for producing great work.

Note: Currently we are all working from home throughout the COVID-19 pandemic.

The Product

Smart Nora is an over-the-counter, contact-free snoring solution relevant to the 40% of adults who snore. Smart Nora is loved by customers for its comfortable contact-free design that enables users to sleep without any attachments to their face or body.

The Role

We are looking for a Software / Data Engineer. This is a co-op placement, full-time from September - December ideally for a 4th-5th year student. We are flexible with an earlier starting date or longer term.

Location

Remote

What you would do

Support the current FW project conducted with our third party project partners by implementing structured testing and documentation
Build supporting tools in Python, Node, or other scripting languages to validate firmware and hardware
Actively participate in Mobile App development project with our third party project partners
Support acoustic performance optimization including microphone and codec gain settings, assisted by automated tests
Be part of the Product team developing the next generation Smart Nora device
Wrangle data and decipher meaning from quantitative tests and analytics
Write clear documentation

Our requirements

Comprehensive understanding of Software Development processes (SDLC)
Comprehensive understanding of Object Oriented Programming (OOP) principles
Experience with Pandas, R, Tableau, or other data processing/visualization tools
Experience with GitHub (send us your profile!)
Exposure to cloud (AWS) and APIs is a plus
Experience in audio processing is a plus
Experience in machine learning is a plus
Majoring in Software Engineering, Computer Science, Industrial & Systems Engineering, or related field.

How to Apply

Please apply using our online application form to include your resume and links to your LinkedIn, GitHub (if have), and Kaggle (if have) profiles.

Accommodations are available on request for candidates throughout the application process. Please let us know your needs so that we may accommodate. Email careers@smartnora.com if you would like to discuss this role before applying.	2020-08-16T23:47:14.000Z	Smart Nora
2020081652	Toronto, ON	CATO - Big Data Engineer	https://ca.indeed.com/rc/clk?jk=20480654089beb5a&fccid=c2a63affe8751868&vjs=3	CAPCO	2020-08-16T23:47:14.000Z	Big Data Engineer
LOCATION: TORONTO

Capco – The Future. Now.

Capco is a distinctly and positively different place to work. Much more than consultants, we are active participants in the global financial services industry. Our passionate business and technology professionals enjoy a unique environment where they are actively encouraged to apply intellect, innovation, experience and teamwork. We are dedicated to fully supporting our world class clients as they respond to challenges and opportunities in: Banking, Capital Markets, Finance Risk & Compliance, Insurance, and Wealth and Investment Management. Experience Capco for yourself at capco.com

Let’s Talk About You

You want to Own Your Career. You’re serious about rising as far and as fast as your work and achievements can take you. And you’re ready to write the next chapter of your career story: a challenging and rewarding role as a Capco Big Data Engineer.


Let’s Get Down To Business

Capco is looking for talented, innovative, and creative people to join our development team to work on a number of projects and applications with a Data focus within the Digital practice.

Fitting that description, you will also need to be personally motivated to work in a team where clients become colleagues too.

Responsibilities

Produces high quality complex, deliverables with minimal input from stakeholders
Manage full software lifecycle for medium complexity projects from requirements, to design, to implementation, to testing
Develop and maintain back end solutions using cutting edge technologies and products
Work with Scrum Masters and product owners to priorities and deliver solutions using an Agile environment
Build reusable code and libraries for future use and follow emerging technologies
Mentor and train junior developers

Education/Experience

Bachelor’s degree (preference given to Computer Science, Engineering, Gaming and STEM-based majors) or equivalent experience
Five (5) or more years of experience as a Full-stack Data Engineer/developer on Data driven projects
Strong understanding of the full development lifecycle including requirements, architecture, design, development and testing
Strong development experience with Scala/Spark
Experience working with REST APIs/Springboot.
Familiarity working with Java and Hive.
Ability to balance competing priorities in a very dynamic and fast-paced environment
Excellent detail-oriented, problem solving skills and the ability to quickly learn and apply new concepts, principles and solutions
Must have excellent communication skills (verbal and written)

Show Us What You’ve Got

It will be very useful if you have some or all of the following skills:

Understanding of big data and distributed programming concepts
Experience working with ASW, GCloud, Docker, Kubernetes
Experience working with Microservices, CQRS, EventSourcing
Experience working with Spring, Akka, Spark
Experience working with Reactive Streams (Rx, Akka, Reactor)
Strong organizational and communication skills
Experience working in an Agile environment
Experience working with code versioning tools
Experience working with build, packaging and continuous integration tools and frameworks


Professional experience is important. But it’s paramount you share our belief in disruptive innovation that puts clients ahead in a tough market. From day one, your key skill will be to perceive new and better ways of doing things to give your clients an unfair advantage.


Now Take the Next Step

If you’re looking forward to progressing your career with us, then we’re looking forward to receiving your application.

Capco is well known for its thought leadership and client-centric model that distinguishes it from other consulting firms. Capco’s strong technology and digital knowledge base, it’s global experience of the Financial Service enables us to deliver projects from strategy through to delivery. We are committed to providing new areas of expertise from which our clients will greatly benefit. We have:

Access to industry-focused talent globally
Ability to leverage best-of-breed, innovative products and solutions for complex architecture and large-scale transformation
Extended global geographic market reach
Ability to capitalize on our client footprint and deep domain expertise within financial services

For more information about Capco, visit www.Capco.com.

Capco is an equal opportunity employer. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, marital status, genetic information, national origin, disability, veteran status, and other protected characteristics.	20480654089beb5a
2020081653	Toronto, ON	ABOUT MOBSQUAD

We are a well-funded, hyper-growth, scale-up looking for an experienced Data Engineer. If you've ever dreamed of working with a top tier technology company scale-up, on leading edge technologies, backed by the very best venture capitalists in the world, then this is your chance.

Some details about MobSquad:

MobSquad solves the significant and growing technology talent shortage faced by US-based start-ups and scale-ups by enabling our clients to quickly have a turnkey "virtual" Canadian subsidiary, where Canadian-based software engineers serve our clients individually on an exclusive basis
We've been featured on the front page of The Washington Post, on NPR multiple times, The Financial Times (UK), The Globe and Mail, the Calgary Herald, BetaKit, CBC, Global News, and many other places. other media outlets
We're a Certified B Corporation, were recognized as the third Best Place to Work in Canada in 2020, and have made numerous contributions to charitable organizations as well as a financial commitment to the Upside Foundation. We believe we are playing a key role in enhancing Canada's innovation economy, and have received financial support from the Government of Canada, Province of Alberta, Province of Nova Scotia, and City of Calgary, to support this ambition
You can learn more about us on our website

ABOUT THE ROLE

As a Data Engineer, you will be part of a Canada-based team working remotely for a leading US scale-up. Your team will operate alongside many other talented developers and data scientists in Canada, and you will be an integral part of the tech community that MobSquad has built.

This role requires someone who has demonstrated an ability to develop, test, optimize, and maintain scalable databases, architectures, and pipelines that enable data scientists and software developers to easily analyze and work with data. The ideal candidate has worked closely with data scientists and data architects and is an expert at optimizing data flow for use across broader teams. The candidate should be able to generate ideas and create tools that add greater functionality and usability to data systems within the company.

ABOUT YOU

You have a bachelor's degree in Computer Science, Information Technology, Data Science, Applied Math, Physics, Engineering, or a comparable analytical field from an accredited institution
You are expert in modeling, working with database architectures, and relational databases
You have over three years of experience with big data tools, such as Hadoop (MapReduce, Hive, Pig), Spark, and Kafka
You have experience with SQL databases (PostgreSQL, MySQL) and NoSQL databases (Cassandra, MongoDB)
You have experience creating and working with ETL data transformation and integration processes
You have experience working with enterprise-grade cloud computing platforms such as Microsoft Azure, Amazon Web Services (EC2, EMR, RDS, Redshift), and Google Cloud
You have expertise in relevant programming languages (Python, R, C/C++, Java, Pearl, Scala)
You have a deep understanding of data modeling tools (ERWin, Enterprise Architect, Visio)
You have experience optimizing big data pipelines and extracting value from large disconnected datasets
You have the ability to develop high-quality code adhering to industry best practices (i.e., code review, unit tests, revision control)

WHAT YOU'LL GET @MOBSQUAD

A full-time position that offers competitive compensation
A benefits program delivered through our bespoke digital platform, giving you control, choice, and flexibility. We give you the ability to build your package of benefits covering health (e.g., medical, dental, vision), wellness (e.g., gym, workout gear, massage, transit), and RRSP (retirement savings)
A downtown office location with first-rate amenities, surrounded by great restaurants and easily-accessible transit
For international candidates, sponsorship for an immediate work permit, expedited permanent residency, and Canadian citizenship within four years

At MobSquad, we support and encourage building a work environment that is diverse, inclusive, and safe for all. We invite and welcome applicants of all backgrounds, regardless of race, religion, sexual orientation, gender identity, national origin, or disability.	2020-08-16T23:47:15.000Z	https://ca.indeed.com/rc/clk?jk=f0322339724635eb&fccid=e140fb6d05504c9a&vjs=3	Data Engineer	MobSquad	f0322339724635eb
2020081654	Toronto, ON	Coursera is a leading online learning platform for higher education, where 64 million learners from around the world come to learn skills of the future. More than 200 of the world’s top universities and industry educators partner with Coursera to offer courses, Specializations, certificates, and degree programs. 2,500 companies trust the company’s enterprise platform Coursera for Business to transform their talent. Coursera for Government equips government employees and citizens with in-demand skills to build a competitive workforce. Coursera for Campus empowers any university to offer high-quality, job-relevant online education to students, alumni, faculty, and staff. Coursera is backed by leading investors that include Kleiner Perkins, New Enterprise Associates, Learn Capital, and SEEK Group.

Data Engineering is unique at Coursera. Our team doesn’t simply build reports on demand. Rather, we build the semantic infrastructure and products that empower our internal and external customers with the data to innovate and perform their jobs better.
We’re looking for a senior data engineer in Toronto who can help us drive data engineering efforts for our platform. In this role, you will work with cross-functional teams to design, develop, and deploy data solutions. Our ideal candidate is an independent, analytically-minded individual with strong data modeling and engineering skills, who shares our passion for education.
Your responsibilities
Architect scalable data models and build efficient and reliable ETL pipelines to bring the data into our core data lake
Design, build, and launch visualization and self-serve analytics products that empower our internal and external customers with flexible insights
Be a technical leader for the team; guide technical and architectural designs for the major team initiatives; mentor junior members of the team
Build data expertise, and partner with data scientists, product managers and engineers to define and standardize business rules and maintain high-fidelity data
Define and partner with other engineers in the development of new tools to enable our customers to understand and access data more efficiently
Work cross-functionally (eg: product managers, engineers, business teams) to support new product and feature launches
Your skills
5+ years experience in a data-related field, including data engineering, data warehousing, business intelligence, data visualization, and/or data science
Strong data engineering skills and at least one scripting language (e.g., Python)
Proficient with relational databases and SQL
Familiarity and experience with big data technologies (eg: Hive, Spark, Presto) preferred
Ability to communicate technical concepts clearly and concisely
Independence and passion for innovation and learning new technologies


If this opportunity interests you, you might like these courses on Coursera -

Big Data Specialization

Big Data Essentials: HDFS, MapReduce and Spark RDD

Data Warehousing for Business Intelligence

Coursera is an Equal Employment Opportunity Employer and considers all qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, age, marital status, national origin, protected veteran status, disability, or any other legally protected class.

If you are an individual with a disability and require a reasonable accommodation to complete any part of the application process, please contact us at accommodations@coursera.org.

Please review our CCPA Applicant Notice here.	https://ca.indeed.com/rc/clk?jk=c475abcdee2df0df&fccid=ef63980be6005ec6&vjs=3	Senior Data Engineer	2020-08-16T23:47:16.000Z	Coursera	c475abcdee2df0df
2020081655	Toronto, ON	893f6b0fd5e55acb	Machine Learning/Data Engineer	Summary of Position and Responsibilities

As part of Xanadu’s Machine Learning team, the selected candidate will be responsible for working with a multidisciplinary team of machine learning experts and quantum algorithm developers to bring machine learning models into production. They will develop, deploy, and maintain code, models, and pipelines leveraging various cloud providers and services; automate model training, testing, deployment, and monitoring; and design solution architectures for data driven applications.

Prospective applicants must have strong technical, programming, and mathematical skills. They must possess the ability to evaluate established methods and tools, learn new ones quickly, and apply their knowledge to solve practical problems. Applicants should be self-motivated and demonstrate the ability to successfully meet objectives. Familiarity with quantum computing is not essential for this position, but is a definite plus.

Basic Qualifications and Experience

MSc in Machine Learning, Mathematics, Computer Science, Physics, Engineering, or a related field.
Experience building and deploying production-grade machine learning applications at scale.
Strong software engineering skills across multiple languages (Python, Scala, Java, C++, etc.)
Experience building and supporting development environments for Machine Learning/Data Science teams.
Experience with distributed computing frameworks like Spark, Dask, or Hadoop.

Preferred Qualifications and Experience

PhD in Machine Learning, Mathematics, Computer Science, Physics, Engineering, or a related field.
Solid mathematical understanding of machine learning, statistical modelling, probability theory, and linear algebra.
Experience with frontend and backend web application development.
Passionate about agile software processes, data-driven development, reliability, testing, and continuous delivery.
Familiarity with and experience working in a fast-growing technology start-up environment.

If you are interested in this opportunity, please submit a copy of your CV along with a cover letter outlining why you think this is the right role for you!

At Xanadu, we are committed to fostering an inclusive, safe, and equitable culture that meets the needs of all individuals. We actively support a barrier-free workplace and ensure team members feel included, valued, and heard. We are dedicated to being a fair and equitable employer, and that includes the representation of women in STEM. Should you require accommodations at any point during the recruitment process please contact Human Resources at hr@xanadu.ai (mailto: hr@xanadu.ai).	https://ca.indeed.com/rc/clk?jk=893f6b0fd5e55acb&fccid=cf8d3d60750f6a59&vjs=3	2020-08-16T23:47:16.000Z	Xanadu Quantum Technologies Inc.
2020081656	Toronto, ON	https://ca.indeed.com/rc/clk?jk=5dae399d2651bf86&fccid=353eb997fc901045&vjs=3	At Veeva, we build enterprise cloud technology that powers the biggest names in the pharmaceutical, biotech, consumer goods, chemical & cosmetics industries. Our customers make vaccines, life-saving medicines, and life-enhancing products that make a difference in everyday lives. Our technology has transformed these industries; enabling them to get critical products and services to market faster. Our core values, Do the Right Thing, Customer Success, Employee Success, and Speed, guide us as we make our customers more efficient and effective in everything they do.

The Role

Veeva Systems is looking for experienced data engineers to build a cloud-based data analytics solution for the life science industry. If you are passionate about data and are eager to design and build data platforms from the ground up this is the role for you. The data analytics platform will provide data ingestion, data storage and rich data analytics capabilities with elegant visualization dashboards.
What You'll Do
Design and implement AWS based ETL processes to onboard data into our data lake from a variety of internal and external sources for our new data analytics platform.
Design data models and data services for optimal storage and retrieval.
Implement scalable data lake interfaces, microservices, and rest based API for querying and storing structured data.
Integrate new technologies to support advanced analytic use cases.
Requirements
5+ years’ experience in Python or Java, preferably at an enterprise cloud software company
Proven ability to write clean, testable, readable code in a team environment
Hands-on experience with building data pipelines in a programming language like Java or Python
3+ years of experience in relational databases with a mastery of SQL
Experience in data modelling, ETL development (pref. Apache Spark), and Data warehousing
Nice to Have
AWS Services (S3, Redshift, Elastic Search)
Experience with large scale big data pipeline – ETL / Kafka / Spark / MapReduce / Hadoop
Familiarity with Open API Specifications and Swagger
Experience working in an agile environment
Experience working in a startup
Perks & Benefits
 Conveniently located in downtown Toronto Snacks, beverages, and weekly lunches from local restaurants Team events and rec league sports teams Allocations for continuous learning & development Health & wellness programs Weekly yoga classes Ping pong and other games

Veeva’s headquarters is located in the San Francisco Bay Area with offices in more than 15 countries around the world.

Veeva Systems is an equal opportunity employer. Accordingly, we are committed to fair and accessible employment practices. Veeva Systems welcomes and encourages applications from people with disabilities. Accommodations are available upon request for candidates taking part in all aspects of the selection process.	Senior Data Engineer	2020-08-16T23:47:17.000Z	5dae399d2651bf86	Veeva Systems
2020081657	Toronto, ON	5328ae30e6b04e6f	https://ca.indeed.com/rc/clk?jk=5328ae30e6b04e6f&fccid=5c98f9ffc20e640f&vjs=3	2020-08-16T23:47:18.000Z	Company Description

We’re a technology company working in the loyalty e-commerce industry. Our solutions enhance the management and monetization of loyalty currencies for more than 50 of the world’s largest loyalty brands, from frequent flyer miles and hotel points to retailer and credit card rewards. Supported by our unparalleled loyalty industry experience and technological expertise, we bring state-of-the-art loyalty commerce platforms and products to individuals and businesses in today’s loyalty marketplace.
Our casual, collaborative office is where our strong workplace culture begins. Our people are what make us great, so we empower them with the freedom to think big and the resources to make things happen. We communicate directly, lead by example, and make sure our team members know how much they are appreciated. Passion for life and work is important to us, and we want to see it in you, too!

Job Description

Points is looking for a Data Engineer to join our Data Engineering team for a permanent position in our downtown Toronto office.
We’re an industry-leading web-based organization that is continuously reshaping how consumers interact with their loyalty programs. We work with the world’s largest airline, hotel, financial, and retail rewards programs, to tackle complex challenges and come up with innovative e-commerce solutions. If you’d like to be a part of it, we’d love to hear from you.
Reporting to the Team Lead, Data Engineering, you will:
Work in a scrum based team that is passionate about data.
Design and develop scalable pipelines for data consumption by downstream applications and for reporting purposes.
Improve upon existing ETL processes and monitoring to maintain data integrity and accuracy.
Automate the boring manual stuff, preferably using Python.
Support production systems to ensure a high degree of data availability.

Qualifications

Experience using GUI ETL tools (we use Talend).
Strong knowledge of SQL.
Experience with pub/sub architectures, such as Kafka.
Experience with containers and related infrastructure, such as Docker and Kubernetes.
Self-discipline and willingness to learn.
Nice to haves
Good knowledge of general software engineering principles and practices.
Experience with columnar-oriented databases, such as Vertica.
Experience integrating with services, such as Dataiku and NetSuite.
Working knowledge of Continuous Integration and Continuous Deployment concepts.
Additional Information

Building a great company culture is as vital to us as building a great business. Over the last 5 years Points has been the recipient of the following awards:
Best Workplaces (Medium) in Canada
Best Workplaces for Women.
Canada’s Top Small and Medium Employers
Greater Toronto’s Top Employers
Here are some of the perks that are included in our Points culture:
Central downtown location in the Financial District
Connected to the PATH network of shops/restaurants
We want to celebrate with you: all employees get an extra day off for their Birthdays!
Flexible work hours and casual dress every day
Marvelous Snack Cart Fridays: free refreshments and snacks!
Free coffee, tea, juice, pop, and snacks
Monthly subsidized lunch program
Green commuter and fitness subsidies
Secure bike storage with showers and towel service
Company-sponsored activities: bowling, movies, sports, paintball, and more!
Points is an equal opportunity employer and is committed to providing an accessible recruitment process. Upon request we will provide accommodation for applicants with disabilities.
All your information will be kept confidential.
No agencies please.	Data Engineer	Points International
2020081658	goeasy	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DbNJQGwjC-NEA80-qlVyEGmSv34Xx0eW4t4u9rXSv_7_tlvMU2e-Gw_wrZ47s1zPCnrySU2yuEL07wtSyx7XOwYh5VPcVL7CxRhOYwHzR-CdmyBrwbeqommOn6y9FD_uKIYpUCb9-PC_XgwGzBDrPy7SD7JQndrQc_GJzDDokXuGxEoEz6oiOMfVPNvDjMAo17IBsZfJZLe73DqPAjiMki-W3-6y2RkeEmwpIs6XKqDH9D6KgYiuUUeDQHVM-83IHsF-1dM0BEBhvLnPplmnV6aFwan_WgKwqw3nmk12RWkGyazLsVp-jcKFqtww39ujHdlEIQnirpGrHjG9xqyJ8XGURl81cO-6hzjKYY1sdpQ-h5ui8qkmXP0h1MMeLc9qUyTm4L_4w5wWSegT1GTO8gf3B302RwPFLWZJlg44RLfvekzI2bNOaBvE5I1Dvudb-MVpH3BSIt8uGZCcL5Pr2l_rs4xmdmboOqEFEjNbFVyg==&p=12&fvj=0&vjs=3	If you are looking to join one of Canada’s fastest growing companies, goeasy Ltd. is the place for you! Recognized as one of Canada’s Most Admired Corporate Cultures, one of Canada’s Top 50 Fintech’s and one of North America’s Most Engaged Workplaces, we want the best and brightest to join our team.

We are a publicly traded company on the TSX with over 4000% shareholder return since 2001, goeasy operates two main business units. easyfinancial is our consumer lending business that offers secured and unsecured installment loans of up to $35,000 and easyhome is Canada’s largest merchandise lease-to-own company. It is our mission to provide everyday Canadians the chance for a better tomorrow, today by giving them access to the credit they need and by offering them a second chance when they have been turned down by banks and traditional lenders. With a retail network of nearly 400 locations across Canada and over 1900 employees, we are able to build lasting relationships with our customers as we help them rebuild their credit and graduate towards prime rates and a brighter financial future.

The Data Engineer on the Data Science and Business Insights team will build, integrate data from various resources, manage data in goeasy operational data store and enterprise data warehouse. This position will develop ETL (Extract, Transform and Load) with various tools on large datasets to ensure data is easily accessible, works smoothly, as well as maintain and expand the data warehouse for reporting and analysis. The Data Engineer will also work closely with the data architect on the design and architecture of our enterprise data warehouse.

Responsibilities:

Develop data set processes for data modeling, mining and production
Develop and maintain ETL processes using SSIS, Scripting and data replication technologies
Participate in development of datamarts for reports and data visualization solutions
Research opportunities for data acquisition and new uses for existing data
Integrate new data management technologies and software engineering tools into existing structures
Support the translation of business requirements for data acquisition/manipulation and provide detailed specifications that can be passed downstream for use
Develop detailed technical specifications and operational support documentation in collaboration with Business Systems Analysts, BI Engineers and Architects.
Identify and communicate technical problems, process and solutions
Create Ad-Hoc queries and reports as needed along with providing on-going analytical support for these requests
Assist in the collection and documentation of user’s requirements
Ensure that existing business processes dependent on the ODS/EDW are monitored and respond quickly to bug fixes, enhancement requests and production ETL related issues.
Dealing with the database users on a daily basis to ensure that problems are dealt with promptly and that appropriate fixes are made to resolve any problems.
Recommend ways to improve data reliability, efficiency and quality
Ensure systems meet business requirements and industry practices
Work effectively with the Business Intelligence and Data Solutions Architects, Data and BI Engineers to ensure that all approved development and deployment procedures are followed.

Qualifications:

Bachelor’s Degree in Computer Science, MIS, Computer Engineering or other Information Technology related degree
4+ years working with SQL Server or comparable relational database system
3+ years of extensive ETL development experience with SSIS and/or ADF
4+ years of experience troubleshooting within a Data Warehouse environment
Expert domain knowledge & experience in Data warehousing, encompassing data model design, dimensional modeling, naming conventions, cross-cutting concerns, common integration technologies, patterns & standards and emerging technologies.
Expert Knowledge of SQL skills to build, debug, and optimize (developing procedures, functions, SQL queries, etc.) and working with large data sets and to perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
2+ years SQL Server Database administration experience
Cloud experience (Azure) is highly preferred
Exposure and experience with Python, R, Hadoop, Azure, other Big data and advanced analytics
Knowledge of AI and ML developments/solutions/implementations
Experience with multiple programming languages (PowerShell scripting, C#, others) with basic scripting skills.
High level of technical aptitude

Inclusion and Equal Opportunity Employment

goeasy is an equal opportunity employer. In addition, goeasy is committed to providing accommodations for applicants upon request at any stage of the recruitment process in accordance with all legislative requirements throughout Canada. Please let us know if you require an accommodation due to a disability during any aspect of the recruitment process and we will work with you to address your needs.

Additional Information:

All candidates considered for hire must successfully pass a criminal background check, credit check, and validation of their work experience to qualify for hire. We thank all interested applicants, however we will only be contacting those for interview who possess the skills and qualifications outlined above.

Why should you work for goeasy?

To learn more about our great company please click the links below:

PAID1234	Mississauga, ON	e8f01b72a171ac29	2020-08-16T23:47:18.000Z	Data Engineer
2020081659	Toronto, ON	Our client, a boutique-size consulting firm, prides itself on being at the forefront of innovation in the Big Data space. Founded in 2010, they provide thought leadership and implementation excellence within the ever-growing data and analytics world.
They take pride in having some of the most highly trained and experienced consultants in the industry which translates into optimal value for their clients. They were one of the first companies to provide analytics and data as a service, via the cloud, as early as 2010. They strive to make sure their customers are well-positioned with the best technologies/tools in the industry, constantly evaluating new and existing technology partnerships. Some of the more prominent companies they have partnered with include; Snowflake, DataRobot, MicroStrategy, Informatica, Amazon AWS & Microsoft. They continue to invest in their most valuable resource, their people. They do this through extensive training both on the job and through various educational programs.
Due to growth, they are seeking to hire an Intermediate - Senior Integration Consultant.

Desired Skills and Experience
: University/College degree in Computer Science, Mathematics, Data Science and/or Relevant Degree
: 5+ years hands-on development, configuration, scripting and administration experience with Data Integration platforms. (i.e. Informatica, Talend, DataStage, SSIS)
: BI Experience (MicroStrategy, Looker, Tableau, PowerBI) considered a nice to have
: Extensive theoretical and practical knowledge of data warehousing principles/concepts and practical development experience in all areas of the data warehousing life cycle
: Experience with Data Management, ETL, Cloud Data (AWS), Data Integration
: Knowledge of OLAP-related principles and concepts
: Strong grasp of data modeling techniques and concepts (Normalized/Denormalized, Conceptual/Logical/Physical, Star, Snowflake, Data Vault)
: Strong knowledge and experience with relational databases such as Snowflake, SQL Server, Oracle (Advanced knowledge of reading and writing SQL, Performance analysis and tuning)
: Knowledge and experience with key Big Data technologies (Hive, Presto, Spark, Kafka, NoSQL databases, Semi-structured data access patterns (Json, Parquet, XML, etc.))
: Strong Python scripting skills
: Excellent communication skills
: Great problem-solving skills
: Leadership and good client management skills

Day to Day Activities Would Include
: Conduct relevant customer interviews to determine key business requirements and objectives
: Build appropriate analytical data models based on outcomes of user interviews
: Analyze and profile data systems to build source to target data mappings
: Review ETL performance and conducts performance tuning as required on mappings/workflows or SQL
: Administration and support of data integration infrastructure
: 2nd level on-call support of ETL services as required

You will be responsible for attaining the following goals:
: Attaining a minimum of 1 new accreditation/certification per year
: Spending 80% or more of their time on billable work
: Completing 90% or more of their agile delivery tasks on time
: Demonstrating competency in 1 new relevant technology every year	Copperstone Connect	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Ceb7pqR8iGmLuXIZQklF6pqed3xddONNM6Jjumj-ndyGK0kULQUmQ2REqfJ5KAzFVeK_es1WVg0s1sLgKW09UsvVRp0-dSizFQr1oNyK_VIqdjla00VU9GNVvu2x4v9p3hsL7NeF-6hBksbAaSgKazsaYTODGMKQQeVuYcxoOuxvFKuD0HRLD9ybgbcXTn1LPcIBgaQpBqWRQZ_wf-2qO_PHwFuWetEceyIfFWYJqfiYFFEJq4afeRQE-ZPngeEIqOCon0mP249Q9CgwJSJrHhE2awYP7vW-Hj-ngNZ2zcxLc0SYO18gomkM5k-M6fLdj5nOGtGMFXhSXxWWvDyknJJxrcivLEiZEKM5Ick7Zo_XrZkqgg9JmwNzlI_6wUzsJAOt_8wS7ZN5S9rEC07D9jYzJEFgPb1xyRojUfJDgxk9KHy1rrBO9oRKKpG5LZZmEIU-oL-yq4FgwgVtBm7Axsn4Eu-aPPSxfxadUAt8YMZox2joU-mYBGgyXaSo51DP-TeBtlJxkyoGQnNRzR5sIwdt2uEuM_YNv9kwUIWNrobhnh5u58C6iL6taXYz7axOLkyzbCbIGdOJ31Dr_eyuNGojUyOHDlkDxGhQGqsSj0SKOfCmY6ms5U9oZNZZdg8vCifiFjAqFCA1BLyAEwpS9WvTgiziK63dVnik1eUzbw3A==&p=13&fvj=0&vjs=3	54e0fcc38385e3a2	Data Engineer/Integration Consultant	2020-08-16T23:47:19.000Z
2020081660	Toronto, ON	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Bt274jo0r_e0cCCqhskoQvohXG3bfDAo8rIdE-Msu3yuVfyF2oiDFKHFOU-vxVulAfe8kPL7riFwoXa1bsfN-szjxO_8qUQWbPsJOnTg0EkutIoK2S4NNLL24JQh9e4DsE9hv907k2_zDbvj0a8WAj5oPSV5J0YhgrbSF1z_nzlGMUEWiJK22Y3v81Khv9agmdEzA0TpAkg25iJg5G3Q3FXk-7F9a5Qm8GgVEAirJv3ihdMIZPyPeGhD0eqLCdaGx5eXHG9wUlV8wUhxinHznptNemxNTNYBN6Dl-iwEGqonj_NWTbBRAv1fpdDvmSdkKwgiQ9yPpBUwCDixAns3FO6JnUa11cjUmKgk3tPrW55-2_FJMyw8J1hUGUShqyZEb_FALFcAQUEFQGGAsV2GgG2m3fWOXVIc3qHUe2beu5ApA_sWniryO5m2g0wu3xRHkmBeqqTkY9qQ==&p=14&fvj=0&vjs=3	2020-08-16T23:47:20.000Z	The growing team here at Myticas is looking for an experienced Data Engineer who would be interested in a remote contract opportunity within the Ottawa, Toronto and Montreal regions.

Accountabilities:

Defining and reviewing security design requirements for cloud infrastructure and application components.
Evaluating architecture patterns from security perspective.
Building and implementing security controls to enable enforcement of compliance with Cloud Control Objectives, using custom Azure policies and integrated controls in DevOps processes

Requirements:

Strong Data Engineer w/ DevOps expertise + Azure Cloud Experience
Must know how to code and stand up scripts.
Experience with Data Digestions
Experience writing scripts to automate (infrastructure)
ARM Templating Expertise
Azure Synapse Expertise
Support developing automated DevOps processes and procedures for the following Azure components:
Azure Synapse (Azure DW) & Studio (private preview)
Azure Data Catalog Gen 2 (Babylon – private preview)
Azure Data Lake Storage Gen 2
Azure ML
ML Flow
Azure SQL Analysis Service
Azure Databricks
ADF data pipelines for data loading to AzSQL/Synapse
ADF data pipelines for connecting to on-prem data sources for data

Candidates looking to apply for this role are to send us an updated version of their resume in confidence. Our team will be sure to review all applicants and follow up accordingly at the conclusion of the review process.

Job is also known as: Data Engineer, DevOps Engineer, Azure Cloud Engineer, Azure Engineer, Cloud Engineer

INDMY	BHJOB15656_15145 - Data Engineer w/ DevOps - Azure Cloud	a32b7c599156a218	Myticas Consulting
2020081661	Job Field:
Information Technology
Job Type:
Full-time
Building Location:
Length of Assignment:

SUMMARY

The Senior Data Engineer is a deep technical expert in building complex data warehousing and business intelligence applications. At this level, the incumbent demonstrates a passion and in-depth knowledge of large, complex application development methodologies. They motivate themselves and the team to refine their skills and adopt best practices for developing pragmatic software solutions for the organization. Leading the charge, they continue to raise the bar on mastery of business intelligence application development within the team and the organization.

KEY DUTIES & RESPONSIBILITIES

Programming

Uses in-depth knowledge of advanced programming techniques, design patterns and hardware/software interfaces to develop business intelligence and data warehouse applications.
Designs, tests and integrates data warehouse and BI modules and resolves programming errors using various debugging tools and techniques.
Provides guidance/mentors on programming practices and techniques to individuals and cross-functional teams.
Provides support, guidance and production assurance for very complex or urgent problems.
Performs work with minimum supervision, and work is assigned in terms of technical objectives.
Prepares technical documentation (e.g., user guides, technical specifications).
Assists in the design of business solutions.

Analysis

Conducts impact analysis for proposed changes to or problems across the system.
Leads team discussions in the analysis and collaboration to clarify and improve specifications or to identify alternative programming solutions.

Continuous Improvement

Makes recommendations or decisions on architecture, application design, standards and process improvements.
Enforces team and organizational standards and practices (e.g. at walkthroughs and peer code reviews).
Engages in continuous learning by developing and executing on a learning plan.
Takes responsibility for individual and the team's results.
Advocates for quality in all aspects of development efforts based on the team's definition of quality.

Risk Management

Estimates and prioritizes work to maximize value while taking into account risk, effort and dependencies.
Raises impediments, risks, and issues as early as possible and work with the team to mitigate as needed.

KNOWLEDGE & SKILLS

University graduation and minimum 5-10 years of relevant experience
Demonstrates in-depth knowledge of Microsoft BI architecture, established data warehouse development methodologies, multi-dimensional data modelling, OLAP, metadata management, data security, predictive analysis and big data processing. Extensive experience in one of these cloud data warehouses (Snowflake, bigtable, Redshift), Data Vault 2.0 methodology, steaming data processing, BI components in SQL Server 2016+, TSQL, and DAX, Power BI.
A good working knowledge of application security, C#, python, PowerShell, metadata management, NoSQL, and data security.
Experience in programming and debugging complex data warehouse and BI applications as part of a multi-disciplinary team environment (following an agile framework such as SCRUM preferred) based on Microsoft Team Foundation Server and git.
Experience in writing unit tests to support production code using a unit test framework.
Experience with database management (i.e. database design, schema creation, concurrency and performance considerations).
Takes ownership and initiative and collaborates well with a team of peers.
Demonstrates a commitment to continuous learning (e.g. user groups, blogs, conferences, community awareness, and next generation tooling).
Able to clearly communicate in both a verbal and written form within a predominantly English working environment.
Has a positive, passionate, idea generating attitude when faced with challenges.

Licenses and/or Professional Accreditation

Certification in Microsoft technologies preferred	bcb75a8b3d33578b	Markham, ON	Senior Data Engineer	2020-08-16T23:47:21.000Z	https://ca.indeed.com/rc/clk?jk=bcb75a8b3d33578b&fccid=84c23fbcabc14d59&vjs=3	BGIS
2020081662	At Bond, we design creative and innovative solutions for our clients, all with the goal of helping them build ever-stronger loyalty to their brands. That can take us in some pretty amazing directions, and as a Data Engineer, you’ll have your hands on the wheel as we drive the future of loyalty.

Working on the bleeding edge of exciting technology, you're afforded the opportunity to experiment with new tools and attempt radically different approaches than traditional software engineering affords. Every day with the Data Engineering team is different and each project presents its own set of new and exciting challenges. Things shift very quickly in our industry and we rely on the Data Engineering team to keep us ahead of the curve and moving in the right direction.
Here's what we want:
Problem Solver: You are curious and loves exploring multiple approaches to find the most efficient, scalable solution and solve a problem
Collaborative: You work well with other people
Passionate: A passion for Big Data and an interest in the latest trends and developments constantly researching new tools and data technologies
Self-starter: You are comfortable helping your team get things done
Here's what you'll be doing:
Design, implement, and maintain data pipelines for extraction, transformation, and loading of data from a wide variety of data sources to various data services
Identify, design, and implement system performance improvements
Identify, design, and implement internal process improvements
Automate manual processes and optimize data delivery
Useful skills/background: You may or may not tick off every box, and that's ok. Each person brings a different background and different skills. If you think you are a good match for what we are looking for tell us why, and tell us what you are doing to improve yourself and we'll see what we can do to help!
A degree in Computer Science/Engineering or related field
2-4 years of experience in a software engineering environment
Experience with SQL and NoSQL systems
Knowledge of Hadoop, Spark, Kafka or other equivalent technologies
Proficiency in some of the following languages: Scala, Java, Python, Bash
Experience with automated testing systems
Mentorship, collaboration, and communication skills
Knowledge of data modelling, data warehousing, ETL processes, and business intelligence reporting tools
Experience working with CI/CD, containerization, and virtualization tools such as Gitlab, Jenkins, Kubernetes, Docker
Experience with tools like Databricks, Snowflake or PowerBI

Why Join Us?
Bond Brand Loyalty is proud to be recognized as one of Canada’s Best Managed Companies.

We’re 400(ish) people working tirelessly together to make the world a more loyal place. You’ll be joining a hyper-talented team with a galaxy of skillsets ranging from research to creative to digital and beyond. You’ll have an excellent opportunity to grow, learn and make an impact as we tackle some of our client’s biggest business challenges.
If you’re looking to build your career, build your skills and build bonds apply today!
Bond Brand Loyalty welcomes and encourages applications from people with disabilities. Accommodations are available on request for candidates taking part in all aspects of the selection process.	2020-08-16T23:47:22.000Z	fd5da69e089b7f27	Mississauga, ON	Bond Brand Loyalty	https://ca.indeed.com/rc/clk?jk=fd5da69e089b7f27&fccid=9343039b36601ad2&vjs=3	Data Engineer
2020081663	Toronto, ON	Perpetua's mission is to give superpowers to anyone that sells online. At the moment, we help media agencies, brands, and Amazon sellers win on Amazon by analyzing large amounts of data and using AI to develop smart optimization algorithms that drive transformational sales growth.

As a Senior Data Engineer, you will be responsible for maintaining and evolving our data infrastructure to support the massive scale of data that we process to power our customers' ad campaigns. This includes advancing our data capabilities to both cut costs and increase performance, while maintaining data integrity and consistency. You will build scalable infrastructure and data pipelines to deliver a platform unparalleled in the advertising space using Google BigQuery, Looker, Airflow and Python (Django, Pandas, SciPy Stack).
What You'll Do
Interact with internal stakeholders to figure out how to make it easier for them to leverage Petabyte-scale data
Accelerate data-informed decision making to transform our product & engineering strategy
Be responsible for developing, maintaining and evolving the Data Platform
Considering technical tradeoffs and advancing our data capabilities
Being responsible for data consistency and integrity both for our internal tools and client APIs
Owning data accessibility and discovery across all parts of the company
Increasing performance of our data pipeline
Evolving our use of available tools and optimising current tools to optimise infrastructure costs
Work in a fast-paced, scaling start-up building software that is truly impactful
What You'll Have
Experience with workflow management tools like Airflow for moving and transforming data
Experience with data mapping, schema design, data structures and algorithms, data quality and integrity
You'll deeply know your datawarehouse from your datalake from your transactional database
Experience designing infrastructure to facilitate both availability and approachability of data to all users (internal and external)
Ability to source requirements from stakeholders and develop a vision for a data-driven company
Ability to build and performance tune complex database queries
Ability to solve problems in new and innovative ways
Self starter who takes initiative; owning a project from start to finish
The ability to communicate complex technical issues in a clear and concise manner
Flexibility to adjust to changing priorities, requirements, and schedules
Experience working in a fast-paced, agile environment
Company Benefits
Impactful work that will help lay the foundation for future projects
Meaningful equity at an early stage company
Ground floor opportunity
Paid-for meals
Unlimited snacks and drinks
Full benefits plus a health spending account
Top of the line technology to help you build your own workspace
Flexible time off policy
At Perpetua, we are dedicated to pursuing and hiring a diverse workforce with varied experiences, perspectives and opinions. We believe diversity helps our team perform better and enables us to build an outstanding product for our customers. We are an equal opportunity employer and are committed to work with applicants requesting accommodation at any stage of the hiring process.	Perpetua Labs	2020-08-16T23:47:23.000Z	Senior Data Engineer	https://ca.indeed.com/rc/clk?jk=9c4363e25aeb557b&fccid=b7254da6203c84b3&vjs=3	9c4363e25aeb557b
2020081664	Sr. Data Engineer	https://ca.indeed.com/rc/clk?jk=035a5f23df595f6f&fccid=2ca7392810684c2b&vjs=3	This position will join our growing team of data and analytics experts for Cox Automotive Canada. You will be responsible for expanding and optimizing our data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. You will support our data initiatives and will ensure optimal data delivery consistent throughout ongoing projects.

As a Senior Data Engineer, you will be responsible for leading Data Engineering projects throughout their entire lifecycle, that being: initial investigation and stakeholder engagement, solution design, application development, testing and sign-off, release and maintenance

The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. They must be self-directed and comfortable supporting the data needs of Cox Automotive Canada. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives, enabling the evolution of our Data Science and Business Intelligence

Responsibilities:
Assemble large, complex data sets that meet functional / non-functional business requirements.
Recommend different ways to constantly improve data reliability and quality
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Improve upon the data ingestion pipelines, ETL jobs, and alarms to maintain data integrity and data availability.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics
Performs other job-related duties as assigned or apparent and Stay up-to-date with advances in data persistence and big data technologies
Prototype and design new data integration solutions that balance security, scalability, fault-tolerance, performance as well as cost effectiveness
Define, document and champion best practice architectural patterns and work with wider technology development teams to proactively seek out new opportunities to utilise Data Solutions technology and data assets
Be responsible for leading the delivery of multiple Data Engineering development projects at a time. Project delivery responsibility extends from design, development, testing, release and maintenance
Be responsible for leading the delivery of multiple Data Engineering development projects at a time. Project delivery responsibility extends from design, development, testing, release and maintenance
Qualifications

Qualifications

3+ years’ technical experience working in Big Data, designing solutions to complex data ingestion problems
Graduate degree in Computer Science, Computer Engineering or a related field
Big Data Certification and AWS certification would be an asset
Must have strong Data Orchestration experience using tools such has AWS Step Functions, Lambda, AWS Data Pipeline, Apache Nifi.
Advance Python Skills to develop efficient, decouple Data pipeline
In-depth knowledge of AWS Big Data Services
Must possess in-depth knowledge and hands on development experience in building Distributed Big Data Solutions including ingestion, caching, processing, consumption, logging & monitoring)
Must have a strong understanding and experience with Cloud Storage infrastructure and operationalizing AWS based storage services & solutions prefer S3 or related specific business questions and identify opportunities for improvement
Experience in building processes supporting data transformation, data structures, metadata, dependency and workload management
A successful history of manipulating, processing and extracting value from large disconnected datasets
Working knowledge of message queuing, stream processing, and highly scalable data stores
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases
Experience supporting and working with cross-functional teams in a dynamic environment
Experience deploying data pipelines with CI/CD
Experience with Snowflake Data Warehouse is plus
Organization: Cox Automotive
Primary Location: Canada-Ontario-Mississauga-2233 Argentia Rd
Employee Status: Regular
Job Level: Individual Contributor
Shift: Day Job -
Travel: Yes, 5 % of the Time
Schedule: Full-time
Unposting Date: Ongoing	Cox Automotive	035a5f23df595f6f	2020-08-16T23:47:23.000Z	Mississauga, ON
2020081665	https://ca.indeed.com/rc/clk?jk=1f6d9f1ab7c60335&fccid=1c4aa3d5a92746d4&vjs=3	Mosaic North America	Senior Big Data Engineer	1f6d9f1ab7c60335	2020-08-16T23:47:24.000Z	Mississauga, ON	The Senior Big Data Engineer DevOps role reports to the Director, Data Sciences DevOps. The role is part of the DevOps team in charge of the daily operations of various integration technologies. The role is responsible for supporting workloads running on the Hadoop environment and associated technologies. This position will be responsible for monitoring the batch jobs, resolving incidents, optimizing workloads, tuning jobs, and making job enhancements.

The role will focus on production support and will also take part in the DevOps rotation for making enhancements. Also, the role will be involved in R&D of emerging technologies with the application administrators and technical architects.

Strategic:
Evaluate tools and technologies in the context of the future state architecture, and evolving business requirements
Responsible for review of project artifacts during the transition phase and ensure operational needs are met
Research & development about new Hadoop & Analytical technologies
Review solution and technical designs
Propose best practices/standards
Benchmark the performance in line with the non-functional requirements
Previous experience in developing and deploying operational procedures, tuning guides and best practices documentation
Attention to detail to review project deliverables for completeness, quality, and compliance with established project standards

Candidate Profile:
3+ years developing and supporting applications leveraging the Hadoop stack
3+ of experience with data integration/ETL tools such as DataStage or alternatives
SDLC knowledge in both waterfall and agile methodologies
Hands-on experience with source code management system (SVN, Git) and continuous integration tools (Jenkins)
Experience on following tools: Hive, SQL, Spark, Kafka, Flume, Sqoop, HBase ,Pig, HDFS, R, NoSQL
Experience on handling data processing, delivering distributed and highly scalable application
Experience with HortonWorks Hadoop Distribution
Experience with large scale domain or enterprise solution analysis development, selection and implementation
Experience with high-volume, transaction processing software applications
Good understanding of workload management, schedulers, scalability and distributed platform architectures
Experience in software development and architecture experience using Java EE technologies (Application Server, Enterprise Service Bus, SOA, Messaging, Data Access Layers)
Experience in scripting languages & automation such as bash, PERL, and Python
Experience in data warehousing, analytics, and business intelligence/visualization/presentation
Experience using SQL against relational databases
Working knowledge of search technologies like Lucene, Solr
5+ years of hands-on experience on Linux, AIX, and z/OS
Professional Skill Requirements
Excellent communication skills (both written and oral) combined with strong interpersonal skills
Strong analytical skills and thought processes combined with the ability to be flexible and work analytically in a problem-solving environment
Attention to detail
Strong organizational & multi-tasking skills
DISCLAIMER:
Acosta/Mosaic North America is an Equal Opportunity Employer
The above statements are intended to describe the general nature and level of work being performed by people assigned to this classification. They are not intended to be construed as an exhaustive list of all responsibilities, duties, and skills required of personnel so classified. Mosaic reserves the right to modify all or part of any job descriptions at its discretion in order to meet and or exceed the needs of the business.
We are committed to providing accommodations for persons with disabilities. If you require accommodation, we will work with you to meet your needs, to the extent required by law.

Qualifications


Primary Location: CA-ON-Mississauga
Work Locations: Acosta-Mosaic Mississauga Corporate Office 2700 Matheson Blvd. E. W. Tower 2nd Floor Mississauga L4W 4V9
Job: Information Technology
Organization: CoE - Client Information Services - Canada
Shift: Standard
Job Type: Full-time
 Day Job
Job Posting: Aug 4, 2020, 10:54:13 AM
2020081666	Toronto, ON	https://ca.indeed.com/rc/clk?jk=1dd125b777e13132&fccid=cdf5f442bc9a18df&vjs=3	2020-08-16T23:47:24.000Z	Data Solution Engineer
Onix helps customers transform and evolve their business through the use of cloud services. As part of an entrepreneurial team in this rapidly growing business, you will help shape the future of how technology is used in the workplace.
Primary Responsibilities
Lead strategic cloud application development discovery session to help customers understand the value of cloud application development and how to position it within their organization.
Proactively help customers address all technical issues that may arise throughout the entire pre-sales cycle.
Ability to facilitate demonstrations, proof of concepts and public-facing presentations.
Use Google Cloud Platform tools to build Enterprise-grade Big Data solutions.
Architect new cloud-based data pipelines.
Ability to bring together multiple data sources into a unified data warehouse.
Apply analytics and visualizations to customer data sets.
Create customer and partner connections to help grow Onix name recognition in the data space.
Help customers understand the right technologies for their use case.
Establish strategic customer relationships and become their go-to trusted advisor for Big Data needs.
Assist in strategic direction and planning for growth of the Cloud Data Team.
Quickly architect sound cloud solutions to radically different customer environments.
Establish strategic customer relationships and become the technical go-to resource for answers as well as a trusted cloud advisor.
In-depth understanding and the ability to demonstrate expertise in designing, deploying, and maintaining custom enterprise web applications.
In-depth understanding and the ability to demonstrate expertise in using a variety of development languages.
In-depth understanding and the ability to demonstrate expertise to determine the best migration path from legacy or on-prem applications into public cloud environments.
Review and analyze customer architecture at the domain and product level and translate and evolve them into cloud-ready applications.
Staying in constant communication with customers to ensure Onix is addressing all of their needs during the pre-sales cycle.
Learning and maintaining an in-depth understanding of current and new development technologies and industry standards.
Assist account manager with technical discovery and responsible for all technical scoping activities during the creation of the Statement of Work.
Ability to frequently travel throughout the United States and Canada. Up to 25%.
Required Skills and Experience
Degree in Computer Science or Math (or related technical major) or equivalent practical experience (math strongly preferred).
Experience with large data sets and Enterprise-grade databases (structured and unstructured)
Experience architecting and building data pipelines.
Deep understanding of the ETL (extract, transform, load) process.
Experience extracting data from multiple sources via APIs and scripting.
Experience transforming data through field mapping, programmatic rulesets, and data integrity checking.
Able to expertly convey ideas and concepts to others.
Excellent communication skills (verbal, written and presentation)
Creative problem-solving skills and the ability to design solutions not immediately apparent.
Ability to participate in multiple projects concurrently.
Customer-oriented and shows a bias for action.
Able to function in a highly dynamic team that moves rapidly from idea to planning to implementation.
Highly adaptable with the ability to learn new technologies quickly without direct oversight.
Strong knowledge of Python Machine Learning standard libraries.
Mastery of N-dimensional NumPy arrays.
Mastery of pandas data frames
Ability to perform element-wise vector and matrix operations on NumPy arrays.
Strong knowledge of Anaconda, Virtualenv, and Jupyter Notebooks
Good functional Knowledge of Tensorflow programming model.
Strong understanding of all commonly used Machine Learning models and the main algorithms that compose the models.
Ability to rapidly prototype proofs-of-concept and technical demonstrations.
Ability to conduct technical BD/ML workshops enabling the audience ( researchers, Doctoral and postdoctoral CS Ph.Ds, ML Ph.Ds, Mathematicians, Scientist etc) to adopt the cloud technologies to for development and implementation of BD/ML for scientific research.
Good knowledge of common networking concepts.
Strong customer-facing communication skills.
Experience in writing software in Java or Python.
Familiarity with web-related technologies (web applications, web services, service-oriented architectures) and network/web related protocols.
Creative problem-solving skills and a drive to solve difficult issues.
Ability to stay positive and motivated while under pressure.
Ability to participate in multiple projects concurrently.
Excellent communication skills (verbal, written and presentation)
Customer-oriented and shows a bias for action.
Provide on-time, well-executed work that leads to excellent customer satisfaction.
Able to expertly convey ideas and concepts to others.
Highly adaptable with the ability to learn new technologies quickly without direct oversight.
Good understanding of the built-in data types. ( lists, dictionaries, tuples sets).
Preferred Skills and Experience
Google Cloud Platform Data Engineer Certification.
Experience with Big Data, PaaS, and IaaS technologies.
Experience in and understanding of data and information management - especially as it relates to IaaS and PaaS.
Experience architecting and developing software for scalable, distributed systems.
Understanding of the public cloud market and pain points driving enterprise cloud adoption.
It is the policy of Onix to ensure equal employment opportunity in accordance with the Ohio Revised Code 125.111 and all applicable federal regulations and guidelines. Employment discrimination against employees and applicants due to race, color, religion, sex, (including sexual harassment), national origin, disability, age (40 years old or more), military status, or veteran status is illegal.
Onix will only employ those who are legally authorized to work in the United States or Canada. This is not a position for which sponsorship will be provided. Individuals with temporary visas such as E, F-1, H-1, H-2, L, B, J, or TN or who need sponsorship for work authorization now or in the future, are not eligible for hire.	Solution Engineer - Data	1dd125b777e13132	Onix Networking Corp
2020081667	Toronto, ON	Cloud Data Engineering Manager	Accenture	https://ca.indeed.com/rc/clk?jk=82b02bab68b5d127&fccid=a4e4e2eaf26690c9&vjs=3	82b02bab68b5d127	There is never a typical day at Accenture, but that’s why we love it here! This is an extraordinary chance to begin a rewarding career at Accenture Technology. Immersed in a digitally compassionate and innovation-led environment, here is where you can help top clients shift to the New using leading-edge technologies on the most ground-breaking projects imaginable.

Interested in building end-to-end marketing solutions for clients? Bring your talent and join Data which operates in the Interactive, Mobility and Analytics space. You will have opportunities to get involved in digital marketing, eCommerce and end-to-end mobility capabilities to help clients to improve productivity and more!

WORK YOU’LL DO
Work across the Service Delivery Lifecycle to analyze, design, build, test, implement and/or maintain multiple system components or applications for Accenture or our clients
Adapts existing methods and procedures to create possible alternative solutions to moderately complex problems
Uses considerable judgment to determine solution and seeks guidance on complex problems
Determines methods and procedures on new assignments with guidance
Manages small teams and/or work efforts (if in an individual contributor role) at a client or within Accenture

WHO WE´RE LOOKING FOR?
Minimum 8 years of experience as a Data Engineer
Must have experience with one of the Cloud Technologies (Azure or AWS)
Azure cloud includes Spark, Python, Databricks, Synapse, Snowflake, Data Factory and ADLS
AWS cloud includes Glue, EC2, EMR, Athena, redshift, Snowflake, S3, Spark, Python and Databricks
Experience with Big Data technologies like MapReduce, Pig, Hive, HBase, Sqoop, Flume, YARN, Kafka, Storm and etc.
3+ years of experience with at least one SQL language such as T-SQL or PL/SQL
3+ years of work experience with ETL and data modeling
Experience in real-time analytics application
Experience in both batch and stream processing technologies
Experience with Java or Scala programming languages
Machine learning experience with Spark or similar
Professional Skills Qualifications:
Proven success in contributing to a team-oriented environment.
Proven ability to work creatively in a problem-solving environment.
Desire to work in an information systems environment.
Demonstrated teamwork and collaboration in professional setting; either military or civilian.
WHAT´S IN IT FOR YOU?
Competitive benefits, including a fair and balanced parental leave policy.
Fantastic opportunities to develop your career across industries with local and global clients.
Performance achievement and career mentorship: our performance management process focuses on your strengths, progress and career possibilities.
Opportunities to get involved in corporate citizenship initiatives, from volunteering to charity work.
To learn more about Accenture, and how you will be challenged and inspired from Day 1, please visit our website at accenture.ca/careers.
It is currently our objective to assign our people to work near where they live. However, given the nature of our business and our need to serve our clients, our employees must be available to travel when needed.

Accenture does not discriminate on the basis of race, religion, color, sex, age, non-disqualifying physical or mental disability, national origin, sexual orientation, gender identity or expression, or any other basis covered by local law. Accenture is committed to providing employment opportunities to current or former members of the armed forces.

We are committed to employment equity. We encourage all people, including women, visible minorities, persons with disabilities and persons of aboriginal descent to apply.

Accenture is a leading global professional services company, providing a broad range of services and solutions in strategy, consulting, digital, technology and operations. Combining unmatched experience and specialized skills across more than 40 industries and all business functions — underpinned by the world’s largest delivery network — Accenture works at the intersection of business and technology to help clients improve their performance and create sustainable value for their stakeholders. With 469,000 people serving clients in more than 120 countries, Accenture drives innovation to improve the way the world works and lives. Visit us at www.accenture.com.	2020-08-16T23:47:25.000Z
2020081668	Toronto, ON	2020-08-16T23:47:26.000Z	Are you looking for unlimited opportunities to develop and succeed? With work that challenges and makes a difference, within a flexible and supportive environment, we can help our customers achieve their dreams and aspirations.
Job Description
Manulife’s Global Data Office (GDO) is seeking a Sr. Data Engineer reporting into the Director, Advanced Analytics and AI Engineering & Enablement Lead. Located in Toronto, Canada - the role will champion and support strategic and global data initiatives that strengthens Manulife’s global data and advanced analytic capabilities, foster cross-segment collaboration and communication helping build an agile data insight driven culture, and lead and nurture open data design and architecture establishing conditions for successful technical and analytic innovation. The Sr. Data Engineer will develop, maintain, and test: data pipelines, application framework, infrastructure for data generation and work closely with Data Scientists to enable their work using modern data architecture and tools.
Job Description
Manulife has a clear vision for a Global Data Strategy. By liberating and strengthening Manulife’s data capabilities we will enable deeper insights, better product and service design, and more effective business processes. The result will be exceptional experiences for our customers.

Key Responsibilities:
Leveraging new & existing Big Data & Cloud technologies contributing to the innovative design, development and management of data analytics labs supporting to increase knowledge and insight from enterprise data
Perform technical systems and data flow development in a variety of projects for complex front, middle and back office applications, with a focus on the reporting and analytics environment; this environment is built on a Microsoft Azure cloud and is underpinned by a Hortonworks Hadoop stack
Perform technical systems and data flow design for small-to-medium sized projects
Work with multiple project execution and deployment teams (e.g. Development, Business Analysis, Architecture, Release Management, Production Support)
Work closely with the technical leads and architecture teams, and align solutions that meet the departmental architectural vision
Assess the completeness and accuracy of data, identifying gaps in data, provide feedback to business and system owners with guidance and options to obtain missing information
Design, build and implement modern data architectures in development and production environments (data orchestration pipelines, data sourcing, cleansing, augmentation and quality control processes)
Translates business needs into data engineering and architecture solutions
Contributes to overall solution, integration and enterprise architectures
Build and support deploying machine learning models in development and production environments
Provide proactive data ingestion and analysis of large structured and unstructured datasets involving a wide range of systems across Group Functions (i.e., Finance, Treasury, Risk, Human Resources, Brand & Communications)
Evaluating existing and proposed data models and how to best access and query them as well as existing and proposed data interfaces and how to clearly document them, including specification of data flow models, data flow timing, data mapping, and data transformation rules including data validations and controls

Education, Experience & Skills:
Demonstrated 2-5 years of professional experience in related industry experience in working in big data/data management & understanding big data analytic tooling and environments including a University degree and or Master’s degree in Engineering, Computer Science or equivalent quantitative program
Experience in Big Data, Analytics and Business Intelligence technologies to support design, build and implementation for advanced analytics and business intelligence reporting;
Experience working with Cloudera and/or Hortonworks Hadoop stack
Experience with big data processing frameworks and techniques such as HDFS, MapReduce, Syncsort, Sqoop, Oozie, Storage formats (Avro, Parquet), Stream processing (NiFi, Kafka), etc.
Understanding of relational and warehousing database technology working with Hadoop and other major databases platforms (e.g., Hadoop, Oracle, SQLServer, Teradata, MySQL, or Postgres)
Experience in data technologies and use of data to support software development, advanced analytics and reporting. Focus on Cloud (Azure), Hadoop-based technologies and programming or scripting languages like Java, Scala, Linux, C++, PHP, Ruby Python, R and SAS.
Knowledge regarding different databases such as Hawq/HDB, MongoDB, Cassandra or Hbase.
Working knowledge of modern data streaming using Kafka, Apache Spark and data ingestion frameworks: NiFi, Hive and Pig
Experience writing complex SQL and NoSQL jobs to analyze data in both traditional DBMS (MS-SQL, Oracle) and Big Data environments (i.e., HADOOP, SPARK, or similar open source and commercial technologies)
Knowledge of non-relational (Cassandra, MongoDB) databases preferred
Predictive analytics and machine learning experience (scikit-learn, Tensorflow, MLlib, recommendation systems) preferred
Experience with integrating to back-end/legacy environments
Experience integrating business and technology teams
Knowledge and familiarity with machine learning models application and production pipelines
Collaborative attitude, willingness to work with team members; able to coach, participate in code reviews, share skills and methods
Remains current with emerging technologies, innovations and practices within the data and analytics industry
Good organizational and problem-solving abilities that enable you to manage through creative abrasion
Good verbal and written communication; effectively articulates technical vision, possibilities, and outcomes
Strong work ethic, results oriented, and accuracy / attention to detail are critical; ability to work in agile or scrum delivery environments
Exceptional oral, written and interpersonal communication skills with the ability to simplify complex technical concepts into business & value-focused language. A key requirement is to communicate clearly and consistently keeping stakeholders well-informed of progress and challenges
Excellent organizational and time management skills, strong business presence with ability to multi-task and effectively deal with competing priorities. Ability to work with minimal or no supervision while performing duties; has the ability and initiative to organize various functions and be a strong team player.
What about Perks?
Manulife has lots of perks including, but not limited to:
Competitive compensation
Retirement Savings Accounts including a RPP (Pension Plan), RRSP (Retirement Savings Plan), and TFSA (Tax Free Savings account)
Manulife Share Ownership Program with employer matching
Customizable Benefits Package including Health, Dental, Vision, and 100% of Mental Health expenses
Financial support for ongoing training, learning, and education
Monthly Innovation Days (Hackathons)
Wearing jeans to work every day
An abundance of career paths and opportunities to advance.
This is a full-time, permanent role located in Toronto, Ontario.

If you are ready to unleash your potential it’s time to start your career with Manulife/John Hancock.
About Manulife
Manulife Financial Corporation is a leading international financial services group that helps people make their decisions easier and lives better. With our global headquarters in Toronto, Canada, we operate as Manulife across our offices in Canada, Asia, and Europe, and primarily as John Hancock in the United States. We provide financial advice, insurance, and wealth and asset management solutions for individuals, groups and institutions. At the end of 2019, we had more than 35,000 employees, over 98,000 agents, and thousands of distribution partners, serving almost 30 million customers. As of March 31, 2020, we had $1.2 trillion (US$0.8 trillion) in assets under management and administration, and in the previous 12 months we made $30.4 billion in payments to our customers. Our principal operations are in Asia, Canada and the United States where we have served customers for more than 155 years. We trade as 'MFC' on the Toronto, New York, and the Philippine stock exchanges and under '945' in Hong Kong.
Manulife is an equal opportunity employer. We strive to attract, develop and retain a workforce that is as diverse as the customers we serve and to foster an inclusive work environment that embraces the strength of cultures and individuals. We are committed to fair recruitment, retention and advancement and we administer all of our practices and programs based on qualification and performance and without discrimination on any protected ground.
It is our priority to remove barriers to provide equal access to employment. A Human Resources representative will consult with applicants contacted to participate at any stage of the recruitment process who request any accommodation. Information received regarding the accommodation needs of applicants will be addressed confidentially.	Senior Data Engineer	https://ca.indeed.com/rc/clk?jk=71cd34c1d32326c5&fccid=1747adf6142beb48&vjs=3	71cd34c1d32326c5	Manulife
2020081669	Toronto, ON	Lead Data Engineer	4ecf6d90140603ee	Manulife	Are you looking for unlimited opportunities to develop and succeed? With work that challenges and makes a difference, within a flexible and supportive environment, we can help our customers achieve their dreams and aspirations.
Job Description
Are you a go-getter who has a passion in building next gen data pipelines and provide Big data solution for business problems? Are you a big fan of simplification and automation?
Manulife is seeking an awesome Lead Data Engineer , with Big Data experience as well as strong understanding of data-ingestion, data curation and both Batch & Stream Data processing, to join our rapidly expanding IT Organization and assist us as we work to be a digital leader in our industries!
Skills and Experience
You will have the following skills and experience:
Lead development teams to define and build data pipelines
Expert in building and operationalizing BigData platforms in cloud using one of the public clouds, preferably MS Azure.
Hands on experience with Big Data streaming frameworks and tools (Spark Streaming, Storm, Kafka, etc.)
Expert in Hadoop ecosystem and toolset – Sqoop, Nifi, Pig, Spark, HDFS, Hive, HBase, etc.
Expert in automating data pipelines in a Big Data ecosystem, DevOps and CICD.
Experience in developing Hadoop integrations (batch or streaming) for data ingestion, data mapping and data processing capabilities
Experience programming in both compiled languages (Java, Scala) and scripting languages (Python or R)
Expert in developing Big Data set processes for data modeling, mining and production
Experience in working with key partners including business and technology to establish definition of success, goals, key use cases and aligning dev team on strategic priorities.
Excellent communication and interpersonal skills
Excellent analytics, problem solving and solutioning skills
A capacity for constant learning from both success and failure, remaining open to change and continuous improvement
Good to Haves
Experience in Exploratory data analysis; Query and process Big Data, provide reports, summarize and visualize the data
Experience in Canary deployments, 0-downtime, 0-dataloss, hot-hot DR
Experience in designing solutions for Big Data warehouses
Experience with Hadoop security frameworks like Knox, Ranger.
Experience with Hadoop metadata frameworks and security policies such as Ranger, Atlas
Experience in data profiling and analysis
Exposure to and an understanding of Agile scrum methodologies and experience of working in an Agile team
Experience in Big Data performance analysis, tuning and capacity planning
Experience in designing business intelligence systems, dashboard reporting, and analytical reporting is a plus
Experience with the Hortonworks Data Platform (version 2.5)
Experience in using Git flow.
Basic understanding of following will be useful but not required:
Exposure to and basic understanding of collaboration tools like Slack, Skype, Teams, and JIRA
What about Perks?
Manulife has lots of perks including, but not limited to:
Competitive compensation
Retirement Savings Accounts including a RPP (Pension Plan), RRSP (Retirement Savings Plan), and TFSA (Tax Free Savings account)
Manulife Share Ownership Program with employer matching
Customizable Benefits Package including Health, Dental, Vision, and 100% of Mental Health expenses
Financial support for ongoing training, learning, and education
Monthly Innovation Days (Hackathons)
Wearing jeans to work every day
An abundance of career paths and opportunities to advance
A flexible work environment with flex hours, work from home arrangements, distributed teams, and condensed work week arrangements.
This is a full time permanent role and the team is located in Kitchener/Waterloo, Ontario. There is opportunity for Toronto based people to work in this role, however there would be travel to Kitchener / Waterloo twice per week.
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
About Manulife
Manulife Financial Corporation is a leading international financial services group that helps people achieve their dreams and aspirations by putting customers' needs first and providing the right advice and solutions. We operate primarily as John Hancock in the United States and Manulife elsewhere. We provide financial advice, insurance, as well as wealth and asset management solutions for individuals, groups and institutions. At the end of 2017, we had approximately 34,000 employees, 73,000 agents, and thousands of distribution partners, serving more than 26 million customers. As of December 31, 2017, we had over $1.04 trillion (US$829.4 billion) in assets under management and administration, and in the previous 12 months we made $26.7 billion in payments to our customers. Our principal operations are in Asia, Canada and the United States where we have served customers for more than 100 years. With our global headquarters in Toronto, Canada, we trade as 'MFC' on the Toronto, New York, and the Philippine stock exchanges and under '945' in Hong Kong

If you are ready to unleash your potential it’s time to start your career with Manulife/John Hancock.
About Manulife
Manulife Financial Corporation is a leading international financial services group that helps people make their decisions easier and lives better. We operate primarily as John Hancock in the United States and Manulife elsewhere. We provide financial advice, insurance, as well as wealth and asset management solutions for individuals, groups and institutions. At the end of 2018, we had more than 34,000 employees, over 82,000 agents, and thousands of distribution partners, serving almost 28 million customers. As of March 31, 2019, we had over $1.1 trillion (US$849 billion) in assets under management and administration, and in the previous 12 months we made $29.4 billion in payments to our customers.
Our principal operations in Asia, Canada and the United States are where we have served customers for more than 100 years. With our global headquarters in Toronto, Canada, we trade as 'MFC' on the Toronto, New York, and the Philippine stock exchanges and under '945' in Hong Kong.

Manulife is an equal opportunity employer. We strive to attract, develop and retain a workforce that is as diverse as the customers we serve and to foster an inclusive work environment that embraces the strength of cultures and individuals. We are committed to fair recruitment, retention and advancement and we administer all of our practices and programs based on qualification and performance and without discrimination on any protected ground.
It is our priority to remove barriers to provide equal access to employment. A Human Resources representative will consult with applicants contacted to participate at any stage of the recruitment process who request any accommodation. Information received regarding the accommodation needs of applicants will be addressed confidentially.	2020-08-16T23:47:27.000Z	https://ca.indeed.com/rc/clk?jk=4ecf6d90140603ee&fccid=1747adf6142beb48&vjs=3
2020081670	Toronto, ON	Senior Data Engineer	https://ca.indeed.com/rc/clk?jk=256688ca085e919f&fccid=5d784228b1eee537&vjs=3	256688ca085e919f	2020-08-16T23:47:27.000Z	Myant	About us:
At Myant, we are creating the world’s first textile computing platform, integrating technology directly into the only thing we’ve been wearing our entire life – clothing. SKIIN is our first consumer facing brand, and SKIIN’s vision is to enhance human ability through connected clothing - think Ironman’s suit, but comfortable. The sensors and actuators embedded within our apparel create your Digital Identity, which will be consumed by those who matter to you - your family members, doctors, coaches, other IoT devices - without you consciously having to think about it. Imagine a world where you walk into your house and the temperature automatically adjusts to your optimal body temperature, the lights adjust to your mood, you can monitor and adjust your everyday lifestyle based on your vital signs, or your doctor is aware of the onset of a disease before you even visit. The line between the digital and physical world is becoming increasingly blurry, and we believe textile is the next medium to bridge that gap.
We’re looking for people who believe in our mission to make wearable technology truly ubiquitous and convenient, so that everyone can benefit from it. We are a cross-functional team solving big challenges at the intersection of fashion, electronics, software, and data science.
Responsibilities:
Test the performance of the algorithms developed by Data Science team
Leverage native APIs for integration of AWS platforms
Take ownership of all your deliverables and communicate your results to timely project delivery
Prepare reports and some technical documentations

Qualifications Required:
Bachelor’s Degree in Computer Science, Computer Engineering, or equivalent work experience
Proficiency with JavaScript and Python language
Basic knowledge of machine learning algorithm and libraries like keras, tensorflow, sklearn
Experience in building RESTful APIs following Micro-Services Architecture
Experienced in NodeJS, PostgreSQL, and GraphQL.
Significant experience in building microservices leveraging various AWS features (AWS Lambda, IAM, SQS, DynamoDB, Kinesis, Redshift, Aurora, EC2, S3, API Gateway etc.)
Solid experience in Biomedical signal processing, and data mining related to physiological patient data is a bonus
Powered by JazzHR
8SQJftU9nl
2020081671	Toronto, ON	Data Developer	2020-08-16T23:47:28.000Z	https://ca.indeed.com/rc/clk?jk=2ef0d527757decfa&fccid=c0b5558e336243b3&vjs=3	COMPANY OVERVIEW:
Success stories like this, don’t happen every day. From humble beginnings as a courier industry solutions provider in Canada, Fleet Complete quickly grew to be one of the world’s leaders in telematics and connected mobility solutions for a wide variety of industries with fleets, assets and mobile workers.

Today, with 20 years in the industry, Fleet Complete is one of the fastest-growing IoT (Internet of Things) companies across the globe, operating in 17 countries with offices in Canada, Netherlands, Denmark, Belgium, Estonia, Latvia, Lithuania and Australia. Fleet Complete continues to win employer, innovation thanks to our relentless customer-centric approach and commitment to company values of Innovation, Quality, Customers, Productivity, People and Community.

Thanks to strong partnerships and sound investments, our trusted Fleet and Mobile workforce platform provides real-time insights, visibility, employee safety and overall operational efficiency. This helps organizations, municipalities and businesses of all sizes to modernize their operations with ease. Fleet Complete is known for hiring, growing and empowering talented people who develop innovative products, build powerful relationships and provide personalized support that is unparalleled in our industry. Join Fleet Complete on our next chapter and we can work together to "help fleets thrive".

Proud to be named one of Greater Toronto’s Top Employers for 2020: http://content.eluta.ca/top-employer-fleet-complete

ESSENTIAL DUTIES & RESPONSIBILITIES:

Create and maintain optimal data pipeline architecture for legacy and the new architecture of IoT streaming data (Telematics and other automotive sensors)
Assemble large, complex data sets that meet functional / non-functional business requirements for data and application products
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics
Work with the Product team and other stakeholders to assist with data-related technical issues and support their data infrastructure needs
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader

QUALIFICATIONS:
All applicants must possess the following:

5+ years of experience in a Data Engineer role
A degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field (Graduate degree would be a plus)
Experience with cloud technologies. Specifically, AWS technologies such as S3, Glacier, Lambda, Athena, Redshift
Experience with object-oriented & functional scripting languages including Python and Java
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Strong analytic skills related to working with unstructured datasets
Build processes supporting data transformation, data structures, metadata, dependency and workload management
A successful history of manipulating, processing and extracting value from large disconnected datasets
Strong project management and organizational skills
Experience supporting and working with cross-functional teams in a dynamic environment
Understands and helps drive business impact via data systems and their resulting output

Fleet Complete will provide support in its recruitment processes to applicants with disabilities, including accommodation that takes into account an applicant's accessibility needs. If you require accommodation during the interview process, please contact the Recruitment Team, 866-649-7949.

Fleet Complete is an equal opportunity employer committed to diversity and inclusion. We are pleased to consider all qualified applicants for employment without regard to race, color, religion, sex, national origin, age, disability, protected veterans’ status or any other legally protected factors.	2ef0d527757decfa	Fleet Complete
2020081672	Toronto, ON	0a0e41fb84eb4708	2020-08-16T23:47:29.000Z	https://ca.indeed.com/rc/clk?jk=0a0e41fb84eb4708&fccid=e1b2607798446d2b&vjs=3	Senior/Lead Big Data Engineer	Punchh	Headquartered in the Bay Area with offices in Austin, TX, Toronto, Canada and Jaipur, India, venture-funded Punchh is the world leader in innovative digital marketing products for brick and mortar retailers, combining AI technologies, mobile-first expertise, and Omni-Channel communications designed to dramatically increase customer lifetime value. Leading global chains in the restaurant, health and beauty sectors rely on Punchh to grow revenue by building customer relationships at every stage to becoming brand loyalists, including more than 100 different chains representing more than $12B in annual spend, 30,000 locations globally, 26M+ consumers, and 1M+ transactions daily. Punchh boasts a customer list that includes Pizza Hut, Quiznos, Coffee Bean & Tea Leaf and many more.

Title

Senior/Lead Big Data Engineer

Location

Toronto, Ontario, Canada

Reporting to

Sr. Dir. of Data

About the role

Punchh is looking for a Senior/Lead Big Data Engineer that will play a critical role in leading Punchh's data innovations. He/she will help create cutting-edge Big Data solutions by leveraging his/her prior industrial experience. This role requires close collaboration with the Machine Learning, Software Engineering, and Product Departments. You will be given the opportunity to not only serve internal teams, but also our business clients as well.

What You'll Do

Punchh is seeking to hire Big Data Engineer at either a senior or tech lead level. Reporting to the Director of Big Data, he/she will play a critical role in leading Punchh's big data innovations. By leveraging prior industrial experience in big data, he/she will help create cutting-edge data and analytics products for Punchh's business partners.

This role requires close collaborations with data, engineering, and product organizations. His/her job functions include

Work with large data sets and implement sophisticated data pipelines with both structured and structured data.
Collaborate with stakeholders to design scalable solutions.
Manage and optimize our internal data pipeline that supports marketing, customer success and data science to name a few.
A technical leader of Punchh's big data platform that supports AI and BI products.
Work with infra and operations team to monitor and optimize existing infrastructure
Occasional business travels are required.

What You'll Need

5+ years of experience as a Big Data engineering professional, developing scalable big data solutions.
Advanced degree in computer science, engineering or other related fields.
Demonstrated strength in data modelling, data warehousing and SQL.
Extensive knowledge with cloud technologies, e.g. AWS and Azure.
Excellent software engineering background. High familiarity with software development life cycle. Familiarity with GitHub/Airflow.
Advanced knowledge of big data technologies, such as programming language (Python, Java), relational (Postgres, mysql), NoSQL (Mongodb), Hadoop (EMR) and streaming (Kafka, Spark).
Strong problem solving skills with demonstrated rigor in building and maintaining a complex data pipeline.
Exceptional communication skills and ability to articulate a complex concept with thoughtful, actionable recommendations.

Benefits

Healthcare coverage
Life and AD&D insurance
Competitive salaries, bonus and stock options
Professional development
Paid Time off
Paid holidays
Free lunch in the office.

Punchh is proud to provide equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. If you'd like more information about your EEO rights as an applicant, please click here.

We also provide reasonable accommodations to individuals with disabilities in accordance with applicable laws.
Notice to recruiters and placement agencies: If you are a recruiter or placement agency, please do not submit résumés to any person or email address at Punchh prior to having a signed agreement with Human Resources. Punchh is not liable for and will not pay placement fees for candidates submitted by any agency other than its approved recruitment partners. Also, any résumés sent to us without an agreement in place will be considered your company's gift to Punchh and may be forwarded to our Talent Acquisition team.
2020081673	Toronto, ON	https://ca.indeed.com/rc/clk?jk=5c06518576e750fc&fccid=b704562e07a2a03f&vjs=3	5c06518576e750fc	SADA	2020-08-16T23:47:29.000Z	Senior Data Engineer	Join SADA as a Sr. Data Engineer!


Your Mission

As a Sr. Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data problems facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring batch or streaming data pipelines, data lakes and data warehouses.

You will be expected to run point on whole projects, end-to-end, and to mentor less experienced Data Engineers. You will be recognized as an expert within the team and will build a reputation with Google and our customers. You will demonstrate repeated delivery of project architectures and critical components that other engineers demur to you for lack of expertise. You will also participate in early-stage opportunity qualification calls, as well as lead client-facing technical discussions for established projects.

Pathway to Success

#BeOneStepAhead: At SADA we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our engineers know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs.

Your success starts by positively impacting the direction of a fast-growing practice with vision and passion. You will be measured quarterly by the breadth, magnitude, and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, how well you collaborate with your peers, and the consultative polish you bring to customer interactions.

As you continue to execute successfully, we will build a customized development plan together that leads you through the engineering or management growth tracks.

Expectations

Required Travel - 30% travel to customer sites, conferences, and other related events. Due to the COVID-19 pandemic, travel has been temporarily restricted.
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:
Google Professional Data Engineer Certified or able to complete within the first 45 days of employment

Required Qualifications:
Mastery in at least one of the following domain areas:
Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or customer-facing role

Useful Qualifications:
Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Experience operationalizing machine learning models on large datasets
Demonstrated leadership and self-direction - a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem

About SADA

Values: We built our core values on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA's values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

Make them rave
Be data driven
Be one step ahead
Be a change agent
Do the right thing

Work with the best: SADA has been the largest partner in North America for GCP since 2016 and has been named the 2019 and 2018 Google Cloud Global Partner of the Year. SADA has also been awarded Best Place to Work by Inc. as well as LA Business Journal!

Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match, professional development reimbursement program as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.
2020081674	Cigna	2020-08-16T23:47:30.000Z	Mississauga, ON	Senior Data Engineer	The Data Engineer will be responsible for expanding and optimizing the data and data pipeline architecture across the enterprise. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, data architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing Express Script Canada’s data architecture to support our next generation of products and data initiatives.

ESSENTIAL FUNCTIONS:
Create and maintain optimal data pipeline architecture.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Optimize the value of technology investments in data management for the business by aligning the business architectures with technology architectures. Identify and drive key technology investments to meet business objectives, remediate technology and process gaps. Determine feasibility, cost and time required, compatibility with current system, and system capabilities.
Identify product, technology and process gaps in current data & technology architectures and recommend solutions to bridge gaps between the business and the data & technology deployed to support the business.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Coordinate with data users and key stakeholders across ESC’s Lines of Business to refine and achieve various long-term objectives for data architecture
Work with stakeholders including Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product.
Work with data and analytics experts to strive for greater functionality in our data systems.

QUALIFICATIONS:
Minimum 6 years of experience in a large-scale, multi-platform, multi-tier processing environment with a Bachelor’s degree in Information Systems or related field
Extensive experience on projects implementing Master Data Management and Enterprise Content Management techniques and platforms.
Expert domain knowledge & experience in data warehousing, reporting and advanced analytics platforms, encompassing data model design, dimensional modeling, master data management, cross-cutting concerns, common integration technologies, patterns & standards and emerging technologies.
Advanced knowledge of data architecture and analytics principles and practices for application development and data warehousing purposes.
Advanced knowledge of data model development and governance methods and practices
Previous data management experience in a data warehouse and data lake environment.
Experience with data center migrations, enterprise database consolidation, data warehouse migration and/or consolidation to data lakes or integrated data architectures.
Experience building multi-cloud analytics and technology strategies.
Highly proficient in Relational Database Management Systems
(RDBMS) and Big Data, data architecture, data modeling (including dimensional),
data warehousing, object-oriented methodologies, and client/server development.
Advanced knowledge of PowerDesigner or similar Data Modelling Tool
Advanced knowledge of Talend or similar Data Integration Tool
Hands-on experience using data technologies to implement data profiling tools and modern database implementations including Enterprise Data Lakes, graphDB, key-value pair, column-store, Big Table, RDF, In-Memory DB, etc..
Working knowledge of Hadoop platforms (Hortonworks or Cloudera) and key technologies like Apache Nifi, Kafka, Spark, Pig, Hive, NoSQL databases like MongoDB, Cassandra or Hbase.
Experience implementing data visualization tools like Tableau, Qlik, etc.
Willingness to work a flexible schedule to accommodate project deadlines and travel requirements

Assets:
Knowledge of the group health insurance (pharmacy, dental, other health) industry or adjudication systems is an asset
Knowledge of advanced analytics tools like R, SAS, and machine learning algorithms.
Knowledge of one or more programming or scripting languages like Java, C, C#, .Net, Javascript, PHP, Python
Knowledge of the DAMA Book of Knowledge
Knowledge of Big Data & Logical Warehouse architecture
Knowledge of TOGAF, Zachman or other
architecture frameworks

ABOUT EXPRESS SCRIPTS CANADA

Express Scripts Canada, a registered business name of ESI Canada, an Ontario partnership indirectly controlled by Express Scripts, Inc. (Nasdaq: ESRX), is one of Canada’s leading providers of health benefits management services. From its corporate headquarters in Mississauga, Ontario, just outside Toronto, Express Scripts Canada provides a full range of integrated pharmacy benefit management (PBM) services to insurers, third-party administrators, plan sponsors and the public sector, including health-claims adjudication and processing services, Home Delivery Pharmacy Services, benefit-design consultation, drug-utilization review, formulary management, and medical and drug-data analysis services, to better facilitate the best possible health outcomes at the lowest possible cost.

It will be a condition of employment that the successful candidate receives the Enhanced Reliability Clearance from the Federal Government. The candidate will be required to provide supporting documentation in order to receive Clearance if required.

We offer a competitive salary, along with a positive work environment built on solid corporate values, integrity, mutual respect, collaboration, passion, service and alignment.

We are an equal opportunity employer that promotes a diverse, inclusive and accessible workplace. By embracing diversity, we build a more effective organization that empowers our employees to be the best that they can be.

We are committed to creating a working environment that is barrier-free and we are prepared to provide accommodation for people with disabilities. Thank you for your interest in this position, however only qualified candidates will be contacted for an interview. No telephone calls please.

For more information about Express Scripts Canada, visit its Web site at www.express-scripts.ca

About Cigna
Cigna Corporation (NYSE: CI) is a global health service company dedicated to improving the health, well-being and peace of mind of those we serve. We offer an integrated suite of health services through Cigna, Express Scripts, and our affiliates including medical, dental, behavioral health, pharmacy, vision, supplemental benefits, and other related products. Together, with our 74,000 employees worldwide, we aspire to transform health services, making them more affordable and accessible to millions. Through our unmatched expertise, bold action, fresh ideas and an unwavering commitment to patient-centered care, we are a force of health services innovation.

When you work with Cigna, you’ll enjoy meaningful career experiences that enrich people’s lives while working together to make the world a healthier place. What difference will you make? To see our culture in action, search #TeamCigna on Instagram.	60d7283f8c769ec7	https://ca.indeed.com/rc/clk?jk=60d7283f8c769ec7&fccid=afbf8c270610a38a&vjs=3
2020081675	Expert BI/ETL Engineer (Tech Lead Cloud Data Engineer)	Finastra	2020-08-16T23:47:31.000Z	https://ca.indeed.com/rc/clk?jk=1aca9ca80ec23fec&fccid=1f9d0530a51ff611&vjs=3	Mississauga, ON	What will you contribute?
Reporting to the Senior Manager, Development, the role of the Expert BI Developer is to ensure the effective design and delivery of the Student Lending reporting solution. This hands-on role serves as a Technical Lead for the Business Analytics and Reporting team providing development, technical guidance, review and support.
Responsibilities & Deliverables:

Your deliverables as an Expert BI Developer will include, but are not limited to, the following:
Develop and deliver a robust Reporting and Business Analytics framework, well aligned with the company’s long-term strategic goals for data architecture vision.
Ensure the solution supports Student Lending client data needs and can be easily extended to newly acquired clients and their standards.
Liaise with vendors and service providers to select the products or services that best meet company cost and performance goals related to data architecture and analytics
Working closely with both enterprise level and project level team - data owners, stewards, users, business analysts, developers, quality analysts, department managers, architects and other stakeholders to understand reporting requirements and ensure strategic goals and tactical implementation are in alignment.
Translate project requirements into functional and non-functional specifications for BI reports and applications.
Lead conceptual and physical design, development and implementation of enterprise level BI and ETL framework, conforming to well defined business, technical rules and SLAs, preserving reusability of artefacts, single version of truth, centralization of logic, testability and well-designed error handling.
Prepare all necessary documentation that clearly describes solution and Meta data.
Monitor, tune up and administer BI Environments for quality and optimal performance purpose. Debug, monitor and troubleshoot BI solutions.
Be aware of and comply with all corporate and department policies, procedures and standards that apply to your work area.
Ensure that Reporting and Business Analytics strategies and architectures are in regulatory compliance .
Required Skills & Experience:
8+ years' of hands-on experience developing BI and Reporting Solutions.
Experience with business requirements analysis, entity relationship planning, data modeling, database design, reporting structures.
Direct experience in implementing enterprise data management processes, procedures, and support on data monitoring.
Understanding of large scale DB and reporting solution design, Source to Target Mappings, distributed DB design, multi environment structures, logical DB partitioning strategy, data archiving and retention, design and development of reporting semantic layer and view objects and logic
Expert knowledge of MS SQL
Expert hands-on experience with Azure Cloud Data Engineering suite: ADF, Databricks, Azure Data Lake, Spark, Azure SQL and Azure SQL Data Warehouse
Experience with DAX, Tabular and Power BI.
Experience with data processing flowcharting techniques.
Experience developing and maintaining ETL tools and platforms such as SSIS, Azure ADF
Experience with data architecting, large-scale data modeling, and business requirements gathering/analysis.
Strong understanding of BI Reporting & ETL technologies, relational and dimensional data structures, Big Data hands-on experience, principles, and best practices.
Strong familiarity with metadata management and associated processes.
Demonstrated expertise with repository creation, and data and information system life cycle methodologies.
Understanding of Web services (SOAP, XML, REST, JSON, UDDI).
Good knowledge of applicable data privacy practices and laws.
#LI-MG1
*************************************************************************************************************
The above statements describe the general nature and level of work being performed by people assigned to this job. They are not intended to be an exhaustive list of all responsibilities, duties, and skills required. Reasonable accommodations may be made to enable qualified individuals with disabilities to perform the essential job functions. If you need assistance or an accommodation due to disability please contact your recruitment partner.
*************************************************************************************************************	1aca9ca80ec23fec
2020081676	Location: Markham, Ontario
Job Description:
Job Summary
This role requires a dynamic individual with experience and passion for using data and analytics to drive business results and help us build the foundation to use our rapidly expanding data lake to improve business decisions. As a member of the Marketing team, the Data Engineer will be a key contributing resource to continually measure, evaluate, and make recommendations on our website marketing efforts as well as ecommerce user-experience.
Responsibilities
• Design, implement, track performance and refresh advanced analytic automated segmentation and predictive models
• Monitor Data quality with IT to ensure robust and accurate data
• Partner with IT to define data solutions from new data sources, and build requirements for extraction into the data lake
• Develop a strong acumen in source and downstream data storage system, and understand how the data is associated with business actions and potential solutions
• Will function as the primary liaison between marketing and IT
Experience and Education
• 5+ years designing data processes to automate organizational decisions ideally in a retail organization
• Bachelor's Degree in Statistics, Business, Quantitative Economics, Mathematics, Marketing, Economics, Engineering, Operations Research or similar programs
Competencies and Skills
• Strong Personal Drive for Excellence
• Excellent organizational and time management skills, with the ability to manage multiple priorities in a high demand environment
• Great sense of urgency and accountability, results-oriented with strong execution skills
• An independent problem solver, must be able to find creative solutions to unusual or unprecedented questions
• Quick learner – eagerness to learn about new tools and business systems to help craft solid solutions
Technology
• Experience in relational and cloud data storage using advanced SQL, using technologies such as Snowflake, Oracle, SQL Server
• Must be proficient in R, Python, Alteryx, Azure ML, or similar Machine Learning and data processing technology,
• Solid understanding of BI tools such as Tableau, MicroStrategy or QlikView
• Experience in Spark, Kafka, JAVA a strong plus
• Proficiency in processing Web Analytic raw data, ideally from Google Analytics 360, a large plus
• Snowflake, Orcale, SQL ServerSnowflake, Oracle, SQL Server
• Must be proficient in either R or Python
• Solid understanding in data visualization tools such as Tableau, MicroStrategy or QlikView
• Experience in Spark, Kafka, JAVA a strong plus
• Very comfortable with MS office (Word/Excel/Access/PowerPoint)
• Understanding of using SQL against relational databases
• Experience in data visualization and reporting tools, such as Tableau
#INDC	Pet Valu	Markham, ON	https://ca.indeed.com/rc/clk?jk=e1337dfa4d5c3f3b&fccid=95581e84c2b81b02&vjs=3	Data Engineer	2020-08-16T23:47:33.000Z	e1337dfa4d5c3f3b
2020081677	Toronto, ON	00df245dd038b055	Fiix	Why Fiix?
Fiix has a big goal – to create a more sustainable world. Sounds lofty right? Our mission is to make every maintenance team successful by enabling the adoption of a CMMS and we’re off to a great start. Teams that are part of the world’s most well known brands (like Toyota, Siemens, and Sara Lee) manage their maintenance activities and achieve greater results with Fiix. But we’re not stopping there. Our team is growing by leaps and bounds and we’re conquering new challenges every day. We’re looking for big thinkers with small egos to join us on our journey to create a more sustainable world.

Why we do it?
We’re a team of market disrupting, like-minded individuals. We all do things our own way, but we come together each and every day to create and deliver the long awaited answer to an antiquated industry – and we have a lot of fun while we’re at it.

We’re looking for a Data Engineer to help take Fiix’s explosive growth to a whole new level. We are looking for an experienced Data Engineer to build our data layer to support the delivery of machine learning-driven products. This is a unique opportunity to join a team of creative and passionate individuals committed to bringing AI to the predictive maintenance world.

What You Will Do:
Work with software engineering to design, build, maintain and optimize our data management and analytics pipeline
Perform data analysis, quality assessments, cleaning, imputation and data aggregation tasks
Perform feature engineering and selection to support machine learning activities
Build data processing pipelines and automate data pipelines in production environments
Work with engineers and data scientists to deploy analytics capabilities and machine learning models in production
Work with engineering and data teams to ingest and structure high throughput IoT data
Develop processes and frameworks to ensure data and model quality
Perform code reviews and testing to ensure software quality is high and requirements are met
Diagnose and repair data issues and assist customers with technical problems
What We're Looking for:
3+ years experience in a high growth software development environment developing data-driven products
Experience working on ETL, data warehousing, data modeling, data architecting, data analysis
Experience with at least some of: 1) Data streaming with Kinesis, Kafka or similar 2) ETL orchestration frameworks such as Airflow, Luigi or similar 3) Data warehouses such as Snowflake or similar
Development skills in Python, MySQL and other relational databases, NoSQL databases such as DynamoDB, Redis or similar
Experience with AWS or other cloud providerEducation background:
Bachelor’s Degree or higher in Computer Science or a related field
Equity Statement

At Fiix, we recognize that people come with a wealth of experience and talent beyond just the technical requirements of a job. If your experience is close to what you see listed here, please still consider applying. Diversity of experience and skills combined with passion is a key to innovation and excellence. Therefore, we encourage people from all backgrounds to apply to our positions. Please let us know if you require accommodations during the interview process.	https://ca.indeed.com/rc/clk?jk=00df245dd038b055&fccid=87852d99a20def3f&vjs=3	Data Engineer - Applied AI	2020-08-16T23:47:34.000Z
2020081678	Toronto, ON		668db5396375205e	Big Data Developer	https://ca.indeed.com/rc/clk?jk=668db5396375205e&fccid=d2841a5c0380b93d&vjs=3	2020-08-16T23:47:35.000Z	CGI
2020081679	Toronto, ON	2020-08-16T23:47:38.000Z	https://ca.indeed.com/rc/clk?jk=3cc1d76cb36c1303&fccid=4e917d9a3b14765d&vjs=3	Paytm	About Paytm Labs:
 At Paytm Labs, we build technologies that powers Paytm India, the world's’ fastest growing mobile payments and commerce ecosystem. In addition to, the Paytm Canada app. We use our skills and our biggest asset – data, to make our dent in this universe.

We are committed to offering the most transparent, secure, and personalized consumer experience to over 230 million users. We believe that this kind of scale, and the unique problems that it presents attracts curious candidates like yourself.

Job Description:
If working with billions of events, petabytes of data and optimizing for last millisecond is something that excites you then read on! We are looking for Data Engineers who have seen their fair share of messy data sets and have been able to structure them for building useful AI products.

You will be working on writing frameworks building for real time and batch pipelines to ingest and transform events(108 scale) from 100’s of applications every day. Our ML and Software engineers consume these for building data products like personalization and fraud detection. You will also help optimize the feature pipelines for fast execution and work with software engineers to build event driven microservices.

You will get to put cutting edge tech in production and freedom to experiment with new frameworks, try new ways to optimize and resources to build next big thing in fintech using data!
Requirements:
You have previously worked on building serious data pipelines ingesting and transforming > 10 ^6 events per minute and terabytes of data per day.
You are passionate about producing clean, maintainable and testable code part of real-time data pipeline.
You understand how microservices work and are familiar with concepts of data modelling.
You can connect different services and processes together even if you have not worked with them before and follow the flow of data through various pipelines to debug data issues.
You have worked with Spark and Kafka before and have experimented or heard about Flink/Druid/Ignite/Presto/Athena and understand when to use one over the other.
On a bad day maintaining zookeeper and bringing up cluster doesn’t bother you.
You may not be a networking expert but you understand issues with ingesting data from applications in multiple data centres across geographies, on-premise and cloud and will find a way to solve them.
Proficient in Java/Scala/Python/Spark
What we Offer!
We are proud to announce that we have been certified as a Great Place to Work!
A collaborative, open work environment that fosters ownership, creativity, and urgency
Enrolment in the Group Health Benefits plan right from Day 1, no waiting period
Fuel for the day: Weekly delivery of groceries and all types of snacks to our office
All types of signature drinks from coffee to lattes to cappuccinos
Catered lunch and desserts on a monthly basis!
Ping Pong and Pool: Become the next Paytm Labs Table Tennis/ Pool champ!
And so much more!
Don't have Paytm Canada App yet?
Check us out in the Google Play or App Store.

We thank all applicants, however, only those selected for an interview will be contacted.

Paytm Labs is committed to meeting the accessibility needs of all individuals in accordance with the Accessibility for Ontarians with Disabilities Act (AODA) and the Ontario Human Rights Code (OHRC). Should you require accommodations during the recruitment and selection process, please let us know. Paytm Labs is an equal opportunity employer.	3cc1d76cb36c1303	Data Engineer
2020081680	Job Field:
Information Technology
Job Type:
Full-time
Building Location:
Length of Assignment:

SUMMARY

The Senior Data Engineer is a deep technical expert in building complex data warehousing and business intelligence applications. At this level, the incumbent demonstrates a passion and in-depth knowledge of large, complex application development methodologies. They motivate themselves and the team to refine their skills and adopt best practices for developing pragmatic software solutions for the organization. Leading the charge, they continue to raise the bar on mastery of business intelligence application development within the team and the organization.

KEY DUTIES & RESPONSIBILITIES

Programming

Uses in-depth knowledge of advanced programming techniques, design patterns and hardware/software interfaces to develop business intelligence and data warehouse applications.
Designs, tests and integrates data warehouse and BI modules and resolves programming errors using various debugging tools and techniques.
Provides guidance/mentors on programming practices and techniques to individuals and cross-functional teams.
Provides support, guidance and production assurance for very complex or urgent problems.
Performs work with minimum supervision, and work is assigned in terms of technical objectives.
Prepares technical documentation (e.g., user guides, technical specifications).
Assists in the design of business solutions.

Analysis

Conducts impact analysis for proposed changes to or problems across the system.
Leads team discussions in the analysis and collaboration to clarify and improve specifications or to identify alternative programming solutions.

Continuous Improvement

Makes recommendations or decisions on architecture, application design, standards and process improvements.
Enforces team and organizational standards and practices (e.g. at walkthroughs and peer code reviews).
Engages in continuous learning by developing and executing on a learning plan.
Takes responsibility for individual and the team's results.
Advocates for quality in all aspects of development efforts based on the team's definition of quality.

Risk Management

Estimates and prioritizes work to maximize value while taking into account risk, effort and dependencies.
Raises impediments, risks, and issues as early as possible and work with the team to mitigate as needed.

KNOWLEDGE & SKILLS

University graduation and minimum 5-10 years of relevant experience
Demonstrates in-depth knowledge of Microsoft BI architecture, established data warehouse development methodologies, multi-dimensional data modelling, OLAP, metadata management, data security, predictive analysis and big data processing. Extensive experience in one of these cloud data warehouses (Snowflake, bigtable, Redshift), Data Vault 2.0 methodology, steaming data processing, BI components in SQL Server 2016+, TSQL, and DAX, Power BI.
A good working knowledge of application security, C#, python, PowerShell, metadata management, NoSQL, and data security.
Experience in programming and debugging complex data warehouse and BI applications as part of a multi-disciplinary team environment (following an agile framework such as SCRUM preferred) based on Microsoft Team Foundation Server and git.
Experience in writing unit tests to support production code using a unit test framework.
Experience with database management (i.e. database design, schema creation, concurrency and performance considerations).
Takes ownership and initiative and collaborates well with a team of peers.
Demonstrates a commitment to continuous learning (e.g. user groups, blogs, conferences, community awareness, and next generation tooling).
Able to clearly communicate in both a verbal and written form within a predominantly English working environment.
Has a positive, passionate, idea generating attitude when faced with challenges.

Licenses and/or Professional Accreditation

Certification in Microsoft technologies preferred	bcb75a8b3d33578b	2020-08-16T23:47:39.000Z	Markham, ON	Senior Data Engineer	https://ca.indeed.com/rc/clk?jk=bcb75a8b3d33578b&fccid=84c23fbcabc14d59&vjs=3	BGIS
2020081681	At Bond, we design creative and innovative solutions for our clients, all with the goal of helping them build ever-stronger loyalty to their brands. That can take us in some pretty amazing directions, and as a Data Engineer, you’ll have your hands on the wheel as we drive the future of loyalty.

Working on the bleeding edge of exciting technology, you're afforded the opportunity to experiment with new tools and attempt radically different approaches than traditional software engineering affords. Every day with the Data Engineering team is different and each project presents its own set of new and exciting challenges. Things shift very quickly in our industry and we rely on the Data Engineering team to keep us ahead of the curve and moving in the right direction.
Here's what we want:
Problem Solver: You are curious and loves exploring multiple approaches to find the most efficient, scalable solution and solve a problem
Collaborative: You work well with other people
Passionate: A passion for Big Data and an interest in the latest trends and developments constantly researching new tools and data technologies
Self-starter: You are comfortable helping your team get things done
Here's what you'll be doing:
Design, implement, and maintain data pipelines for extraction, transformation, and loading of data from a wide variety of data sources to various data services
Identify, design, and implement system performance improvements
Identify, design, and implement internal process improvements
Automate manual processes and optimize data delivery
Useful skills/background: You may or may not tick off every box, and that's ok. Each person brings a different background and different skills. If you think you are a good match for what we are looking for tell us why, and tell us what you are doing to improve yourself and we'll see what we can do to help!
A degree in Computer Science/Engineering or related field
2-4 years of experience in a software engineering environment
Experience with SQL and NoSQL systems
Knowledge of Hadoop, Spark, Kafka or other equivalent technologies
Proficiency in some of the following languages: Scala, Java, Python, Bash
Experience with automated testing systems
Mentorship, collaboration, and communication skills
Knowledge of data modelling, data warehousing, ETL processes, and business intelligence reporting tools
Experience working with CI/CD, containerization, and virtualization tools such as Gitlab, Jenkins, Kubernetes, Docker
Experience with tools like Databricks, Snowflake or PowerBI

Why Join Us?
Bond Brand Loyalty is proud to be recognized as one of Canada’s Best Managed Companies.

We’re 400(ish) people working tirelessly together to make the world a more loyal place. You’ll be joining a hyper-talented team with a galaxy of skillsets ranging from research to creative to digital and beyond. You’ll have an excellent opportunity to grow, learn and make an impact as we tackle some of our client’s biggest business challenges.
If you’re looking to build your career, build your skills and build bonds apply today!
Bond Brand Loyalty welcomes and encourages applications from people with disabilities. Accommodations are available on request for candidates taking part in all aspects of the selection process.	2020-08-16T23:47:40.000Z	fd5da69e089b7f27	Mississauga, ON	Bond Brand Loyalty	https://ca.indeed.com/rc/clk?jk=fd5da69e089b7f27&fccid=9343039b36601ad2&vjs=3	Data Engineer
2020081682	Toronto, ON	Perpetua's mission is to give superpowers to anyone that sells online. At the moment, we help media agencies, brands, and Amazon sellers win on Amazon by analyzing large amounts of data and using AI to develop smart optimization algorithms that drive transformational sales growth.

As a Senior Data Engineer, you will be responsible for maintaining and evolving our data infrastructure to support the massive scale of data that we process to power our customers' ad campaigns. This includes advancing our data capabilities to both cut costs and increase performance, while maintaining data integrity and consistency. You will build scalable infrastructure and data pipelines to deliver a platform unparalleled in the advertising space using Google BigQuery, Looker, Airflow and Python (Django, Pandas, SciPy Stack).
What You'll Do
Interact with internal stakeholders to figure out how to make it easier for them to leverage Petabyte-scale data
Accelerate data-informed decision making to transform our product & engineering strategy
Be responsible for developing, maintaining and evolving the Data Platform
Considering technical tradeoffs and advancing our data capabilities
Being responsible for data consistency and integrity both for our internal tools and client APIs
Owning data accessibility and discovery across all parts of the company
Increasing performance of our data pipeline
Evolving our use of available tools and optimising current tools to optimise infrastructure costs
Work in a fast-paced, scaling start-up building software that is truly impactful
What You'll Have
Experience with workflow management tools like Airflow for moving and transforming data
Experience with data mapping, schema design, data structures and algorithms, data quality and integrity
You'll deeply know your datawarehouse from your datalake from your transactional database
Experience designing infrastructure to facilitate both availability and approachability of data to all users (internal and external)
Ability to source requirements from stakeholders and develop a vision for a data-driven company
Ability to build and performance tune complex database queries
Ability to solve problems in new and innovative ways
Self starter who takes initiative; owning a project from start to finish
The ability to communicate complex technical issues in a clear and concise manner
Flexibility to adjust to changing priorities, requirements, and schedules
Experience working in a fast-paced, agile environment
Company Benefits
Impactful work that will help lay the foundation for future projects
Meaningful equity at an early stage company
Ground floor opportunity
Paid-for meals
Unlimited snacks and drinks
Full benefits plus a health spending account
Top of the line technology to help you build your own workspace
Flexible time off policy
At Perpetua, we are dedicated to pursuing and hiring a diverse workforce with varied experiences, perspectives and opinions. We believe diversity helps our team perform better and enables us to build an outstanding product for our customers. We are an equal opportunity employer and are committed to work with applicants requesting accommodation at any stage of the hiring process.	Perpetua Labs	2020-08-16T23:47:40.000Z	Senior Data Engineer	https://ca.indeed.com/rc/clk?jk=9c4363e25aeb557b&fccid=b7254da6203c84b3&vjs=3	9c4363e25aeb557b
2020081683	https://ca.indeed.com/rc/clk?jk=e8f01b72a171ac29&fccid=6df7987324612088&vjs=3	2020-08-16T23:47:41.000Z	goeasy	If you are looking to join one of Canada’s fastest growing companies, goeasy Ltd. is the place for you! Recognized as one of Canada’s Most Admired Corporate Cultures, one of Canada’s Top 50 Fintech’s and one of North America’s Most Engaged Workplaces, we want the best and brightest to join our team.

We are a publicly traded company on the TSX with over 4000% shareholder return since 2001, goeasy operates two main business units. easyfinancial is our consumer lending business that offers secured and unsecured installment loans of up to $35,000 and easyhome is Canada’s largest merchandise lease-to-own company. It is our mission to provide everyday Canadians the chance for a better tomorrow, today by giving them access to the credit they need and by offering them a second chance when they have been turned down by banks and traditional lenders. With a retail network of nearly 400 locations across Canada and over 1900 employees, we are able to build lasting relationships with our customers as we help them rebuild their credit and graduate towards prime rates and a brighter financial future.

The Data Engineer on the Data Science and Business Insights team will build, integrate data from various resources, manage data in goeasy operational data store and enterprise data warehouse. This position will develop ETL (Extract, Transform and Load) with various tools on large datasets to ensure data is easily accessible, works smoothly, as well as maintain and expand the data warehouse for reporting and analysis. The Data Engineer will also work closely with the data architect on the design and architecture of our enterprise data warehouse.

Responsibilities:

Develop data set processes for data modeling, mining and production
Develop and maintain ETL processes using SSIS, Scripting and data replication technologies
Participate in development of datamarts for reports and data visualization solutions
Research opportunities for data acquisition and new uses for existing data
Integrate new data management technologies and software engineering tools into existing structures
Support the translation of business requirements for data acquisition/manipulation and provide detailed specifications that can be passed downstream for use
Develop detailed technical specifications and operational support documentation in collaboration with Business Systems Analysts, BI Engineers and Architects.
Identify and communicate technical problems, process and solutions
Create Ad-Hoc queries and reports as needed along with providing on-going analytical support for these requests
Assist in the collection and documentation of user’s requirements
Ensure that existing business processes dependent on the ODS/EDW are monitored and respond quickly to bug fixes, enhancement requests and production ETL related issues.
Dealing with the database users on a daily basis to ensure that problems are dealt with promptly and that appropriate fixes are made to resolve any problems.
Recommend ways to improve data reliability, efficiency and quality
Ensure systems meet business requirements and industry practices
Work effectively with the Business Intelligence and Data Solutions Architects, Data and BI Engineers to ensure that all approved development and deployment procedures are followed.

Qualifications:

Bachelor’s Degree in Computer Science, MIS, Computer Engineering or other Information Technology related degree
4+ years working with SQL Server or comparable relational database system
3+ years of extensive ETL development experience with SSIS and/or ADF
4+ years of experience troubleshooting within a Data Warehouse environment
Expert domain knowledge & experience in Data warehousing, encompassing data model design, dimensional modeling, naming conventions, cross-cutting concerns, common integration technologies, patterns & standards and emerging technologies.
Expert Knowledge of SQL skills to build, debug, and optimize (developing procedures, functions, SQL queries, etc.) and working with large data sets and to perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
2+ years SQL Server Database administration experience
Cloud experience (Azure) is highly preferred
Exposure and experience with Python, R, Hadoop, Azure, other Big data and advanced analytics
Knowledge of AI and ML developments/solutions/implementations
Experience with multiple programming languages (PowerShell scripting, C#, others) with basic scripting skills.
High level of technical aptitude

Inclusion and Equal Opportunity Employment

goeasy is an equal opportunity employer. In addition, goeasy is committed to providing accommodations for applicants upon request at any stage of the recruitment process in accordance with all legislative requirements throughout Canada. Please let us know if you require an accommodation due to a disability during any aspect of the recruitment process and we will work with you to address your needs.

Additional Information:

All candidates considered for hire must successfully pass a criminal background check, credit check, and validation of their work experience to qualify for hire. We thank all interested applicants, however we will only be contacting those for interview who possess the skills and qualifications outlined above.

Why should you work for goeasy?

To learn more about our great company please click the links below:

PAID1234	Mississauga, ON	e8f01b72a171ac29	Data Engineer
2020081684	Sr. Data Engineer	https://ca.indeed.com/rc/clk?jk=035a5f23df595f6f&fccid=2ca7392810684c2b&vjs=3	This position will join our growing team of data and analytics experts for Cox Automotive Canada. You will be responsible for expanding and optimizing our data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. You will support our data initiatives and will ensure optimal data delivery consistent throughout ongoing projects.

As a Senior Data Engineer, you will be responsible for leading Data Engineering projects throughout their entire lifecycle, that being: initial investigation and stakeholder engagement, solution design, application development, testing and sign-off, release and maintenance

The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. They must be self-directed and comfortable supporting the data needs of Cox Automotive Canada. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives, enabling the evolution of our Data Science and Business Intelligence

Responsibilities:
Assemble large, complex data sets that meet functional / non-functional business requirements.
Recommend different ways to constantly improve data reliability and quality
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Improve upon the data ingestion pipelines, ETL jobs, and alarms to maintain data integrity and data availability.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics
Performs other job-related duties as assigned or apparent and Stay up-to-date with advances in data persistence and big data technologies
Prototype and design new data integration solutions that balance security, scalability, fault-tolerance, performance as well as cost effectiveness
Define, document and champion best practice architectural patterns and work with wider technology development teams to proactively seek out new opportunities to utilise Data Solutions technology and data assets
Be responsible for leading the delivery of multiple Data Engineering development projects at a time. Project delivery responsibility extends from design, development, testing, release and maintenance
Be responsible for leading the delivery of multiple Data Engineering development projects at a time. Project delivery responsibility extends from design, development, testing, release and maintenance
Qualifications

Qualifications

3+ years’ technical experience working in Big Data, designing solutions to complex data ingestion problems
Graduate degree in Computer Science, Computer Engineering or a related field
Big Data Certification and AWS certification would be an asset
Must have strong Data Orchestration experience using tools such has AWS Step Functions, Lambda, AWS Data Pipeline, Apache Nifi.
Advance Python Skills to develop efficient, decouple Data pipeline
In-depth knowledge of AWS Big Data Services
Must possess in-depth knowledge and hands on development experience in building Distributed Big Data Solutions including ingestion, caching, processing, consumption, logging & monitoring)
Must have a strong understanding and experience with Cloud Storage infrastructure and operationalizing AWS based storage services & solutions prefer S3 or related specific business questions and identify opportunities for improvement
Experience in building processes supporting data transformation, data structures, metadata, dependency and workload management
A successful history of manipulating, processing and extracting value from large disconnected datasets
Working knowledge of message queuing, stream processing, and highly scalable data stores
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases
Experience supporting and working with cross-functional teams in a dynamic environment
Experience deploying data pipelines with CI/CD
Experience with Snowflake Data Warehouse is plus
Organization: Cox Automotive
Primary Location: Canada-Ontario-Mississauga-2233 Argentia Rd
Employee Status: Regular
Job Level: Individual Contributor
Shift: Day Job -
Travel: Yes, 5 % of the Time
Schedule: Full-time
Unposting Date: Ongoing	Cox Automotive	035a5f23df595f6f	Mississauga, ON	2020-08-16T23:47:42.000Z
2020081685	https://ca.indeed.com/rc/clk?jk=1f6d9f1ab7c60335&fccid=1c4aa3d5a92746d4&vjs=3	Mosaic North America	Senior Big Data Engineer	1f6d9f1ab7c60335	Mississauga, ON	The Senior Big Data Engineer DevOps role reports to the Director, Data Sciences DevOps. The role is part of the DevOps team in charge of the daily operations of various integration technologies. The role is responsible for supporting workloads running on the Hadoop environment and associated technologies. This position will be responsible for monitoring the batch jobs, resolving incidents, optimizing workloads, tuning jobs, and making job enhancements.

The role will focus on production support and will also take part in the DevOps rotation for making enhancements. Also, the role will be involved in R&D of emerging technologies with the application administrators and technical architects.

Strategic:
Evaluate tools and technologies in the context of the future state architecture, and evolving business requirements
Responsible for review of project artifacts during the transition phase and ensure operational needs are met
Research & development about new Hadoop & Analytical technologies
Review solution and technical designs
Propose best practices/standards
Benchmark the performance in line with the non-functional requirements
Previous experience in developing and deploying operational procedures, tuning guides and best practices documentation
Attention to detail to review project deliverables for completeness, quality, and compliance with established project standards

Candidate Profile:
3+ years developing and supporting applications leveraging the Hadoop stack
3+ of experience with data integration/ETL tools such as DataStage or alternatives
SDLC knowledge in both waterfall and agile methodologies
Hands-on experience with source code management system (SVN, Git) and continuous integration tools (Jenkins)
Experience on following tools: Hive, SQL, Spark, Kafka, Flume, Sqoop, HBase ,Pig, HDFS, R, NoSQL
Experience on handling data processing, delivering distributed and highly scalable application
Experience with HortonWorks Hadoop Distribution
Experience with large scale domain or enterprise solution analysis development, selection and implementation
Experience with high-volume, transaction processing software applications
Good understanding of workload management, schedulers, scalability and distributed platform architectures
Experience in software development and architecture experience using Java EE technologies (Application Server, Enterprise Service Bus, SOA, Messaging, Data Access Layers)
Experience in scripting languages & automation such as bash, PERL, and Python
Experience in data warehousing, analytics, and business intelligence/visualization/presentation
Experience using SQL against relational databases
Working knowledge of search technologies like Lucene, Solr
5+ years of hands-on experience on Linux, AIX, and z/OS
Professional Skill Requirements
Excellent communication skills (both written and oral) combined with strong interpersonal skills
Strong analytical skills and thought processes combined with the ability to be flexible and work analytically in a problem-solving environment
Attention to detail
Strong organizational & multi-tasking skills
DISCLAIMER:
Acosta/Mosaic North America is an Equal Opportunity Employer
The above statements are intended to describe the general nature and level of work being performed by people assigned to this classification. They are not intended to be construed as an exhaustive list of all responsibilities, duties, and skills required of personnel so classified. Mosaic reserves the right to modify all or part of any job descriptions at its discretion in order to meet and or exceed the needs of the business.
We are committed to providing accommodations for persons with disabilities. If you require accommodation, we will work with you to meet your needs, to the extent required by law.

Qualifications


Primary Location: CA-ON-Mississauga
Work Locations: Acosta-Mosaic Mississauga Corporate Office 2700 Matheson Blvd. E. W. Tower 2nd Floor Mississauga L4W 4V9
Job: Information Technology
Organization: CoE - Client Information Services - Canada
Shift: Standard
Job Type: Full-time
 Day Job
Job Posting: Aug 4, 2020, 10:54:13 AM	2020-08-16T23:47:42.000Z
2020081686	Toronto, ON	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DQQnUBuQBSuyaIQhpC59TW7hrTbBg8v-nGtzzV8dunbQHh9VTp-k89gJ8q3B9UEXLHcYbAG67d8jpOK4E3ok8olcOhfyyiChWMI3BPvUFdra3ljwgLS_vFsJXlW6jHt2NpQMzd2p9F6BXGm9ND3feUN42yGy4tVXBU7xPsBm2zUk7nGBqHByUhHIV2H7K_Lr9L0nXq85_uwFRMuFXOo4YoYKLdPK9ppz1IHUAK1PwmPAneq_DUFIPE90uSm09_fKIVrmLxHYYzLuW3Z4fqILge0xev5eK6k3TbsMzDj6G91BxA0uWwQbkwUAb_YdtyxKQ-ctlh9N97m4nQup91JHQjxdV7uqhUtdOBfxaTv9EWHVkUbznbM54VWJIR6Eoxl9T-g-Maee9sUvdO-yovUjwekHw3aro4INJJzdivjulTttzpw_hLfOT0INdDo-VaWePPoMxtO_zKW11opggWuJFW5HyLX3yocwcp9hejXjhOjhoHIEqlej46-XYL8Z1BPXVLBmw1lfB19bztJPL041_fwk6pHo7XY3LjprolV6qPwMOwS1fDEcArFCI3Djkw6eeYbTBrbi-xJmJn8_3E3uOg52aX06npcTEK-YOLSH2LMy9BQUGrzTNZ40wrYea-0B0niyepyJ2JVow5PEB3LiGR1LrGHgDZAz23UINKXRlpKW3a408DeQtcX9jJda5lgSRpAqMEwzQ33UTkq6upr21pV551uuxBuH-cwwDrcIdkQeaPhYv55NBhbJpQhaZUELe044KiX2TBbnfIUcIEdhNBo2O-y_QDm9DyVZaQGsgF890JiJjtEpBJoLM4mEM0HxAAUZZP3sMKf1F2wXhM6SUMi3TJdFSXdrE=&p=10&fvj=0&vjs=3	TES - The Employment Solution	05f7bf2a7458c7fc	2020-08-16T23:47:43.000Z	Job Title- Data Engineer
Location- Downtown, Toronto
Type - Full Time Permanent
Salary - Negotiable + Benefits



Focus on data architecture, best practices, reliability, security, and complianceImprove and extend ETL, data processing, and analytics processesFacility with PowerBI, including creating dashboards and data sourcesDeveloping high complexity, fast performing SELECT queries.Developing T-SQL procedures, functions, triggers, jobs, scripts, etc.Development of Advanced T-SQL such as temporal tables, PIVOTs, recursive table expressions and more.Modeling and implementing Data Mart solution for Power BI analytics
Managing indexes, statistics, query plans alerts, database activity, and overall performance activity.In-depth experience working with relational databases, such as Microsoft SQL Server or PostgreSQLEnthusiasm for applying good data design, testing, documentation, and support practicesExperience building and optimizing data pipelines, architectures, and data setsKnowledge of message queueing, stream processing, and data stores/warehousesWorking knowledge of AWS products related to data engineeringBachelor's degree in Computer Science, Software Engineering or an equivalent
Excellent communication skills - both written and verbal; ability to speak in Spanish is a bonus

To apply please send an email to sheetalk@tes.net	Data Engineer
2020081687	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DbNJQGwjC-NEA80-qlVyEGmSv34Xx0eW4t4u9rXSv_7_tlvMU2e-Gw_wrZ47s1zPCnrySU2yuEL07wtSyx7XOwYh5VPcVL7CxRhOYwHzR-CdmyBrwbeqommOn6y9FD_uKIYpUCb9-PC_XgwGzBDrPy7SD7JQndrQc_GJzDDokXuPiGIHKK0Gqlhv-QDHDcUsX9CwIWiGr9U1sa3ad6WJuDcf_XHYDAEw-czNIhUKEXhzyWzM4FDb6ErfDWnTn-E14WDJN9YEDr3yegTjZyu98x9BwPJthFdRJZELbvyAIMIEb5f2eHJe0DHgd69oluMyGXB7R7VEmqXPkW0jIKewS4cYp7h-uKlquw_ZMnwnuj4REQOeQXczRIzAtsc8U90TezviCt7Dk-OeDFygOeuShK1p3kI36PbGmkAZle7ufTuIZFxuvmg0yii6GdmFYFMf_ZX9tIwKg08PFn0LAGpU0t55hima2qCpYcSpzlqcOCmQ==&p=11&fvj=0&vjs=3	goeasy	If you are looking to join one of Canada’s fastest growing companies, goeasy Ltd. is the place for you! Recognized as one of Canada’s Most Admired Corporate Cultures, one of Canada’s Top 50 Fintech’s and one of North America’s Most Engaged Workplaces, we want the best and brightest to join our team.

We are a publicly traded company on the TSX with over 4000% shareholder return since 2001, goeasy operates two main business units. easyfinancial is our consumer lending business that offers secured and unsecured installment loans of up to $35,000 and easyhome is Canada’s largest merchandise lease-to-own company. It is our mission to provide everyday Canadians the chance for a better tomorrow, today by giving them access to the credit they need and by offering them a second chance when they have been turned down by banks and traditional lenders. With a retail network of nearly 400 locations across Canada and over 1900 employees, we are able to build lasting relationships with our customers as we help them rebuild their credit and graduate towards prime rates and a brighter financial future.

The Data Engineer on the Data Science and Business Insights team will build, integrate data from various resources, manage data in goeasy operational data store and enterprise data warehouse. This position will develop ETL (Extract, Transform and Load) with various tools on large datasets to ensure data is easily accessible, works smoothly, as well as maintain and expand the data warehouse for reporting and analysis. The Data Engineer will also work closely with the data architect on the design and architecture of our enterprise data warehouse.

Responsibilities:

Develop data set processes for data modeling, mining and production
Develop and maintain ETL processes using SSIS, Scripting and data replication technologies
Participate in development of datamarts for reports and data visualization solutions
Research opportunities for data acquisition and new uses for existing data
Integrate new data management technologies and software engineering tools into existing structures
Support the translation of business requirements for data acquisition/manipulation and provide detailed specifications that can be passed downstream for use
Develop detailed technical specifications and operational support documentation in collaboration with Business Systems Analysts, BI Engineers and Architects.
Identify and communicate technical problems, process and solutions
Create Ad-Hoc queries and reports as needed along with providing on-going analytical support for these requests
Assist in the collection and documentation of user’s requirements
Ensure that existing business processes dependent on the ODS/EDW are monitored and respond quickly to bug fixes, enhancement requests and production ETL related issues.
Dealing with the database users on a daily basis to ensure that problems are dealt with promptly and that appropriate fixes are made to resolve any problems.
Recommend ways to improve data reliability, efficiency and quality
Ensure systems meet business requirements and industry practices
Work effectively with the Business Intelligence and Data Solutions Architects, Data and BI Engineers to ensure that all approved development and deployment procedures are followed.

Qualifications:

Bachelor’s Degree in Computer Science, MIS, Computer Engineering or other Information Technology related degree
4+ years working with SQL Server or comparable relational database system
3+ years of extensive ETL development experience with SSIS and/or ADF
4+ years of experience troubleshooting within a Data Warehouse environment
Expert domain knowledge & experience in Data warehousing, encompassing data model design, dimensional modeling, naming conventions, cross-cutting concerns, common integration technologies, patterns & standards and emerging technologies.
Expert Knowledge of SQL skills to build, debug, and optimize (developing procedures, functions, SQL queries, etc.) and working with large data sets and to perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
2+ years SQL Server Database administration experience
Cloud experience (Azure) is highly preferred
Exposure and experience with Python, R, Hadoop, Azure, other Big data and advanced analytics
Knowledge of AI and ML developments/solutions/implementations
Experience with multiple programming languages (PowerShell scripting, C#, others) with basic scripting skills.
High level of technical aptitude

Inclusion and Equal Opportunity Employment

goeasy is an equal opportunity employer. In addition, goeasy is committed to providing accommodations for applicants upon request at any stage of the recruitment process in accordance with all legislative requirements throughout Canada. Please let us know if you require an accommodation due to a disability during any aspect of the recruitment process and we will work with you to address your needs.

Additional Information:

All candidates considered for hire must successfully pass a criminal background check, credit check, and validation of their work experience to qualify for hire. We thank all interested applicants, however we will only be contacting those for interview who possess the skills and qualifications outlined above.

Why should you work for goeasy?

To learn more about our great company please click the links below:

PAID1234	Mississauga, ON	e8f01b72a171ac29	2020-08-16T23:47:43.000Z	Data Engineer
2020081688	Toronto, ON	Our client, a boutique-size consulting firm, prides itself on being at the forefront of innovation in the Big Data space. Founded in 2010, they provide thought leadership and implementation excellence within the ever-growing data and analytics world.
They take pride in having some of the most highly trained and experienced consultants in the industry which translates into optimal value for their clients. They were one of the first companies to provide analytics and data as a service, via the cloud, as early as 2010. They strive to make sure their customers are well-positioned with the best technologies/tools in the industry, constantly evaluating new and existing technology partnerships. Some of the more prominent companies they have partnered with include; Snowflake, DataRobot, MicroStrategy, Informatica, Amazon AWS & Microsoft. They continue to invest in their most valuable resource, their people. They do this through extensive training both on the job and through various educational programs.
Due to growth, they are seeking to hire an Intermediate - Senior Integration Consultant.

Desired Skills and Experience
: University/College degree in Computer Science, Mathematics, Data Science and/or Relevant Degree
: 5+ years hands-on development, configuration, scripting and administration experience with Data Integration platforms. (i.e. Informatica, Talend, DataStage, SSIS)
: BI Experience (MicroStrategy, Looker, Tableau, PowerBI) considered a nice to have
: Extensive theoretical and practical knowledge of data warehousing principles/concepts and practical development experience in all areas of the data warehousing life cycle
: Experience with Data Management, ETL, Cloud Data (AWS), Data Integration
: Knowledge of OLAP-related principles and concepts
: Strong grasp of data modeling techniques and concepts (Normalized/Denormalized, Conceptual/Logical/Physical, Star, Snowflake, Data Vault)
: Strong knowledge and experience with relational databases such as Snowflake, SQL Server, Oracle (Advanced knowledge of reading and writing SQL, Performance analysis and tuning)
: Knowledge and experience with key Big Data technologies (Hive, Presto, Spark, Kafka, NoSQL databases, Semi-structured data access patterns (Json, Parquet, XML, etc.))
: Strong Python scripting skills
: Excellent communication skills
: Great problem-solving skills
: Leadership and good client management skills

Day to Day Activities Would Include
: Conduct relevant customer interviews to determine key business requirements and objectives
: Build appropriate analytical data models based on outcomes of user interviews
: Analyze and profile data systems to build source to target data mappings
: Review ETL performance and conducts performance tuning as required on mappings/workflows or SQL
: Administration and support of data integration infrastructure
: 2nd level on-call support of ETL services as required

You will be responsible for attaining the following goals:
: Attaining a minimum of 1 new accreditation/certification per year
: Spending 80% or more of their time on billable work
: Completing 90% or more of their agile delivery tasks on time
: Demonstrating competency in 1 new relevant technology every year	Copperstone Connect	54e0fcc38385e3a2	Data Engineer/Integration Consultant	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Ceb7pqR8iGmLuXIZQklF6pqed3xddONNM6Jjumj-ndyGK0kULQUmQ2REqfJ5KAzFVeK_es1WVg0s1sLgKW09UsvVRp0-dSizFQr1oNyK_VIqdjla00VU9GNVvu2x4v9p3hsL7NeF-6hBksbAaSgKazsaYTODGMKQQeVuYcxoOuxvFKuD0HRLD9ybgbcXTn1LPsePA0AwSp19MH8Y2jsDIRye-eQ-2DqeMcBGDwIuUkKXx0wUloJjsn55xeMQ4pgWQKu7ddNk4PMxaPe_9n1rufThVD31e36PwkcoJA0jHn8peEMO12_-e3JuAjDodWXEi8PXmp8aKjsdN_RBvjEO5CDax6B2uFbSp7qUThNJOYHBlkUaFNmkfJVHORveLF-Fsgs9MS8bAA71DqkxLEYO2WfGsdy_z68CHH62UH6DBVG9EknA8tv9CuakbJ0Aib4_ScVeBMq3E7NayZ_4oPoM1w43eGokSue62syNyLXyJvPfEtyIKCs722UCkO9LST2ESJYM021K6lLao45FFZdSiPRgRb8iHULkFC_6QEopgtn5Sf2aktEP2AfwHV07s5_PgmlERuhYTUBZ21aUWDjOgN14GNnCjYbFKuzeqQh3IDy2r2XnxSwbWoyDEp9CEkI4ro_pRKi-gsNvmXLEgPklineaPvoHewP9hVhncTyDKeLA==&p=12&fvj=0&vjs=3	2020-08-16T23:47:45.000Z
2020081689	Toronto, ON	The growing team here at Myticas is looking for an experienced Data Engineer who would be interested in a remote contract opportunity within the Ottawa, Toronto and Montreal regions.

Accountabilities:

Defining and reviewing security design requirements for cloud infrastructure and application components.
Evaluating architecture patterns from security perspective.
Building and implementing security controls to enable enforcement of compliance with Cloud Control Objectives, using custom Azure policies and integrated controls in DevOps processes

Requirements:

Strong Data Engineer w/ DevOps expertise + Azure Cloud Experience
Must know how to code and stand up scripts.
Experience with Data Digestions
Experience writing scripts to automate (infrastructure)
ARM Templating Expertise
Azure Synapse Expertise
Support developing automated DevOps processes and procedures for the following Azure components:
Azure Synapse (Azure DW) & Studio (private preview)
Azure Data Catalog Gen 2 (Babylon – private preview)
Azure Data Lake Storage Gen 2
Azure ML
ML Flow
Azure SQL Analysis Service
Azure Databricks
ADF data pipelines for data loading to AzSQL/Synapse
ADF data pipelines for connecting to on-prem data sources for data

Candidates looking to apply for this role are to send us an updated version of their resume in confidence. Our team will be sure to review all applicants and follow up accordingly at the conclusion of the review process.

Job is also known as: Data Engineer, DevOps Engineer, Azure Cloud Engineer, Azure Engineer, Cloud Engineer

INDMY	BHJOB15656_15145 - Data Engineer w/ DevOps - Azure Cloud	a32b7c599156a218	Myticas Consulting	2020-08-16T23:47:45.000Z	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Bt274jo0r_e0cCCqhskoQvohXG3bfDAo8rIdE-Msu3yuVfyF2oiDFKHFOU-vxVulAfe8kPL7riFwoXa1bsfN-szjxO_8qUQWbPsJOnTg0EkutIoK2S4NNLL24JQh9e4DsE9hv907k2_zDbvj0a8WAj5oPSV5J0YhgrbSF1z_nzlDN3sXLN8IrYjrR6OPNREQPiGSTOw2PyJovpYezaF4y1jC3c73R6lqnBTlZ7bmtqEJTTGrMK4IZEV3lDXeWZbJkrFHp_l4RMqM0zPOhwV44ZACy1oD4xg5H0_D9Od9J2ezQkFZ8tGRfFHX2_97_Rgc0M1sR_3swEuVE3vBwsmOZNE7lQPPjjDxofeN7IFP-NgBxMlIt8daG2nb3Ynfxge7HV6fBAo7wrmwGls_PEppfa4HX9PSqje2oN30a-3m76aDt6OoRb139WxgZl9c9UHI4TZmT3jmEavA==&p=13&fvj=0&vjs=3
2020081690	Data Engineer (Intermediate 4-6 years) - Contract - Remote during COVID	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Dkh866153otbRJ8nVMuEXnXPd315JS377__3VyD6HnNQkmcaOHMemmy5pLrNV6j0WaVWEWWabnp4PP6-2u77DLfT_FCIGvCW8CQmQW140Z7EVU32Jnwl42FALsxnbc4n5M6FOTfoW2OoTMOcGG21HZGOoXL7fWT8eaT67VlcClh4sHQcJsHCX_rbwPT-QilxMtIVdb_glL5L859lgNJc_dc--t6nZx5hqUCmLEVCmxukL8eTkQr4JS-SeBcHxZ2C42f04ptV0NyBs4jjhWSmWWx-OAEQhJKrr8OSFQDXKvVVBibtSc1fCXt2hgekKG7iBXwUVX_ODmKU8HpVDHtyy4N7KS9Py0H01oA2EO1DCW9f4QI2Lc3nTsFQ1DYux_DPZ710o2_QWPUL3hsLSgPMXG3u4bO9w4YvjgqOm5tFgyIvBVyJBUpC2cp9yNUMr_ciacaU1evGcG_86Jj8uCnb6bCORWJjFHoOa8ugCVCO-FcqTVMz7crIAb&p=14&fvj=1&vjs=3	Role: Data Engineer (Intermediate 4-6 years)Structure: Contract (6 months initially)Location: North York, ONHours: Monday - Friday 40 hours (remote during COVID-19)Pay: $50.00 p/h inc.The role: We are looking for Intermediate Data Engineers who have roughly between 4-6 years experience who ideally has experience moving data from Google cloud over to Big query.However, if you are someone who has good experience with ETL and Data Pipeline Design & Data Validation experience and can script in Python and create strong SQL Queries then this will also be a potential match for this role.Ideal Qualifications to be successful in this role: - Degree in Computer Science, Engineering, Information Systems (or equivalent combination of skill and experience)- 4-6 years of experience working with data architecture projects- Excellent SQL coding and experience with a broad array of development tools and platforms including exposure to a big data environment tools/languages, such as SQL, R, SAS, Python, etc.- Experience creating relational database design and data models- Experience with various analytical data platforms and technologies- Experience with using REST API, Cloud (GCP Ideally)- Experience in custom or structured data integration design, implementation, and maintenance- Experience with business intelligence tools such as Qlikview, Tableau and Microstrategy (Nice to have for Data Visualizations)- Previous experience working in retail or eCommerce is highly preferred- Ability to curate data to tell stories and provide business insightsPlease apply with an updated resume and ensure the required skills you are able to speak to for this position are included.For more roles like this please go to www.corgta.com/find-a-job/Job Type: Full-timeSalary: $45.00-$50.00 per hour	CorGTA	2020-08-16T23:47:46.000Z	North York, ON	997673b8effa647f
2020081691	Job Field:
Information Technology
Job Type:
Full-time
Building Location:
Length of Assignment:

SUMMARY

The Senior Data Engineer is a deep technical expert in building complex data warehousing and business intelligence applications. At this level, the incumbent demonstrates a passion and in-depth knowledge of large, complex application development methodologies. They motivate themselves and the team to refine their skills and adopt best practices for developing pragmatic software solutions for the organization. Leading the charge, they continue to raise the bar on mastery of business intelligence application development within the team and the organization.

KEY DUTIES & RESPONSIBILITIES

Programming

Uses in-depth knowledge of advanced programming techniques, design patterns and hardware/software interfaces to develop business intelligence and data warehouse applications.
Designs, tests and integrates data warehouse and BI modules and resolves programming errors using various debugging tools and techniques.
Provides guidance/mentors on programming practices and techniques to individuals and cross-functional teams.
Provides support, guidance and production assurance for very complex or urgent problems.
Performs work with minimum supervision, and work is assigned in terms of technical objectives.
Prepares technical documentation (e.g., user guides, technical specifications).
Assists in the design of business solutions.

Analysis

Conducts impact analysis for proposed changes to or problems across the system.
Leads team discussions in the analysis and collaboration to clarify and improve specifications or to identify alternative programming solutions.

Continuous Improvement

Makes recommendations or decisions on architecture, application design, standards and process improvements.
Enforces team and organizational standards and practices (e.g. at walkthroughs and peer code reviews).
Engages in continuous learning by developing and executing on a learning plan.
Takes responsibility for individual and the team's results.
Advocates for quality in all aspects of development efforts based on the team's definition of quality.

Risk Management

Estimates and prioritizes work to maximize value while taking into account risk, effort and dependencies.
Raises impediments, risks, and issues as early as possible and work with the team to mitigate as needed.

KNOWLEDGE & SKILLS

University graduation and minimum 5-10 years of relevant experience
Demonstrates in-depth knowledge of Microsoft BI architecture, established data warehouse development methodologies, multi-dimensional data modelling, OLAP, metadata management, data security, predictive analysis and big data processing. Extensive experience in one of these cloud data warehouses (Snowflake, bigtable, Redshift), Data Vault 2.0 methodology, steaming data processing, BI components in SQL Server 2016+, TSQL, and DAX, Power BI.
A good working knowledge of application security, C#, python, PowerShell, metadata management, NoSQL, and data security.
Experience in programming and debugging complex data warehouse and BI applications as part of a multi-disciplinary team environment (following an agile framework such as SCRUM preferred) based on Microsoft Team Foundation Server and git.
Experience in writing unit tests to support production code using a unit test framework.
Experience with database management (i.e. database design, schema creation, concurrency and performance considerations).
Takes ownership and initiative and collaborates well with a team of peers.
Demonstrates a commitment to continuous learning (e.g. user groups, blogs, conferences, community awareness, and next generation tooling).
Able to clearly communicate in both a verbal and written form within a predominantly English working environment.
Has a positive, passionate, idea generating attitude when faced with challenges.

Licenses and/or Professional Accreditation

Certification in Microsoft technologies preferred	bcb75a8b3d33578b	Markham, ON	Senior Data Engineer	https://ca.indeed.com/rc/clk?jk=bcb75a8b3d33578b&fccid=84c23fbcabc14d59&vjs=3	2020-08-16T23:47:47.000Z	BGIS
2020081692	Toronto, ON	2020-08-16T23:47:48.000Z	Senior Data Engineer	https://ca.indeed.com/rc/clk?jk=256688ca085e919f&fccid=5d784228b1eee537&vjs=3	256688ca085e919f	Myant	About us:
At Myant, we are creating the world’s first textile computing platform, integrating technology directly into the only thing we’ve been wearing our entire life – clothing. SKIIN is our first consumer facing brand, and SKIIN’s vision is to enhance human ability through connected clothing - think Ironman’s suit, but comfortable. The sensors and actuators embedded within our apparel create your Digital Identity, which will be consumed by those who matter to you - your family members, doctors, coaches, other IoT devices - without you consciously having to think about it. Imagine a world where you walk into your house and the temperature automatically adjusts to your optimal body temperature, the lights adjust to your mood, you can monitor and adjust your everyday lifestyle based on your vital signs, or your doctor is aware of the onset of a disease before you even visit. The line between the digital and physical world is becoming increasingly blurry, and we believe textile is the next medium to bridge that gap.
We’re looking for people who believe in our mission to make wearable technology truly ubiquitous and convenient, so that everyone can benefit from it. We are a cross-functional team solving big challenges at the intersection of fashion, electronics, software, and data science.
Responsibilities:
Test the performance of the algorithms developed by Data Science team
Leverage native APIs for integration of AWS platforms
Take ownership of all your deliverables and communicate your results to timely project delivery
Prepare reports and some technical documentations

Qualifications Required:
Bachelor’s Degree in Computer Science, Computer Engineering, or equivalent work experience
Proficiency with JavaScript and Python language
Basic knowledge of machine learning algorithm and libraries like keras, tensorflow, sklearn
Experience in building RESTful APIs following Micro-Services Architecture
Experienced in NodeJS, PostgreSQL, and GraphQL.
Significant experience in building microservices leveraging various AWS features (AWS Lambda, IAM, SQS, DynamoDB, Kinesis, Redshift, Aurora, EC2, S3, API Gateway etc.)
Solid experience in Biomedical signal processing, and data mining related to physiological patient data is a bonus
Powered by JazzHR
8SQJftU9nl
2020081693	Toronto, ON	0a0e41fb84eb4708	https://ca.indeed.com/rc/clk?jk=0a0e41fb84eb4708&fccid=e1b2607798446d2b&vjs=3	Senior/Lead Big Data Engineer	Punchh	2020-08-16T23:47:49.000Z	Headquartered in the Bay Area with offices in Austin, TX, Toronto, Canada and Jaipur, India, venture-funded Punchh is the world leader in innovative digital marketing products for brick and mortar retailers, combining AI technologies, mobile-first expertise, and Omni-Channel communications designed to dramatically increase customer lifetime value. Leading global chains in the restaurant, health and beauty sectors rely on Punchh to grow revenue by building customer relationships at every stage to becoming brand loyalists, including more than 100 different chains representing more than $12B in annual spend, 30,000 locations globally, 26M+ consumers, and 1M+ transactions daily. Punchh boasts a customer list that includes Pizza Hut, Quiznos, Coffee Bean & Tea Leaf and many more.

Title

Senior/Lead Big Data Engineer

Location

Toronto, Ontario, Canada

Reporting to

Sr. Dir. of Data

About the role

Punchh is looking for a Senior/Lead Big Data Engineer that will play a critical role in leading Punchh's data innovations. He/she will help create cutting-edge Big Data solutions by leveraging his/her prior industrial experience. This role requires close collaboration with the Machine Learning, Software Engineering, and Product Departments. You will be given the opportunity to not only serve internal teams, but also our business clients as well.

What You'll Do

Punchh is seeking to hire Big Data Engineer at either a senior or tech lead level. Reporting to the Director of Big Data, he/she will play a critical role in leading Punchh's big data innovations. By leveraging prior industrial experience in big data, he/she will help create cutting-edge data and analytics products for Punchh's business partners.

This role requires close collaborations with data, engineering, and product organizations. His/her job functions include

Work with large data sets and implement sophisticated data pipelines with both structured and structured data.
Collaborate with stakeholders to design scalable solutions.
Manage and optimize our internal data pipeline that supports marketing, customer success and data science to name a few.
A technical leader of Punchh's big data platform that supports AI and BI products.
Work with infra and operations team to monitor and optimize existing infrastructure
Occasional business travels are required.

What You'll Need

5+ years of experience as a Big Data engineering professional, developing scalable big data solutions.
Advanced degree in computer science, engineering or other related fields.
Demonstrated strength in data modelling, data warehousing and SQL.
Extensive knowledge with cloud technologies, e.g. AWS and Azure.
Excellent software engineering background. High familiarity with software development life cycle. Familiarity with GitHub/Airflow.
Advanced knowledge of big data technologies, such as programming language (Python, Java), relational (Postgres, mysql), NoSQL (Mongodb), Hadoop (EMR) and streaming (Kafka, Spark).
Strong problem solving skills with demonstrated rigor in building and maintaining a complex data pipeline.
Exceptional communication skills and ability to articulate a complex concept with thoughtful, actionable recommendations.

Benefits

Healthcare coverage
Life and AD&D insurance
Competitive salaries, bonus and stock options
Professional development
Paid Time off
Paid holidays
Free lunch in the office.

Punchh is proud to provide equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. If you'd like more information about your EEO rights as an applicant, please click here.

We also provide reasonable accommodations to individuals with disabilities in accordance with applicable laws.
Notice to recruiters and placement agencies: If you are a recruiter or placement agency, please do not submit résumés to any person or email address at Punchh prior to having a signed agreement with Human Resources. Punchh is not liable for and will not pay placement fees for candidates submitted by any agency other than its approved recruitment partners. Also, any résumés sent to us without an agreement in place will be considered your company's gift to Punchh and may be forwarded to our Talent Acquisition team.
2020081694	Toronto, ON	Lead Data Engineer	2020-08-16T23:47:49.000Z	4ecf6d90140603ee	Manulife	Are you looking for unlimited opportunities to develop and succeed? With work that challenges and makes a difference, within a flexible and supportive environment, we can help our customers achieve their dreams and aspirations.
Job Description
Are you a go-getter who has a passion in building next gen data pipelines and provide Big data solution for business problems? Are you a big fan of simplification and automation?
Manulife is seeking an awesome Lead Data Engineer , with Big Data experience as well as strong understanding of data-ingestion, data curation and both Batch & Stream Data processing, to join our rapidly expanding IT Organization and assist us as we work to be a digital leader in our industries!
Skills and Experience
You will have the following skills and experience:
Lead development teams to define and build data pipelines
Expert in building and operationalizing BigData platforms in cloud using one of the public clouds, preferably MS Azure.
Hands on experience with Big Data streaming frameworks and tools (Spark Streaming, Storm, Kafka, etc.)
Expert in Hadoop ecosystem and toolset – Sqoop, Nifi, Pig, Spark, HDFS, Hive, HBase, etc.
Expert in automating data pipelines in a Big Data ecosystem, DevOps and CICD.
Experience in developing Hadoop integrations (batch or streaming) for data ingestion, data mapping and data processing capabilities
Experience programming in both compiled languages (Java, Scala) and scripting languages (Python or R)
Expert in developing Big Data set processes for data modeling, mining and production
Experience in working with key partners including business and technology to establish definition of success, goals, key use cases and aligning dev team on strategic priorities.
Excellent communication and interpersonal skills
Excellent analytics, problem solving and solutioning skills
A capacity for constant learning from both success and failure, remaining open to change and continuous improvement
Good to Haves
Experience in Exploratory data analysis; Query and process Big Data, provide reports, summarize and visualize the data
Experience in Canary deployments, 0-downtime, 0-dataloss, hot-hot DR
Experience in designing solutions for Big Data warehouses
Experience with Hadoop security frameworks like Knox, Ranger.
Experience with Hadoop metadata frameworks and security policies such as Ranger, Atlas
Experience in data profiling and analysis
Exposure to and an understanding of Agile scrum methodologies and experience of working in an Agile team
Experience in Big Data performance analysis, tuning and capacity planning
Experience in designing business intelligence systems, dashboard reporting, and analytical reporting is a plus
Experience with the Hortonworks Data Platform (version 2.5)
Experience in using Git flow.
Basic understanding of following will be useful but not required:
Exposure to and basic understanding of collaboration tools like Slack, Skype, Teams, and JIRA
What about Perks?
Manulife has lots of perks including, but not limited to:
Competitive compensation
Retirement Savings Accounts including a RPP (Pension Plan), RRSP (Retirement Savings Plan), and TFSA (Tax Free Savings account)
Manulife Share Ownership Program with employer matching
Customizable Benefits Package including Health, Dental, Vision, and 100% of Mental Health expenses
Financial support for ongoing training, learning, and education
Monthly Innovation Days (Hackathons)
Wearing jeans to work every day
An abundance of career paths and opportunities to advance
A flexible work environment with flex hours, work from home arrangements, distributed teams, and condensed work week arrangements.
This is a full time permanent role and the team is located in Kitchener/Waterloo, Ontario. There is opportunity for Toronto based people to work in this role, however there would be travel to Kitchener / Waterloo twice per week.
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
About Manulife
Manulife Financial Corporation is a leading international financial services group that helps people achieve their dreams and aspirations by putting customers' needs first and providing the right advice and solutions. We operate primarily as John Hancock in the United States and Manulife elsewhere. We provide financial advice, insurance, as well as wealth and asset management solutions for individuals, groups and institutions. At the end of 2017, we had approximately 34,000 employees, 73,000 agents, and thousands of distribution partners, serving more than 26 million customers. As of December 31, 2017, we had over $1.04 trillion (US$829.4 billion) in assets under management and administration, and in the previous 12 months we made $26.7 billion in payments to our customers. Our principal operations are in Asia, Canada and the United States where we have served customers for more than 100 years. With our global headquarters in Toronto, Canada, we trade as 'MFC' on the Toronto, New York, and the Philippine stock exchanges and under '945' in Hong Kong

If you are ready to unleash your potential it’s time to start your career with Manulife/John Hancock.
About Manulife
Manulife Financial Corporation is a leading international financial services group that helps people make their decisions easier and lives better. We operate primarily as John Hancock in the United States and Manulife elsewhere. We provide financial advice, insurance, as well as wealth and asset management solutions for individuals, groups and institutions. At the end of 2018, we had more than 34,000 employees, over 82,000 agents, and thousands of distribution partners, serving almost 28 million customers. As of March 31, 2019, we had over $1.1 trillion (US$849 billion) in assets under management and administration, and in the previous 12 months we made $29.4 billion in payments to our customers.
Our principal operations in Asia, Canada and the United States are where we have served customers for more than 100 years. With our global headquarters in Toronto, Canada, we trade as 'MFC' on the Toronto, New York, and the Philippine stock exchanges and under '945' in Hong Kong.

Manulife is an equal opportunity employer. We strive to attract, develop and retain a workforce that is as diverse as the customers we serve and to foster an inclusive work environment that embraces the strength of cultures and individuals. We are committed to fair recruitment, retention and advancement and we administer all of our practices and programs based on qualification and performance and without discrimination on any protected ground.
It is our priority to remove barriers to provide equal access to employment. A Human Resources representative will consult with applicants contacted to participate at any stage of the recruitment process who request any accommodation. Information received regarding the accommodation needs of applicants will be addressed confidentially.	https://ca.indeed.com/rc/clk?jk=4ecf6d90140603ee&fccid=1747adf6142beb48&vjs=3
2020081695	Toronto, ON	https://ca.indeed.com/rc/clk?jk=5c06518576e750fc&fccid=b704562e07a2a03f&vjs=3	5c06518576e750fc	SADA	2020-08-16T23:47:50.000Z	Senior Data Engineer	Join SADA as a Sr. Data Engineer!


Your Mission

As a Sr. Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data problems facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring batch or streaming data pipelines, data lakes and data warehouses.

You will be expected to run point on whole projects, end-to-end, and to mentor less experienced Data Engineers. You will be recognized as an expert within the team and will build a reputation with Google and our customers. You will demonstrate repeated delivery of project architectures and critical components that other engineers demur to you for lack of expertise. You will also participate in early-stage opportunity qualification calls, as well as lead client-facing technical discussions for established projects.

Pathway to Success

#BeOneStepAhead: At SADA we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our engineers know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs.

Your success starts by positively impacting the direction of a fast-growing practice with vision and passion. You will be measured quarterly by the breadth, magnitude, and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, how well you collaborate with your peers, and the consultative polish you bring to customer interactions.

As you continue to execute successfully, we will build a customized development plan together that leads you through the engineering or management growth tracks.

Expectations

Required Travel - 30% travel to customer sites, conferences, and other related events. Due to the COVID-19 pandemic, travel has been temporarily restricted.
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:
Google Professional Data Engineer Certified or able to complete within the first 45 days of employment

Required Qualifications:
Mastery in at least one of the following domain areas:
Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or customer-facing role

Useful Qualifications:
Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Experience operationalizing machine learning models on large datasets
Demonstrated leadership and self-direction - a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem

About SADA

Values: We built our core values on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA's values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

Make them rave
Be data driven
Be one step ahead
Be a change agent
Do the right thing

Work with the best: SADA has been the largest partner in North America for GCP since 2016 and has been named the 2019 and 2018 Google Cloud Global Partner of the Year. SADA has also been awarded Best Place to Work by Inc. as well as LA Business Journal!

Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match, professional development reimbursement program as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.
2020081696	Toronto, ON	Data Developer	https://ca.indeed.com/rc/clk?jk=2ef0d527757decfa&fccid=c0b5558e336243b3&vjs=3	COMPANY OVERVIEW:
Success stories like this, don’t happen every day. From humble beginnings as a courier industry solutions provider in Canada, Fleet Complete quickly grew to be one of the world’s leaders in telematics and connected mobility solutions for a wide variety of industries with fleets, assets and mobile workers.

Today, with 20 years in the industry, Fleet Complete is one of the fastest-growing IoT (Internet of Things) companies across the globe, operating in 17 countries with offices in Canada, Netherlands, Denmark, Belgium, Estonia, Latvia, Lithuania and Australia. Fleet Complete continues to win employer, innovation thanks to our relentless customer-centric approach and commitment to company values of Innovation, Quality, Customers, Productivity, People and Community.

Thanks to strong partnerships and sound investments, our trusted Fleet and Mobile workforce platform provides real-time insights, visibility, employee safety and overall operational efficiency. This helps organizations, municipalities and businesses of all sizes to modernize their operations with ease. Fleet Complete is known for hiring, growing and empowering talented people who develop innovative products, build powerful relationships and provide personalized support that is unparalleled in our industry. Join Fleet Complete on our next chapter and we can work together to "help fleets thrive".

Proud to be named one of Greater Toronto’s Top Employers for 2020: http://content.eluta.ca/top-employer-fleet-complete

ESSENTIAL DUTIES & RESPONSIBILITIES:

Create and maintain optimal data pipeline architecture for legacy and the new architecture of IoT streaming data (Telematics and other automotive sensors)
Assemble large, complex data sets that meet functional / non-functional business requirements for data and application products
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics
Work with the Product team and other stakeholders to assist with data-related technical issues and support their data infrastructure needs
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader

QUALIFICATIONS:
All applicants must possess the following:

5+ years of experience in a Data Engineer role
A degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field (Graduate degree would be a plus)
Experience with cloud technologies. Specifically, AWS technologies such as S3, Glacier, Lambda, Athena, Redshift
Experience with object-oriented & functional scripting languages including Python and Java
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Strong analytic skills related to working with unstructured datasets
Build processes supporting data transformation, data structures, metadata, dependency and workload management
A successful history of manipulating, processing and extracting value from large disconnected datasets
Strong project management and organizational skills
Experience supporting and working with cross-functional teams in a dynamic environment
Understands and helps drive business impact via data systems and their resulting output

Fleet Complete will provide support in its recruitment processes to applicants with disabilities, including accommodation that takes into account an applicant's accessibility needs. If you require accommodation during the interview process, please contact the Recruitment Team, 866-649-7949.

Fleet Complete is an equal opportunity employer committed to diversity and inclusion. We are pleased to consider all qualified applicants for employment without regard to race, color, religion, sex, national origin, age, disability, protected veterans’ status or any other legally protected factors.	2020-08-16T23:47:51.000Z	2ef0d527757decfa	Fleet Complete
2020081697	Expert BI/ETL Engineer (Tech Lead Cloud Data Engineer)	Finastra	https://ca.indeed.com/rc/clk?jk=1aca9ca80ec23fec&fccid=1f9d0530a51ff611&vjs=3	Mississauga, ON	What will you contribute?
Reporting to the Senior Manager, Development, the role of the Expert BI Developer is to ensure the effective design and delivery of the Student Lending reporting solution. This hands-on role serves as a Technical Lead for the Business Analytics and Reporting team providing development, technical guidance, review and support.
Responsibilities & Deliverables:

Your deliverables as an Expert BI Developer will include, but are not limited to, the following:
Develop and deliver a robust Reporting and Business Analytics framework, well aligned with the company’s long-term strategic goals for data architecture vision.
Ensure the solution supports Student Lending client data needs and can be easily extended to newly acquired clients and their standards.
Liaise with vendors and service providers to select the products or services that best meet company cost and performance goals related to data architecture and analytics
Working closely with both enterprise level and project level team - data owners, stewards, users, business analysts, developers, quality analysts, department managers, architects and other stakeholders to understand reporting requirements and ensure strategic goals and tactical implementation are in alignment.
Translate project requirements into functional and non-functional specifications for BI reports and applications.
Lead conceptual and physical design, development and implementation of enterprise level BI and ETL framework, conforming to well defined business, technical rules and SLAs, preserving reusability of artefacts, single version of truth, centralization of logic, testability and well-designed error handling.
Prepare all necessary documentation that clearly describes solution and Meta data.
Monitor, tune up and administer BI Environments for quality and optimal performance purpose. Debug, monitor and troubleshoot BI solutions.
Be aware of and comply with all corporate and department policies, procedures and standards that apply to your work area.
Ensure that Reporting and Business Analytics strategies and architectures are in regulatory compliance .
Required Skills & Experience:
8+ years' of hands-on experience developing BI and Reporting Solutions.
Experience with business requirements analysis, entity relationship planning, data modeling, database design, reporting structures.
Direct experience in implementing enterprise data management processes, procedures, and support on data monitoring.
Understanding of large scale DB and reporting solution design, Source to Target Mappings, distributed DB design, multi environment structures, logical DB partitioning strategy, data archiving and retention, design and development of reporting semantic layer and view objects and logic
Expert knowledge of MS SQL
Expert hands-on experience with Azure Cloud Data Engineering suite: ADF, Databricks, Azure Data Lake, Spark, Azure SQL and Azure SQL Data Warehouse
Experience with DAX, Tabular and Power BI.
Experience with data processing flowcharting techniques.
Experience developing and maintaining ETL tools and platforms such as SSIS, Azure ADF
Experience with data architecting, large-scale data modeling, and business requirements gathering/analysis.
Strong understanding of BI Reporting & ETL technologies, relational and dimensional data structures, Big Data hands-on experience, principles, and best practices.
Strong familiarity with metadata management and associated processes.
Demonstrated expertise with repository creation, and data and information system life cycle methodologies.
Understanding of Web services (SOAP, XML, REST, JSON, UDDI).
Good knowledge of applicable data privacy practices and laws.
#LI-MG1
*************************************************************************************************************
The above statements describe the general nature and level of work being performed by people assigned to this job. They are not intended to be an exhaustive list of all responsibilities, duties, and skills required. Reasonable accommodations may be made to enable qualified individuals with disabilities to perform the essential job functions. If you need assistance or an accommodation due to disability please contact your recruitment partner.
*************************************************************************************************************	2020-08-16T23:47:51.000Z	1aca9ca80ec23fec
2020081698	Procom	Sr. Core Network Data Engineer
On behalf of our client in the telecom sector, Procom is looking for a Core Network Data Engineer.
Sr. Core Network Data Engineer - Job Description
Design, maintain, operate wireless packet core network with the “Best Network” objectives in mind.
Complete change and implementation activity on network elements, changing resource/connectivity allocations and verifying successful network integration of any changes.
Monitor health/status of HSPA (GGSN, SGSN, GPRS DNS), LTE (MME, PGW, SGW), VoWIFI (AAA, EPDG) network elements, react to alarms, and perform corrective actions as necessary.
Deploying NFVs in an IaaS Openstack environment.
CCNA to CCNP level IP networking, facilitating monitoring, verification and troubleshooting of connectivity between core data network elements, resource allocations and mobile data call path from RAN to target destinations
Innovate new and creative solutions to improve the way we work and deliver our projects.
Develop solutions to automate repetitive tasks and improve working efficiency within the team.
Some night shift work and weekend/holiday coverage will be required for maintenance window implementations and operational support.
Sr. Core Network Data Engineer - Mandatory Skills
Degree/Diploma in Computer Science, or Engineering.
Practical knowledge with DevOps tools and programming/scripting languages such as Python, Bash, PowerShell, Go, Ansible, Git.
Experience working with cloud computing platform such as Openstack
Understanding or experience of Agile, Lean, DevOps, adaptive working environment.
Understanding of 3GPP and IP networking protocols
Experience working on Linux operating system such as Ubuntu, Red Hat
Ability to work and adapt in a continuously changing work environment.
Proven interpersonal and leadership skills with a passion to see the work through the eyes of the customer.
Sr. Core Network Data Engineer - Assignment Start Date
ASAP – 12 months to start
Sr. Core Network Data Engineer – Assignment Location
Mississauga, ON	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0BreR47D9bMWJ28XlwS8rs2_GIFY3-vSdy_Xwl-swcV-nLMwXF3u7ntxl6CekgI_C941PK3ME-fZaKkO57-1wkBLP1YJrJbCIBfYcJqB9KIDoDrVOrCvprx2SXVax1W2nFLHP0vh9N_anvZ2h9WgJlNpd33YWMXSiFtPbvEKaV_D8ZZaT3dRtRDRz6DJO90xbT-e-3LnlEGIuVnlmN_DEzZG3Z5eaewNpRfK02MLaGx7vYaHhxMnkrXdwWfoLhJDz5VlLUHpzXU4JFN0KiqD_MVf3ZJrM5dv41rBxwT4Upg1SENsxUxqzy0oEIainbGa-wQvwMv4gllxg1WSXoGDvUO6Mruf6VbVS8NpAfCZlJnHEyKTA2sSfu0pugblBzMMfY3g42wgFmYpgK79M6GBU9Lrqm9gScSmgu20zGHVwgU6SCw5fWt9D6Xt-DG6hi1gzz4EF__k4DPvKSi39P5tMef8BszkJ7a5QSO0L2A5dm9Az1AQHW1HQoSHWsWJV_Rvtdirai4kqv7mk7_mRFs2JUjSc8j90PodGPc9yMcD6Xg0gHy-enEk1lftHX3oQjFeg__hEIh8lG5HgWHDv_Tlp3q2ITKUzYHszNushLR6a1r1ecaX7bdHVELV7TxCABLEv8=&p=7&fvj=0&vjs=3	Mississauga, ON	2020-08-16T23:47:52.000Z	acf8d2a1bace698b	Senior Core Data Engineer - 291021
2020081699	Cigna	2020-08-16T23:47:53.000Z	Mississauga, ON	Senior Data Engineer	The Data Engineer will be responsible for expanding and optimizing the data and data pipeline architecture across the enterprise. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, data architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing Express Script Canada’s data architecture to support our next generation of products and data initiatives.

ESSENTIAL FUNCTIONS:
Create and maintain optimal data pipeline architecture.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Optimize the value of technology investments in data management for the business by aligning the business architectures with technology architectures. Identify and drive key technology investments to meet business objectives, remediate technology and process gaps. Determine feasibility, cost and time required, compatibility with current system, and system capabilities.
Identify product, technology and process gaps in current data & technology architectures and recommend solutions to bridge gaps between the business and the data & technology deployed to support the business.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Coordinate with data users and key stakeholders across ESC’s Lines of Business to refine and achieve various long-term objectives for data architecture
Work with stakeholders including Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product.
Work with data and analytics experts to strive for greater functionality in our data systems.

QUALIFICATIONS:
Minimum 6 years of experience in a large-scale, multi-platform, multi-tier processing environment with a Bachelor’s degree in Information Systems or related field
Extensive experience on projects implementing Master Data Management and Enterprise Content Management techniques and platforms.
Expert domain knowledge & experience in data warehousing, reporting and advanced analytics platforms, encompassing data model design, dimensional modeling, master data management, cross-cutting concerns, common integration technologies, patterns & standards and emerging technologies.
Advanced knowledge of data architecture and analytics principles and practices for application development and data warehousing purposes.
Advanced knowledge of data model development and governance methods and practices
Previous data management experience in a data warehouse and data lake environment.
Experience with data center migrations, enterprise database consolidation, data warehouse migration and/or consolidation to data lakes or integrated data architectures.
Experience building multi-cloud analytics and technology strategies.
Highly proficient in Relational Database Management Systems
(RDBMS) and Big Data, data architecture, data modeling (including dimensional),
data warehousing, object-oriented methodologies, and client/server development.
Advanced knowledge of PowerDesigner or similar Data Modelling Tool
Advanced knowledge of Talend or similar Data Integration Tool
Hands-on experience using data technologies to implement data profiling tools and modern database implementations including Enterprise Data Lakes, graphDB, key-value pair, column-store, Big Table, RDF, In-Memory DB, etc..
Working knowledge of Hadoop platforms (Hortonworks or Cloudera) and key technologies like Apache Nifi, Kafka, Spark, Pig, Hive, NoSQL databases like MongoDB, Cassandra or Hbase.
Experience implementing data visualization tools like Tableau, Qlik, etc.
Willingness to work a flexible schedule to accommodate project deadlines and travel requirements

Assets:
Knowledge of the group health insurance (pharmacy, dental, other health) industry or adjudication systems is an asset
Knowledge of advanced analytics tools like R, SAS, and machine learning algorithms.
Knowledge of one or more programming or scripting languages like Java, C, C#, .Net, Javascript, PHP, Python
Knowledge of the DAMA Book of Knowledge
Knowledge of Big Data & Logical Warehouse architecture
Knowledge of TOGAF, Zachman or other
architecture frameworks

ABOUT EXPRESS SCRIPTS CANADA

Express Scripts Canada, a registered business name of ESI Canada, an Ontario partnership indirectly controlled by Express Scripts, Inc. (Nasdaq: ESRX), is one of Canada’s leading providers of health benefits management services. From its corporate headquarters in Mississauga, Ontario, just outside Toronto, Express Scripts Canada provides a full range of integrated pharmacy benefit management (PBM) services to insurers, third-party administrators, plan sponsors and the public sector, including health-claims adjudication and processing services, Home Delivery Pharmacy Services, benefit-design consultation, drug-utilization review, formulary management, and medical and drug-data analysis services, to better facilitate the best possible health outcomes at the lowest possible cost.

It will be a condition of employment that the successful candidate receives the Enhanced Reliability Clearance from the Federal Government. The candidate will be required to provide supporting documentation in order to receive Clearance if required.

We offer a competitive salary, along with a positive work environment built on solid corporate values, integrity, mutual respect, collaboration, passion, service and alignment.

We are an equal opportunity employer that promotes a diverse, inclusive and accessible workplace. By embracing diversity, we build a more effective organization that empowers our employees to be the best that they can be.

We are committed to creating a working environment that is barrier-free and we are prepared to provide accommodation for people with disabilities. Thank you for your interest in this position, however only qualified candidates will be contacted for an interview. No telephone calls please.

For more information about Express Scripts Canada, visit its Web site at www.express-scripts.ca

About Cigna
Cigna Corporation (NYSE: CI) is a global health service company dedicated to improving the health, well-being and peace of mind of those we serve. We offer an integrated suite of health services through Cigna, Express Scripts, and our affiliates including medical, dental, behavioral health, pharmacy, vision, supplemental benefits, and other related products. Together, with our 74,000 employees worldwide, we aspire to transform health services, making them more affordable and accessible to millions. Through our unmatched expertise, bold action, fresh ideas and an unwavering commitment to patient-centered care, we are a force of health services innovation.

When you work with Cigna, you’ll enjoy meaningful career experiences that enrich people’s lives while working together to make the world a healthier place. What difference will you make? To see our culture in action, search #TeamCigna on Instagram.	60d7283f8c769ec7	https://ca.indeed.com/rc/clk?jk=60d7283f8c769ec7&fccid=afbf8c270610a38a&vjs=3
20200816100	Toronto, ON	Salentica Sr. Project Consultant	https://ca.indeed.com/rc/clk?jk=e6c2ddd43685b4b3&fccid=609381a18aebd914&vjs=3	SS&C Advent	Salentica is a division of SS&C Technologies, Inc., a leading provider of software solutions and services for the international investment community. Investment managers, broker/dealers, sponsors, and custodians around the world use SS&C’s mission-critical and decision support systems. SS&C brings together experts in investments, providing superior client service from its headquarters in Windsor, CT, and subsidiaries in Canada, Australia, and Europe.
We have an immediate opening for a Salentica Project Consultant in our Jacksonville, Fl Office
Responsibilities
Lead and guide client through project implementation phases, including:
business process due diligence
Data analysis and mapping
solution design
application design
configuration
documentation of system requirements
Execute data analysis of client source data databases and client files
Execute business analysis on project implementation phases, including capture of detailed requirements for CRM
Work closely with data engineer to present data analysis in support of data migrations
Execute analysis and profiling on client data files, and where appropriate facilitate sessions with client and Salentica.
Work closely with project manager and client to execute typical business analysis tasks
Provide CRM configuration services to SS&C Salentica clients
Other project duties as assigned
Demonstrate superior communication skills
Ability to work independently as a self starter and in a team environment
Ability to work with internal teams to deliver client solutions
Working knowledge of financial services/asset management and RIA markets
Recognizes and minimizes Salentica project risk exposure.
Ensures project documents are complete, current, and stored appropriately
Position Qualifications

Industry Experience ( Wealth Management)
Bachelor’s degree, preferably in business IT systems
Ability to handle multiple projects/tasks simultaneously
Proven strong collaboration skills
Demonstrated aptitude and ability for planning and execution
Ability to recognize a client requirement, document, and outline steps for execution
Superior time management skills
Superior written, verbal communications skills
Ability to manage stakeholder expectations and report/communicate these back to the SS&C/Salentica Delivery team
Ability to take ownership of a task from initiation to completion
What will set you apart?

PMP designation
MS CRM and/or Salesforce certifications
Major IT implementation experience and understanding how systems are affected by business process change
Experience in dealing with complex business problems, identifying business/functional user requirements and recommending how to best support them through processes and applications
Ability to have influence without authority, and a strong ability to sell recommendations and solutions by stating advantages and value in business terms
Related financial service/wealth management industry experience

 SS&C Offers:
An extensive health benefits program which includes Health, Dental, and Vision
401k matching program
Generous Tuition Reimbursement and Training Allowance program
Business Casual work environment and Work-Life Balance.	e6c2ddd43685b4b3	2020-08-16T23:47:54.000Z
20200816101	Toronto, ON	https://ca.indeed.com/rc/clk?jk=f9ab2bbaf19601e6&fccid=b704562e07a2a03f&vjs=3	Join SADA as a Regional Cloud Engineering Manager!

Your Mission

As a Regional Cloud Engineering Manager, you will manage a team of Cloud Engineers and work collaboratively with other architects and engineers to design, prototype and lead the deployment of scalable Google Cloud Platform (GCP) architectures. You will work with engineering teams, customers and sales teams to qualify potential engagements, craft robust architectural proposals, and help craft Statements of Work (SOWs) that engineering teams can successfully execute. You're also hands-on, able to conduct experiments and build functioning prototypes that prove out ideas and build confidence in the solutions you advocate.

You will be a recognized expert within SADA and will develop a reputation with customers as well as the Google Cloud sales and professional services organizations for the quality of your work. You will also lead early-stage opportunity technical qualification calls, as well as lead client-facing technical discussions. You'll enable your team to unblock clients from adopting GCP in accordance with best practices, ensuring client satisfaction via technical excellence.

You will also be the face of SADA sales engineering for the region. This means building relationships with our clients, engineers, sellers, and partners to ensure a proactive approach to serving their needs. You'll be expected to provide guidance to each of these groups as well as to present strategies and overviews to our internal audience. You'll be the senior most technical resource during the sales process for our most important clients and will be expected to present complex technical solutions to our clients' business stakeholders.

In addition, you'll be presenting the work you and your team accomplish via case studies, webinars, speaking roles at conferences, etc.


Pathway to Success

#BeAChangeAgent: You are a rainmaker! You are way out in front of our delivery organization, meeting with the spectrum of corporate and enterprise customers that need our consultative services. You have your finger on the pulse of their technical needs and take pride in helping them solve their real-world problems on GCP.

You will be measured quarterly by a combination of (a) the volume of GCP revenue your team generates and (b) the satisfaction of our clients and sales teams.


Expectations

Required Travel - 30% travel to customer sites, conferences, and other related events. Due to the COVID-19 pandemic, travel has been temporarily restricted.

Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives

Training - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.


Job Requirements

Required Credentials:

Google Professional Cloud Architect Certified and/or Google Professional Data Engineer Certified, or able to complete one of the above within the first 45 days of employment.

Required Qualifications:

Mastery in at least one of the following domain areas as well as general expertise across all:
Infrastructure Modernization: migrating n-tiered workloads from on-prem and other clouds to GCP with near-zero-downtime (includes full spectrum of lift and shift to complete re-platforming scenarios), or building hybrid-cloud solutions based on Anthos and containerization technologies, such as Docker, Kubernetes, and Istio
Application Development: building custom web and mobile applications on top of the GCP stack
Data Engineering: data warehouse modernization (including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines and reporting/analytic tools), OLTP/OLAP data migrations, or backup, restore and disaster recovery solutions.
Experience providing oversight and direction of cloud projects
Experience leading technical design sessions, architecting and documenting technical solutions that are aligned with client business objectives, and identifying gaps between the client's current and desired end states
Experience strategizing, designing, architecting and leading the deployment of scalable solutions on GCP
Experience across multiple cloud platforms: GCP, AWS, Azure
Experience with container engines: Kubernetes, Docker, AWS Elastic Container Service
Experience with automation technologies including Terraform, Google Cloud Deployment Manager, AWS Cloud Formation or Microsoft Azure Automation
Experience working with engineering and sales teams to elicit customer requirements
Ability to communicate across business units and the ability to interface with and communicate complex technical concepts to a broad range of internal and external stakeholders
Time management skills with the ability to manage multiple streams and lead less experienced architects
Experience as a technical consultant or another customer-facing technical role

Useful Qualifications:

Experience managing teams of engineers
Hands-on experience designing and recommending elegant solutions that drive business outcomes
Experience building, designing and migrating complex cloud architectures
Strong aptitude for learning new technologies and techniques with a willingness and capability to skill up the team
A constant desire to learn
Ability to lead an in-depth client meeting/workshop across a broad range of topics including discovery, cloud compliance, and security
Deep understanding of best practices, design patterns, reference and compliance architectures with an uncanny ability to build and recommend these as needed
Knowledge and understanding of industry trends, new technologies and the ability to apply these to customer architectures to drive outcomes
Highly self-motivated and able to work independently as well as leading a team to success.

About SADA

Values: We built our core values on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA's values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.
Make them rave
Be data driven
Be one step ahead
Be a change agent
Do the right thing
Work with the best: SADA has been the largest partner in North America for GCP since 2016 and has been named the 2019 and 2018 Google Cloud Global Partner of the Year. SADA has also been awarded Best Place to Work by Inc. as well as LA Business Journal!

Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match, professional development reimbursement program as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.	SADA	Regional Cloud Engineering Manager	2020-08-16T23:47:55.000Z	f9ab2bbaf19601e6
20200816102	Toronto, ON	https://ca.indeed.com/rc/clk?jk=38794e819475c388&fccid=3002307a9e5b4706&vjs=3	Scotiabank	38794e819475c388	Requisition ID: 88492

Join the Global Community of Scotiabankers to help customers become better off.

Position:
The Senior Manager role in the Advanced Analytics team is designed for individuals with a curiosity for deriving insights out of data and applying them to address business opportunities, in partnership with global teams and business lines driving innovation and advanced analytics across the Enterprise.

The Senior Manager contributes to the development and refinement of business strategies and programs of enterprise banking by providing data-driven insights to the decision makers. This individual is responsible for compiling, aggregating, testing and validating different hypotheses from business lines and analytics teams and converting them into practical business intelligence and recommendations to identify and seize new business opportunities. She/he also manages analytical tools with both high-level dashboard and detailed drilldown analysis functionalities for various groups of internal users.

The Senior Manager will work closely with key internal stakeholders as well as various Strategy, Finance and Analytics departments on all matters related to customer, product and functions portfolio to drive growth aligned to the data and analytics strategic priorities. They will require a combination of business focus, strong analytical and problem-solving skills and programming knowledge to quickly draw insights from large, disparate data sources to support Bank strategies.
WHAT’S IN IT FOR YOU?
Opportunity to make an impact in the transformation of Scotiabank
Exposure to global teams and business lines where analytics techniques are being applied
Hands-on and end to end practical projects which provide an opportunity to gain new knowledge and develop skills
A compensation program with competitive salary, opportunities for annual performance incentives based on performance thresholds, a competitive benefits program and continuing education programs

Requirements & Qualifications:
5+ years of related experience in strategy, data analytics, financial services or related field
Strong knowledge of the banking industry is a must
Post-secondary degree in Business, Computer Science, Engineering or equivalent. MBA will be an asset
Experience cleaning, transforming and visualizing large data sets working with various data formats (e.g. unstructured logs, XML, JSON, flat files)
Hands-on experience with Big Data ecosystem tools (e.g. Hive, Pig, Sqoop and Spark) and strong skills for querying relational databases (e.g. SQL Server, DB2, MySQL, SAS)
Production experience with experimental design, statistical analysis, machine learning and predictive modeling (e.g. cross-sell, upsell, attrition, acquisition and lookalike models)
Experience with common machine Learning libraries in Python, R, Spark
Experience building, using and implementing visualization tools like PowerBI and Tableau
Strong collaboration skills with ability to translate technical knowledge into business value
Exceptional written and communication skills with ability to prepare presentations for Senior Management
Data Engineer and/or Dev Ops experience is a Plus

Accountabilities:
Work in an Agile environment to deploy new solutions
Perform self-initiated data mining and analysis to identify previously unknown trends, cause-effect relationships, and insights to support the development and refining of business strategies and programs
Will report newly-found business insights to Directors with recommendations on how to leverage these insights for the benefit of the Bank and driving growth strategy
Will Interview and work with Business Lines, Analytics and Enterprise Strategy and decision makers to gather BI reporting and analytics requirements; act as a subject matter expert for analytics and BI initiatives
Collaborate with business lines and other stakeholders and identify opportunities to drive business value by leveraging analytics
Develop models with Decision Sciences to validate business hypotheses
Acquire new data needed for new analytics initiatives working with Enterprise Data Management Office
Create and apply model and algorithm testing strategies to measure conduct multi-variate testing and A/B testing to measure effectiveness of models and make ongoing changes
Design, implement, and maintain dynamic dashboards as a tool for Executives and decision makers to get up-to-date information on the situation and changes in the business line and the industry
Will build analytics tools, create training materials, communicate and launch the results to key stakeholders
Continuously improve and manage the analytics / BI platform(s) by collecting and analyzing feedback and making recommendations
Liaise with Analytics, Enterprise Data Management Office, Enterprise Strategy to work on cross-departmental analytics initiatives
Support analytical use-case delivery, as well as customer/financial benefits tracking and communication
Coach a team of data scientist for analytics. This includes providing them guidance on how to work in agile environment, infuse analytics in building solutions and transforming the Bank’s digital capabilities, determine the analytics tools and technology that should be applied in each solution and coaching the team as they work with colleagues from across the organization.
Location(s): Canada : Ontario : Toronto
As Canada's International Bank, we are a diverse and global team. We speak more than 100 languages with backgrounds from more than 120 countries. Our employees are committed to a superior customer experience and use the Bank’s six guiding sales practice principles to ensure they act with honesty and integrity.

At Scotiabank, we value the unique skills and experiences each individual brings to the Bank, and are committed to creating and maintaining an inclusive and accessible environment for everyone. If you require accommodation (including, but not limited to, an accessible interview site, alternate format documents, ASL Interpreter, or Assistive Technology) during the recruitment and selection process, please let our Recruitment team know. If you require technical assistance, please click here. Candidates must apply directly online to be considered for this role. We thank all applicants for their interest in a career at Scotiabank; however, only those candidates who are selected for an interview will be contacted.	Senior Manager, Customer Insights - Analytics	2020-08-16T23:47:55.000Z
20200816103	Toronto, ON	About Etraveli Group
Etraveli is one of the leading global flight centric Online Travel Agencies (OTAs) with €4bn+in annual gross sales. We also operate flygresor.se, the #1 metasearcher in Sweden and Tripstack, the independent B2B arm of the group offering a variety of complex technology solutions.
Our diverse, dynamically growing team of 1000+ talented professionals is always on the lookout for more members to join our ranks and explore unlimited business opportunities together! Our 110 websites in 70+ countries across the globe include (but are not limited to) gotogate.com, pamediakopes.gr, mytrip.com, flightnetwork, supersavertravel.se, trip.ru & flygresor.se.
About Tripstack
TripStack is a brand under the Etraveli Group, It is revolutionizing the travel & technology field with a mission to challenge the status quo by solving difficult problems and changing the way millions of people travel.
We offer unique flight content, consisting of Virtual Interline & Low-Cost carriers in one search - this enables our partners to offer the end consumer the most relevant flight options at the lowest price.
Our company’s core values are what form our foundation. They inform how we work, how we execute, how we choose our future teammates and how we present ourselves to the world as TripStack employees.


Playing to Win.
#1 within flights
At TripStack we play as a team, we win as a team and we constantly aim to become better. We challenge convention, ourself and our teammates. We dare to think big and we value those who think differently.

Drive for Excellence

Good is not good enough.
We show a never give up attitude and we move fast, knowing we sometimes make mistakes, but that is needed to keep a high pace and to solve complicated stuff. And what we do is complicated, but by driving for high quality & excellence we simplify the product offering, enabling the end user more and cheaper ways to travel.


Act with Ownership

Have pride in your work
At TripStack we expect all to take accountability for your work but also accountability for the team work. We value proactivity, employees who take initiative to get things done- also when it is out of normal scope - it help us achieve our goals.
TripStack is part of the Etraveli Group, a global online travel agency and the fastest growing in Europe whose presence across the web spans 50 countries.
The Role
We are looking for an experienced data engineer to join our Virtual Interlining team for TripStack. Virtual Interlining is a technology we provide that combines flights from different carriers that don’t traditionally work together to go from point A to B via C. These unique fares provide significantly lower prices to the end consumer and much higher flight margins to our partners. Did you know that there are nearly 45 million flights operated worldwide on an annual basis? Indexing that data to provide customers with better flight options is a daunting and exciting mission.

You will use various methods to transform raw data into useful data systems. For example, you will create algorithms and conduct statistical analysis. Overall, you will strive for efficiency by aligning data systems with business goals.
To succeed in this role, you should have strong analytical and programming skills and the ability to combine data from different sources. Data engineer skills also include familiarity with several programming languages (preferably golang, python) and knowledge of machine learning methods would be considered an asset. You should have a good understanding of data warehousing concepts, having worked with large datasets and designed star/snowflake schemas. Experience with data processing tools like Apache Hadoop, Spark, Apache Druid( real-time OLAP) would be considered an asset.

Responsibilities:

Analyze and organize raw data
Build data systems and pipelines
Evaluate business needs and objectives
Interpret trends and patterns
Conduct data analysis and report on results
Prepare data for prescriptive and predictive modeling
Build prototypes
Combine raw information from different sources- flat files, databases, NoSQL caches
Explore ways to enhance data quality and reliability
Identify opportunities for data acquisition and prototype them using machine learning
Collaborate with Data Scientists and Architects on several projects
Requirements
Bachelor’s degree in Computer Science or technical discipline required. Master’s degree not mandatory but would be considered an asset.
4 years previous experience as a data engineer or in a similar role
Technical expertise with data models, data mining, and segmentation techniques
Knowledge of programming languages (Golang, Java, and Python)
Hands-on experience with SQL database design and proficient at writing SQL queries specifically Postgres
Knowledge of Apache Hadoop, Spark an asset
Knowledge of real-time OLAP like Apache Druid is an asset
Exceptional numerical and analytical skills	fb3fb7f802f5a80a	2020-08-16T23:47:57.000Z	Data Engineer	Etraveli Group	https://ca.indeed.com/rc/clk?jk=fb3fb7f802f5a80a&fccid=d76467c879ed211c&vjs=3
20200816104	Toronto, ON	SADA	2fef127f6d4b3e92	https://ca.indeed.com/rc/clk?jk=2fef127f6d4b3e92&fccid=b704562e07a2a03f&vjs=3	Join SADA as a Senior Pre-Sales Cloud Engineer!

Your Mission

As a Senior Pre-Sales Cloud Engineer at SADA, you will work collaboratively with other architects and engineers to design, prototype and lead the deployment of scalable Google Cloud Platform (GCP) architectures. You will work with engineering teams, customers and sales teams to qualify potential engagements, craft robust architectural proposals, and deliver Statements of Work (SOWs) that engineering teams can successfully execute. You're also hands-on, able to conduct experiments and build functioning prototypes that prove out ideas and build confidence in the solutions you advocate.

You will be a recognized expert within SADA and will develop a reputation with customers as well as the Google Cloud sales and professional services organizations for the quality of your work. You will demonstrate repeated delivery of project architectures that other engineers and architects demur to you for lack of expertise. You will also lead early-stage opportunity technical qualification calls, as well as lead client-facing technical discussions.

Pathway to Success

#BeAChangeAgent: You are a rainmaker! You are way out in front of our delivery organization, meeting with the spectrum of corporate and enterprise customers that need our consultative services. You have your finger on the pulse of their technical needs and take pride in helping them solve their real-world problems on GCP.

You will be measured quarterly by a combination of (a) the volume of signed SOWs that you shepherd through the sales funnel, and (b) the level of customer satisfaction measured at the end of each engagement.

As you continue to execute successfully, we will build a customized development plan together that leads you through the solution architecture or management growth tracks.

Expectations

Required Travel - 30% travel to customer sites, conferences, and other related events. Due to the COVID-19 pandemic, travel has been temporarily restricted.
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives
Training - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:
Google Professional Cloud Architect Certified and/or Google Professional Data Engineer Certified, or able to complete one of the above within the first 45 days of employment.
Required Qualifications:
Mastery in at least one of the following domain areas:
Infrastructure Modernization: migrating n-tiered workloads from on-prem and other clouds to GCP with near-zero-downtime (includes full spectrum of lift and shift to complete re-platforming scenarios), or building hybrid-cloud solutions based on Anthos and containerization technologies, such as Docker, Kubernetes, and Istio
Application Development: building custom web and mobile applications on top of the GCP stack
Data Engineering: data warehouse modernization (including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines and reporting/analytic tools), OLTP/OLAP data migrations, or backup, restore and disaster recovery solutions.
Experience providing oversight and direction of cloud projects
Experience leading technical design sessions, architecting and documenting technical solutions that are aligned with client business objectives, and identifying gaps between the client's current and desired end states
Experience strategizing, designing, architecting and leading the deployment of scalable solutions on GCP
Experience across multiple cloud platforms: GCP, AWS, Azure
Experience with container engines: Kubernetes, Docker, AWS Elastic Container Service
Experience with automation technologies including Terraform, Google Cloud Deployment Manager, AWS Cloud Formation or Microsoft Azure Automation
Experience working with engineering and sales teams to elicit customer requirements
Ability to communicate across business units and the ability to interface with and communicate complex technical concepts to a broad range of internal and external stakeholders
Time management skills with the ability to manage multiple streams and lead less experienced architects
Experience as a technical consultant or another customer-facing technical role
Useful Qualifications:
Hands-on experience designing and recommending elegant solutions that drive business outcomes
Experience building, designing and migrating complex cloud architectures
Strong aptitude for learning new technologies and techniques with a willingness and capability to skill up the team
Ability to lead an in-depth client meeting/workshop across a broad range of topics including discovery, cloud compliance, and security
Deep understanding of best practices, design patterns, reference and compliance architectures with an uncanny ability to build and recommend these as needed
Knowledge and understanding of industry trends, new technologies and the ability to apply these to customer architectures to drive outcomes
Highly self-motivated and able to work independently as well as in a team environment

About SADA

Values: We built our core values on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA's values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

Make them rave
Be data driven
Be one step ahead
Be a change agent
Do the right thing

Work with the best: SADA has been the largest partner in North America for GCP since 2016 and has been named the 2019 and 2018 Google Cloud Global Partner of the Year. SADA has also been awarded Best Place to Work by Inc. as well as LA Business Journal!

Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match, professional development reimbursement program as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.	2020-08-16T23:47:58.000Z	Senior Pre-Sales Cloud Engineer
20200816105	Toronto, ON	2020-08-16T23:47:59.000Z	40aaab8c4c4a4804	SADA	https://ca.indeed.com/rc/clk?jk=40aaab8c4c4a4804&fccid=b704562e07a2a03f&vjs=3	Technical Account Manager, Google Cloud Platform	Join SADA as a Technical Account Manager (TAM)!

Our technical competency is what sets SADA apart from other partners. We are transparent in our approach, celebrate our diverse workforce, and strengthen our competencies through training and development. As a TAM at SADA, you will collaborate with some of the most innovative organizations in their pursuit of a cloud-first approach.

This position requires a collaborative stakeholder with experience in infrastructure design, implementation, data engineering, and support. They must be able to architect solutions which include multi-cloud and hybrid-cloud scenarios. (S)he must be consultative, while anticipating potential needs and liabilities around growth and diversification. The ideal candidate is able to understand objectives/watchpoints and apply Google Cloud technology to solve for such outcomes.
If you like the idea of thinking strategically to help clients succeed with Google's cutting-edge solutions, apply now!

Accountabilities:
Provide robust and scalable technology solutions to enable our customers to scale their operations into GCP.
Advise customers on technology standards, methodologies and processes as they relate to infrastructure, application architecture, and data engineering.
Design and develop infrastructure blueprints for the implementation of new solutions that bring customers storage and compute workloads from cloud and non-cloud environments to GCP.
Participate in proof of concept development to assist in defining technology direction for our customers.
Conduct regular touchpoints with clients to review their cloud strategy and provide updates on best practices and new products.
Build solutions which leverage novel approaches to existing business and technology challenges.
Qualifications & Previous Experience

Must have:
Systems Engineering, System Administration, or Systems Architecture experience
Strong working knowledge of cloud offerings and solutions (Google Cloud Platform, Microsoft Azure, Amazon AWS)
Deep understanding of TCP, IP and other network protocols
Familiarity with DNS, DHCP and other network services
Experience administering a variety of Linux distributions
Experience with information security practices and procedures
Strong working knowledge of VMware, Hyper-V, KVM, or other virtual software
Ability to define infrastructure as code using tools like Google Deployment Manager, Terraform, Chef, etc
Strong scripting abilities via BASH, Python, etc
Knowledge of network topology and associated technologies
Mature understanding of DevOps best practices for cloud-native build and release pipelines
Strong technical aptitude and the ability to digest advanced technical topologies and concepts
Preferable:
A Bachelor of Science Degree in Computer Science or equivalent experience
Ability to write architectural design documents or review design documents provided by others
Knowledge of monitoring systems, capacity planning, and performance optimization across a variety of technologies (such as traditional compute, serverless, and data systems)
Knowledge and understanding of industry trends and new technologies and ability to apply trends to architectural needs
Full understanding of compute theory from hardware through serverless abstraction
Experience working with containerization technologies (Kubernetes, Docker, etc)
Certifications (strongly preferred, any of the below):
Google Certified Professional Cloud Architect
Google Certified Professional Data Engineer
AWS Certified Solutions Architect - Professional
AWS Certified DevOps Engineer - Professional


About SADA Systems, Inc

Values: We built our core values on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA's values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

Make them rave
Be data driven
Be one step ahead
Be a change agent
Do the right thing

Work with the best: SADA has been the largest partner in North America for GCP since 2016 and has been named the 2019 and 2018 Google Cloud Global Partner of the Year. SADA has also been awarded Best Place to Work by Inc. as well as LA Business Journal!

Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match, professional development reimbursement program as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.