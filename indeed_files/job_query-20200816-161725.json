{"1": {"job_id": "997673b8effa647f", "job_title": "Data Engineer (Intermediate 4-6 years) - Contract - Remote during COVID", "job_employer": "CorGTA", "job_location": "North York, ON", "job_link": "https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Dkh866153otbRJ8nVMuEXnXPd315JS377__3VyD6HnNQkmcaOHMemmy5pLrNV6j0WaVWEWWabnp4PP6-2u77DLfT_FCIGvCW8CQmQW140Z7EVU32Jnwl42FALsxnbc4n5M6FOTfoW2OoTMOcGG21HZGOoXL7fWT8eaT67VlcClh4sHQcJsHCX_TrsPvH-IiO6yo1PZsjbofmHO61EcqfDKo5OsECN1bB9IQ-nGYNrF7aUHR_gmlkv5xoKTUYov7v1y3t_PqVZYvqLPdOM_BSqDDZiLjHVteQiyEollMyX1frcT40PwDOaFbnVeysc9Pmr-Cr2AyedcErZ9ekT3OzY0j_l7iac_8qx2oeQv4P93PANihzOZXenoXuuVQZsSImUUzLHzMx1ep-nxXKGMB3LzozapskXpbcpbXMGnTTk3dpuafT3a_V9oRftms3j4jvRv6qFBkgGfTjzpponjOM1QVjSsQTpdvSPqvXxeYPC2jeLR2udswkNK&p=0&fvj=1&vjs=3", "job_description": "Role: Data Engineer (Intermediate 4-6 years)Structure: Contract (6 months initially)Location: North York, ONHours: Monday - Friday 40 hours (remote during COVID-19)Pay: $50.00 p/h inc.The role: We are looking for Intermediate Data Engineers who have roughly between 4-6 years experience who ideally has experience moving data from Google cloud over to Big query.However, if you are someone who has good experience with ETL and Data Pipeline Design & Data Validation experience and can script in Python and create strong SQL Queries then this will also be a potential match for this role.Ideal Qualifications to be successful in this role: - Degree in Computer Science, Engineering, Information Systems (or equivalent combination of skill and experience)- 4-6 years of experience working with data architecture projects- Excellent SQL coding and experience with a broad array of development tools and platforms including exposure to a big data environment tools/languages, such as SQL, R, SAS, Python, etc.- Experience creating relational database design and data models- Experience with various analytical data platforms and technologies- Experience with using REST API, Cloud (GCP Ideally)- Experience in custom or structured data integration design, implementation, and maintenance- Experience with business intelligence tools such as Qlikview, Tableau and Microstrategy (Nice to have for Data Visualizations)- Previous experience working in retail or eCommerce is highly preferred- Ability to curate data to tell stories and provide business insightsPlease apply with an updated resume and ensure the required skills you are able to speak to for this position are included.For more roles like this please go to www.corgta.com/find-a-job/Job Type: Full-timeSalary: $45.00-$50.00 per hour", "job_collect_date": "2020-08-16T16:17:26.000Z"}, "2": {"job_id": "a32b7c599156a218", "job_title": "BHJOB15656_15145 - Data Engineer w/ DevOps - Azure Cloud", "job_employer": "Myticas Consulting", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Bt274jo0r_e0cCCqhskoQvohXG3bfDAo8rIdE-Msu3yuVfyF2oiDFKHFOU-vxVulAfe8kPL7riFwoXa1bsfN-szjxO_8qUQWbPsJOnTg0EkutIoK2S4NNLL24JQh9e4DsE9hv907k2_zDbvj0a8WAj5oPSV5J0Yhgrx-GCyclKc-hDtWfEX8ku8peyU_BRgw8rCF_Kc_UlEAfGuzZ6Rv7rrD6-rCYCJI89ya-DWERhgsGWjV1hQSzvDasjA4tjGkzdHalD-0DfOVqJ_D5mgyik31Jk0ooG9NN3kv_LXRmX__fYuCuD60D_Rd3McGgyBY0vZgJfPiVDFMO3HmnsIJKZttdLM4ONmiLJQr9Dy2dckKaEu5RMyW_bf8bm__sKuA1en6GfbNqf_MfslN83rqJJB0XHl8P9BeGW3YjD2FdCgKEighCKr_t67FO3zh-RYKmxp3RfEkUrEw==&p=1&fvj=0&vjs=3", "job_description": "The growing team here at Myticas is looking for an experienced Data Engineer who would be interested in a remote contract opportunity within the Ottawa, Toronto and Montreal regions.\n\nAccountabilities:\n\nDefining and reviewing security design requirements for cloud infrastructure and application components.\nEvaluating architecture patterns from security perspective.\nBuilding and implementing security controls to enable enforcement of compliance with Cloud Control Objectives, using custom Azure policies and integrated controls in DevOps processes\n\nRequirements:\n\nStrong Data Engineer w/ DevOps expertise + Azure Cloud Experience\nMust know how to code and stand up scripts.\nExperience with Data Digestions\nExperience writing scripts to automate (infrastructure)\nARM Templating Expertise\nAzure Synapse Expertise\nSupport developing automated DevOps processes and procedures for the following Azure components:\nAzure Synapse (Azure DW) & Studio (private preview)\nAzure Data Catalog Gen 2 (Babylon \u2013 private preview)\nAzure Data Lake Storage Gen 2\nAzure ML\nML Flow\nAzure SQL Analysis Service\nAzure Databricks\nADF data pipelines for data loading to AzSQL/Synapse\nADF data pipelines for connecting to on-prem data sources for data\n\nCandidates looking to apply for this role are to send us an updated version of their resume in confidence. Our team will be sure to review all applicants and follow up accordingly at the conclusion of the review process.\n\nJob is also known as: Data Engineer, DevOps Engineer, Azure Cloud Engineer, Azure Engineer, Cloud Engineer\n\nINDMY", "job_collect_date": "2020-08-16T16:17:27.000Z"}, "3": {"job_id": "515fc373275101cd", "job_title": "Data Engineer", "job_employer": "International Financial Group", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Aji6ATb26Yyb09g5imj6JxAacRjLEDyoHO-vrhJmHqVeMhJT2nQd4uVV542dh9veDiQd_ptQwlb3PNoJncLbcc-EmRMaHyr0IoDf2TK3iSP0JjJYTnc-l1kPWdLBy6x7lVqb2TTaXJWq1fpNjxDUiryRn_PyJWNqqug_pAhwLyI0XYD4GYbqmRK-0Kg47lTtVL5j70Sxg1xsQ_fbVqjXQTsHUpfudt9hBP9L4MzsXkQ4OFe_f2hZBZ0vw3BvNmlZAkmFPUCpaYWd-MJ1lQY6iThvUGyCQW_bfmw1-9t1gNItDmyDVM7K1mWOqQtOwtYZ2uoYrl_8Z00qKiqlgIn7lrslOuccAPfYgmFp-wk9Lnm8cZlEyZ7anVETdaq4O4qB8_YmPbD3XkWfTYoN34b5MBFAQp_cJWTlJ4F0T6oupimY2Edc-O0jiWZkDmtncLOlw=&p=2&fvj=0&vjs=3", "job_description": "Job Title: Data Engineer\nLocation: Downtown Toronto\nThe individual our client seeks is highly analytical, self motivated, curious and has a track record of quantitative and qualitative analysis. If this sounds like you please apply below!\nOur technology client is past the boot strapping stage of a start up and continues to grow at an accelerated rate. The environment is fast paced and at times both structured and unstructured. This organization will provide you with the challenges you are looking for and the continued growth for learning you are looking for in your career. The office environment is fantastic and the client is all about social change. Our client seeks a passionate and talented Data Engineer who is comfortable working in a very fast paced environment.\nResponsibilities:\nHelp build, scale and maintain the data platform.\nPlay a key role in data infrastructure, analytics projects, and systems design and development.\nExtract the data, transform the data and load the data into a database or data warehouse (ETL).\nIntroduce new technologies to the environment through research and POCs.\nEnsure high quality standards are met (documentation is in place; quality checks are working and data in dashboards is updated according to data sources).\nWill assure standards to be followed by the data analysts & data scientists in terms of analytical data gathering and transformations.\nDelivering an enterprise data platform that will accelerate the delivery of business intelligence, machine learning and the ability to generate new insights.\nYou will focus on integrations and data modelling, ETL along with the automation of data sets.\n\nRequirements:\nBA/BS degree in Mathematics, Computer Science, Mathematics or related technical field, or equivalent practical experience.\nHave a deep technical understanding, hands-on experience in distributed computing, big data, ETL, dimensional modeling, columnar databases and data visualization.\nExperience working with data warehouses, including data warehouse technical architectures, infrastructure components, ETL and reporting/analytic tools and environments.\nExperience with data modelling techniques for modern data architectures.\nHands-on implementation experience with cloud data platforms (e.g. Azure, GCP, AWS).\nExperience in writing software in one or more languages such as Java, C++, Python, Go, Ruby and/or JavaScript.\nSoft skills: High performer, polished/high work ethic, smart learner, flexible, organized, has initiative/fast producer.\nKnowledge of Ruby on Rails, experience building data visualizations with D3 or in JavaScript / React and familiarity with Postgres would be a \u201cNice to have\u201d\n\nFor consideration please email your resume to \"Eileen@ifgpr.com\" with \"Data Engineer\" in the subject line.", "job_collect_date": "2020-08-16T16:17:28.000Z"}, "4": {"job_id": "fbcc383eeed7b6dc", "job_title": "Data Engineer", "job_employer": "Accenture", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=fbcc383eeed7b6dc&fccid=a4e4e2eaf26690c9&vjs=3", "job_description": "There is never a typical day at Accenture, but that\u2019s why we love it here! This is an extraordinary chance to begin a rewarding career at Accenture Technology. Immersed in a digitally compassionate and innovation-led environment, here is where you can help top clients shift to the New using leading-edge technologies on the most ground-breaking projects imaginable.\n\nInterested in building end-to-end marketing solutions for clients? Bring your talent and join Data which operates in the Interactive, Mobility and Analytics space. You will have opportunities to get involved in digital marketing, eCommerce and end-to-end mobility capabilities to help clients to improve productivity and more!\n\nWORK YOU\u2019LL DO\nWork across the Service Delivery Lifecycle to analyze, design, build, test, implement and/or maintain multiple system components or applications for Accenture or our clients\nResponsible for the maintenance, improvement, cleaning, and manipulation of data in the business\u2019s operational and analytics databases\nSupport our database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects\nDetermines methods and procedures on new assignments with guidance\nManages small teams and/or work efforts (if in an individual contributor role) at a client or within Accenture\n\nWHO WE\u00b4RE LOOKING FOR?\nMinimum 5 years of experience as a Data Engineer\nMust have experience with one of the Cloud Technologies (Azure or AWS)\nAzure cloud includes Spark, Python, Databricks, Synapse, Snowflake, Data Factory and ADLS\nAWS cloud includes Glue, EC2, EMR, Athena, redshift, Snowflake, S3, Spark, Python and Databricks\nExperience with Big Data technologies like MapReduce, Pig, Hive, HBase, Sqoop, Flume, YARN, Kafka, Storm and etc.\n2+ years of experience with at least one SQL language such as T-SQL or PL/SQL\n2+ years of work experience with ETL and data modeling\nExperience building and optimizing \u2018big data\u2019 data pipelines, architectures and data sets\nExperience in both batch and stream processing technologies\nExperience with object-oriented/object function scripting languages: Java, C++, Scala\nMachine learning experience with Spark or similar\nProfessional Skills Qualifications:\nProven success in contributing to a team-oriented environment.\nProven ability to work creatively in a problem-solving environment.\nDesire to work in an information systems environment.\nDemonstrated teamwork and collaboration in professional setting; either military or civilian.\nWHAT\u00b4S IN IT FOR YOU?\nCompetitive benefits, including a fair and balanced parental leave policy.\nFantastic opportunities to develop your career across industries with local and global clients.\nPerformance achievement and career mentorship: our performance management process focuses on your strengths, progress and career possibilities.\nOpportunities to get involved in corporate citizenship initiatives, from volunteering to charity work.\nTo learn more about Accenture, and how you will be challenged and inspired from Day 1, please visit our website at accenture.ca/careers.\nIt is currently our objective to assign our people to work near where they live. However, given the nature of our business and our need to serve our clients, our employees must be available to travel when needed.\n\nAccenture does not discriminate on the basis of race, religion, color, sex, age, non-disqualifying physical or mental disability, national origin, sexual orientation, gender identity or expression, or any other basis covered by local law. Accenture is committed to providing employment opportunities to current or former members of the armed forces.\n\nWe are committed to employment equity. We encourage all people, including women, visible minorities, persons with disabilities and persons of aboriginal descent to apply.\n\nAccenture is a leading global professional services company, providing a broad range of services and solutions in strategy, consulting, digital, technology and operations. Combining unmatched experience and specialized skills across more than 40 industries and all business functions \u2014 underpinned by the world\u2019s largest delivery network \u2014 Accenture works at the intersection of business and technology to help clients improve their performance and create sustainable value for their stakeholders. With 469,000 people serving clients in more than 120 countries, Accenture drives innovation to improve the way the world works and lives. Visit us at www.accenture.com", "job_collect_date": "2020-08-16T16:17:28.000Z"}, "5": {"job_id": "25f6a9d8e47d869e", "job_title": "Data Engineer, GEMINI (Contract)", "job_employer": "Vector Institute", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=25f6a9d8e47d869e&fccid=1a9a6d236cf89e9e&vjs=3", "job_description": "POSITION OVERVIEW\n\n\nThe Vector Institute is seeking a Data Engineer, GEMINI to join our team in Toronto. This role will predominantly work with the General Medicine Inpatient Initiative (GEMINI) team at St. Michael\u2019s Hospital, in addition to providing support to the Vector community.\n\n\nThe Data Engineer, GEMINI will primarily work in R to lead a team to automate and optimize GEMINI\u2019s data pipeline workflow including extracting, transforming and loading data, conducting quality and validation checks, and standardizing data from multiple data sources. The ideal candidate will have excellent programming skills, a strong understanding of data pipelines and analytic methods, an aptitude for data visualization, and strong leadership and communication skills. You will be joining a dynamic and mission-driven team of clinicians, scientists, and quality improvement experts.\nEMPLOYMENT TYPE\n\n\nContract (18 months)\n\n\nABOUT THE VECTOR INSTITUTE\n\n\nA thriving, independent not-for-profit, the Vector Institute strives to advance the Artificial Intelligence ecosystem in Ontario, developing and attracting the world\u2019s best machine learning and deep learning experts, and creating an unrivalled convergence of research, investment, entrepreneurialism, and economic growth. Located in the MaRS Discovery District in downtown Toronto, we are part of a dynamic and vibrant community of research, academia, health science and commerce.\n\n\nABOUT GEMINI\n\n\nCo-led by Drs. Fahad Razak and Amol Verma, GEMINI has developed methods to extract and standardize data from electronic health records to harness the tremendous potential of data generated through routine patient care of General Internal Medicine hospital inpatients for research and quality improvement purposes. GEMINI is a unique data platform in the Canadian healthcare landscape and currently exists at 7 hospitals, with data collected on 345,000+ patient visits, including billions of data points. The GEMINI data platform has recently been funded to expand to the 30 largest hospitals in Ontario to support data analytics that inform the COVID-19 pandemic response. GEMINI supports a network of nearly 100 collaborating scientists and more than 40 students, including clinicians, computer scientists, biostatisticians, epidemiologists, social scientists, and engineers.\nRESPONSIBILITIES\n\n\nDevelop scripts to extract, transform and load data from multiple data sources into GEMINI\u2019s platform;\n\nGather requirements and conduct analyses to design, develop and maintain secure data pipelines;\nAutomate and optimize GEMINI\u2019s data pipeline workflows;\nProvide leadership and guidance to data pipeline team;\nMaintain and update on-going quality assurance of data workflows;\nEngages with team and collaborators to understand needs and requirements;\nDocuments data pipeline architecture;\nRoutinely tests and monitors system for failures, errors, breaches;\nTroubleshoot problems;\nProvides teaching and training as needed;\nCreate and update Standard Operating Procedures and other documentation files, as needed;\nEstablish methods to improve and automate data workflow of rapidly changing COVID-19-related data;\nSupport GEMINI\u2019s data platform as required; and,\nPerform other functions as required (e.g., providing scientific advice and support to other members Vector\u2019s teams and corporate projects led by Vector).\n\n\nSUCCESS MEASURES\n\n\nGEMINI\u2019s data pipeline workflow is automated and optimized through the development of algorithms and procedures.\nHigh quality coding practices are maintained.\nSignificant contributions are made towards GEMINI\u2019s data platform.\n\n\nPROFILE OF IDEAL CANDIDATE\n\n\nA degree in Engineering, Computer Science and/or related discipline; graduate degree preferred\nAt least 5 years\u2019 relevant professional experience\nFully knowledgeable in designing and testing of data pipelines required\nExtensive experience in SQL as well as at least one of either R or Python required\nFamiliarity of key databases such as: PostgreSQL, MySQL, Oracle, etc. is preferred\nKnowledge of Linux commands and Shell scripts required\nStrong analytical, technical design and problem-solving skills required\nStrong working knowledge of Microsoft Office products (e.g. Outlook, Word, Excel, PowerPoint) required\nExperience with GitHub preferred\nGood judgement and understanding of what issues to escalate, resolve on your own, making suggestions for possible resolution required\nAbility to learn new technology expediently required\nDemonstrated success working within interdisciplinary teams\nExperience with the Ontario healthcare system is an asset\nExcellent attention to detail and proven ability to learn new skills\nExperience working independently and as part of a team\nExcellent organizational skills to manage multiple tasks in a timely manner\nDemonstrated flexibility and have the ability to adapt and manage changing priorities\n\n\nPlease address applications (cover letter and resume) to Kailyn Burke, HR Generalist, using the link provided. Review of applications will begin August 19, 2020. We thank all applicants for their interest in this exciting opportunity and will be in touch with those whose qualifications most closely match with our needs. Please note that candidates may be required to demonstrate proficiency in R.\n\nThe Vector Institute is committed to employment equity and diversity in the workplace and welcomes applications from women, racialized persons/visible minorities, Indigenous peoples, persons with disabilities, and LGBTQ+ persons. All qualified candidates are encouraged to apply.\n\n\nFurther, we are committed to fostering an environment of inclusivity and accessibility. If you require an accommodation at any point throughout the recruitment and selection process, please\n\ncontact hr@vectorinstitute.ai and we will happily work with you to meet your needs.", "job_collect_date": "2020-08-16T16:17:29.000Z"}, "6": {"job_id": "7cc1b932ac3ee1bf", "job_title": "Junior Data Engineer", "job_employer": "Fluid Hose & Coupling", "job_location": "Mississauga, ON", "job_link": "https://ca.indeed.com/company/FLUID-HOSE-&-COUPLING-INC/jobs/Junior-Data-Engineer-7cc1b932ac3ee1bf?fccid=5156f9b5283a3233&vjs=3", "job_description": "Job summaryResponsible for the completion of long-term IT engineering projects. Performs engineering design evaluations and works to complete projects within budget and scheduling restraints. Develops, implements, and monitors information systems policies and controls to ensure data accuracy, security, and regulatory compliance.Responsibilities\u00b7 Responsible for the analysis, improvement, cleaning, and manipulation of legacy relational databases at the company.\u00b7 Responsible for the architectural design and development of a modernized database, integrated with legacy systems.\u00b7 Responsible for the full-stack development of web applications which enhance customer and employee relationships with company data.\u00b7 Responsible for general development and maintenance of the company's IT systems, including web server maintenance and website updates.\u00b7 Works with the business\u2019s leadership team, in order to aid in the implementation of database requirements and troubleshoot any existent issues.\u00b7 Defines and builds the data pipelines that will enable faster, better, data-informed decision-making within the business.\u00b7 Manages Salesforce database base as required by sales leadershipEducation and Skill Requirement\u00b7 Computer Programming degree or equivalent. Equivalence may be demonstrated from past projects, results, etc\u00b7 Computer Programming/Cloud Computing/Data Science/Data Engineer/Database Management/IT Security and Infrastructure\u00b7 Ability to learn new technologies unguided/with minimal intervention (Adaptability)\u00b7 Complete knowledge of Web programming, design, e-commerce, SQL and relational DB Design. Understands data structures and hierarchies (1-2+ years of experience with SQL, or any other database management system)\u00b7 Clear understanding of System Architecture and ERP systems\u00b7 Detail oriented, organized with strong analytical and critical thinking skills\u00b7 Excellent communication skills combined with patience in the process\u00b7 Excellent Excel and MS Office skills with a deep understanding of Excel macros and programming. May be required to demonstrate this in a remote interview by example\u00b7 Quick learner and familiar with CRUD Operations\u00b7 Resourcefulness to keep moving forward (does not stall at a challenge)\u00b7 Preference will be given to those who understand App design and machine learningJob Types: Full-time, PermanentSalary: $40,000.00-$40,001.00 per yearBenefits:Dental CareExtended Health CareOn-site ParkingSchedule:8 Hour ShiftMonday to FridayEducation:Bachelor's Degree (Required)Work remotely:No", "job_collect_date": "2020-08-16T16:17:30.000Z"}, "7": {"job_id": "904ae8a46b296ad2", "job_title": "Data Engineer", "job_employer": "NLB Services Inc", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/company/NLB-Services-Inc/jobs/Data-Engineer-904ae8a46b296ad2?fccid=0b209b5526418580&vjs=3", "job_description": "Title: Data EngineerLocation: Toronto, CADuration: 6+ monthsExpertise in the design, creation, management, and business use of large datasets, across AWS Data and Analytics product.Excellent business and interpersonal skills to be able to work with business owners to understand data requirements, and to build the required data pipelines.Crafting, implementing, and operating stable, scalable, low cost solutions data pipelines to ingest real-time and event-based data using AWS technologies.5+ years of work experience with Data Pipelines, Data Modelling, and Data Architecture. \u00b7Expert-level skills in writing and optimizing SQL.Proficiency in python scripting languages and integrating it with lambda-based processing in AWSExperience operating very large data warehouses or data lakes.Experience with building data pipelines and applications to stream and process datasets at low latencies. \u00b7Demonstrate efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data. \u00b7Sound knowledge of distributed systems and data architecture (lambda)- design and implement batch and stream data processing pipelines using KinesisGood understanding of different DB technologies like Dynamo DB (NoSQL),Aurora DB ( RDBMS), Redshift (In-Memory DBs), across AWS stack.Job Types: Full-time, ContractSalary: $70.00-$75.00 per hourExperience:AWS: 1 year (Preferred)Work remotely:Temporarily due to COVID-19", "job_collect_date": "2020-08-16T16:17:30.000Z"}, "8": {"job_id": "6e7b18ed4b257445", "job_title": "Data Engineer", "job_employer": "CI Investments Inc", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=6e7b18ed4b257445&fccid=e981b35e4c0452a3&vjs=3", "job_description": "ABOUT US\nCI Investments Inc. is one of the country\u2019s largest investment fund companies. CI is known for its innovation and ability to adapt quickly to the changing needs of Canadian investors. It provides employees with a fast-paced and challenging work environment with opportunities for advancement. CI is part of CI Financial, a diverse group of financial services firms.\nPOSITION: Data Engineer\nLOCATION: Toronto (M5J 0A3)\nSTATUS: Contract (6 months with extension possibility)\nJOB OVERVIEW\nWe are currently seeking a Data Engineer to join our Client Reporting and Data Management team. The successful candidate will work closely with our data science team on the development of our centralized predictive analytics function. In this role, you will assist with solving high-value business problems by extracting and manipulating large, complex datasets for use by data scientists. The role will be a six-month contract position, with an option to extend based on performance.\nWHAT YOU WILL DO\nCollaborate with business analysts, data scientists, software engineers, and solution architects to develop data pipelines to feed our data marketplace\nExtract, analyze & interpret large, complex datasets for use in predictive modelling\nUtilize AWS tools to develop automated, productionized data pipelines\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nDevelop and support ETL code for data warehouse and data marts to support the reporting and data analytic systems.\nWHAT YOU WILL BRING\nAt least two years of work experience in quantitative analysis\nPost-secondary degree in a quantitative discipline\nExperience with large-scale, AWS big data storage such as S3 and EBS\nExperience creating ETL jobs using AWS Glue or Talend\nExperience with AWS Data pipeline tools like Cloudwatch and Stepfunctions\nExperience working with data preparation tools like Talend\nExperience in the Financial Services Industry is an asset\nStrong knowledge with programming methodologies (version control, testing, QA) and agile development methodologies.\nIn-depth knowledge of AWS tools required to develop automated, productionized data pipelines\nIn depth knowledge of and experience with relational, SQL and NoSQL databases\nFluency with SQL and Python\nExperience working with large, complex datasets\nExcellent communication, writing and interpersonal skills\nWHAT YOU CAN EXPECT FROM US\nOur dedication to the Employee Experience at CI is aimed at supporting, empowering and inspiring our talented team through:\nRecognition & Compensation\nTraining & Development\nHealth & Well-being\nCommunication & Feedback\nIf you are a passionate, committed and dynamic individual, please submit your resume in confidence by clicking \u201cApply\u201d.\nOnly qualified candidates selected for an interview will be contacted.\nCI Financial Corp. and all of our affiliates (\u201cCI\u201d) are committed to fair and accessible employment practices and we are committed to providing accommodations for persons with disabilities. If you require accommodations in order to apply for any job opportunities, or require this posting in an additional format, please contact us at accessible.recruitment@ci.com, or call 416-364-1145 ext. 4747. If you are contacted by CI regarding a job opportunity or testing and require accommodation in any stage of the recruitment process, please use the above contact information. We will work with all applicants to determine appropriate accommodation for individual accessibility needs.", "job_collect_date": "2020-08-16T16:17:31.000Z"}, "9": {"job_id": "59c48782b60cacd0", "job_title": "Data Engineer", "job_employer": "AMZN CAN Fulfillment Svcs, ULC", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=59c48782b60cacd0&fccid=fe2d21eef233e94a&vjs=3", "job_description": "3+ years of experience as a Data Engineer or in a similar roleExperience with data modeling, data warehousing, and building ETL pipelinesExperience in SQL\n\nAmazon.com was recently voted #2 most admired company in the US, #1 most innovative, and # 1 in Customer Service. We are investing heavily in building an excellent advertising business, and are responsible for defining, and delivering a collection of self-service performance advertising products \u2013 \u201calways-on analytics\u201d that is fully scalable and reliable. Our products are strategically important to our leadership, finance, economists, analysts, and BI partners to drive long-term growth. We mine billions of ad impressions and millions of clicks daily and are breaking fresh ground to create world-class products. We are highly motivated, collaborative, and fun loving with an entrepreneurial spirit and bias for action. With a broad mandate to experiment and innovate, we are growing at an unprecedented rate with a seemingly endless range of new opportunities.\nThe Advertising Analytics and Data Management team is looking for an exceptional Data Engineer who is passionate about data and the insights that large amounts of data can provide, who thinks/acts globally, and who has the ability to contribute to major novel innovations in the industry. The role will focus on working with a team of data engineers, business and tech savvy professionals to lay down scalable data architecture to ingest large amounts of structured and unstructured datasets and work with stakeholders to drive business decisions based on these datasets.\n\nThe ideal candidate will possess both a data engineering background and a strong business acumen that enables him/her to think strategically and add value to the customer experience. He/She will experience a wide range of problem solving situations, requiring extensive use of data collection and analysis techniques such as data mining and machine learning.\n\nThe successful candidate will work with multiple global site leaders, Business Analysts, Software Developers, Database Engineers, Product Management in addition to stakeholders in sales, finance, marketing and service teams to create a coherent customer view. They will:\n\nDevelop and improve the current data architecture using AWS Redshift, AWS S3, AWS Aurora (Postgres) and Hadoop/EMR.Improve upon the data ingestion models, ETL jobs, and alarming to maintain data integrity and data availability.Stay up-to-date with advances in data persistence and big data technologies and run pilots to design the data architecture to scale with the increased data sets of advertiser experience.Partner with analysts across teams such as product management, operations, sales, finance, marketing and engineering to build and verify hypothesis to improve the business performance.Manage weekly business reports via dashboards and paper the analyses of daily, weekly, and monthly reporting of performance via Key Performance Indicators.\n\nAWS ExperienceAdvertising domain knowledge is a plus\nAmazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.\n\n#sspajobs", "job_collect_date": "2020-08-16T16:17:32.000Z"}, "10": {"job_id": "76d86fb24f1bdc31", "job_title": "Data Engineer - Datamart Azure", "job_employer": "ARISOFT INC.", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/company/ARISOFT-INC./jobs/Data-Engineer-76d86fb24f1bdc31?fccid=1dbce7609f8e0b52&vjs=3", "job_description": "Data EngineerRequirements: \u00b7 8+ years of hands-on development experience in multiple projects, with progressively increasing responsibility and ETL background, and an understanding of type 2 dimension, and prior experience with data marts.\u00b7 5+ years of hands-on experience working with data warehousing like applications and big data.\u00b7 Experience leveraging big data technologies (One or more of Hadoop, Python, Spark) is required.\u00b7 Exposure to Microsoft Azure (or other cloud) platforms is preferred\u00b7 Experience working with various data exchange formats (JSON, CSV, XML etc.)\u00b7 Solid understanding of relational and dimensional database design and knowledge of logical and physical data models is preferred\u00b7 Experience with SDLC and/or Agile methodologies for project development, and participation in all phases of project development, is required\u00b7 Excellent knowledge of SQL and Linux shell scripting\u00b7 Experience in deploying and managing SQL and NoSQL databases is preferred\u00b7 Experience with job scheduling (TIDAL, CAWLA, Oozie) and file transfer (e.g. SFTP)\u00b7 Excellent diagnostic, analytical and problem-solving skills are preferred\u00b7 Experience with continuous delivery tools (Jenkins, Bamboo, Circle CI), and an understanding of the principles and pragmatics for build pipelines, artefact repositories, zero-downtime deployment, etc. is preferred\u00b7 Experience building real-time data pipelines using Kafka or spark streaming is preferredKey Accountabilities: \u00b7 Perform technical systems and data flow development in a variety of projects for complex front, middle and back office applications, with a focus on the reporting and analytics environment; this environment is built on a Microsoft Azure cloud and is underpinned by a Hortonworks Hadoop stack\u00b7 Perform technical systems and data flow design for small-to-medium sized projects\u00b7 Work with multiple project execution and deployment teams (e.g. Development Architecture, Release Management, Production Support)\u00b7 Work closely with source system SMEs to produce source to target mappings\u00b7 Translate business requirements to technical specifications\u00b7 Work closely with the technical leads and architecture teams, and align solutions that meet the departmental architectural vision\u00b7 Able to handle multiple priorities seamlesslyJob Types: Full-time, ContractSalary: $89,471.00-$170,126.00 per yearExperience:total IT: 10 years (Required)dartmart: 2 years (Required)Data engineer: 10 years (Required)Azure: 3 years (Required)Big data: 3 years (Required)ETL: 5 years (Required)Work remotely:Temporarily due to COVID-19", "job_collect_date": "2020-08-16T16:17:33.000Z"}, "11": {"job_id": "4cd09805ecea1dd2", "job_title": "Data Engineer", "job_employer": "Scotiabank", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=4cd09805ecea1dd2&fccid=3002307a9e5b4706&vjs=3", "job_description": "Requisition ID: 88577\n\nJoin the Global Community of Scotiabankers to help customers become better off.\n\nJOB SUMMARY\nThe Bank\u2019s Internal Audit Department plays a key role in the risk management process of the Bank. Our mandate is to provide independent and objective assurance over the design and operation of the Bank\u2019s internal controls and to advise senior management on improvements to the Bank\u2019s operations.\n\nThe role of the Data Engineer is to assist in the development of the data infrastructure and leverage existing analytical tools and methods to help Audit professionals to execute audits through enhanced data sampling, advanced analytics, continuous monitoring and visualization. As an active member of the team, they will grow in capability, coverage and knowledge to move the team forward.\n\nThe successful candidate will have a solid knowledge of R or Python, working knowledge of data analytics and statistical methods, as well as solid working knowledge of data acquisition and transformation (SQL, SAS). Experience with Mainframe commands and procedural programming, including Mainframe SAS, CLIST and ACF2, is strongly preferred. The ideal candidate will be able to communicate with Auditors to execute on projects independently and increase engagement.\nKEY ACCOUNTABILITIES\nIdentify, acquire and use large volumes of data to provide incremental business value through data analytics methods and algorithms that help the Internal Audit group execute Audits with greater efficiency, accuracy and coverage\nAssist in the development of the data infrastructure / architectureDesign and build a data pipeline that cleans, transforms and aggregates unstructured data in organized databases / data sourcesDevelop and write complex queries (that are suitable and scalable) for \u2018big data\u2019, build models, and ensure data is accessible to Data Scientists and Analysts, and the data sets are working smoothly.Develop data infrastructure necessary to support audits and continuous monitoring\nWork with large datasets and distributed computing tools for analysis, data mining and modelingCollaborate with business lines and other stakeholders to enhance Internal Audit's use of data and analyticsPrepare detailed documentation to outline data sources, models and algorithms used and developed, as well as results obtainedEnhance and acquire skills as needed to move the team forward\nFUNCTIONAL COMPETENCIES\nDegree in Statistics, Computer Science or related fieldExperience with data ingestion, cleansing, transformation, and integrationExtensive experience with statistical, data processing and analytical tools like R, Python and SASExperience on mainframe with CLIST, skeletons, and panels.Working knowledge of ACF2Experience with SAS on the mainframe including FSEDIT.Strong analytical skillsSQL skills for querying relational databases (e.g., SQL Server, DB2, MySQL)Nice to have: implementing visualization tools like Microsoft Power BI or TableauExcellent written and oral communication skills to communicate with Auditors and present clear resultsAt least 5 years experience with similar tools / functions\nLocation(s): Canada : Ontario : Toronto\nAs Canada's International Bank, we are a diverse and global team. We speak more than 100 languages with backgrounds from more than 120 countries. Our employees are committed to a superior customer experience and use the Bank\u2019s six guiding sales practice principles to ensure they act with honesty and integrity.\n\nAt Scotiabank, we value the unique skills and experiences each individual brings to the Bank, and are committed to creating and maintaining an inclusive and accessible environment for everyone. If you require accommodation (including, but not limited to, an accessible interview site, alternate format documents, ASL Interpreter, or Assistive Technology) during the recruitment and selection process, please let our Recruitment team know. If you require technical assistance, please click here. Candidates must apply directly online to be considered for this role. We thank all applicants for their interest in a career at Scotiabank; however, only those candidates who are selected for an interview will be contacted.", "job_collect_date": "2020-08-16T16:17:34.000Z"}, "12": {"job_id": "24716d17b181ff38", "job_title": "Intermediate Data Engineer", "job_employer": "Hitachi Solutions", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=24716d17b181ff38&fccid=28f79c18789111e8&vjs=3", "job_description": "Company Description\n\nThe Company\nHitachi Solutions is an impact-driven global leader in consulting dedicated to delivering competitive, end-to-end solutions based on the Microsoft Cloud. Our deeply connected teams are unified by our values and our commitment to helping clients succeed and compete with the largest global enterprises. We are a division of the 38th largest company in the world and carry the strength of a vast network of interconnected Hitachi companies all while remaining nimble, agile, and ready to pivot at a moment\u2019s notice.\nHitachi Solutions started as three founding partners and transformed into nearly 2,000 consultants, developers, and support personnel all around the globe. With 36 Microsoft Partner of the Year awards, Hitachi Solutions has established itself as a leading partner in the ever-growing landscape of technology consulting.\nThe Culture\nOur team is a mix of curious, fun, and get it done. We celebrate collaborative thought leadership and individual talents so that our ideas are translated into real-world results.\nEach day, our team members show up to work as a blank slate with the sole purpose of coloring their canvases with knowledge. We are a complex group of resilient learners and encourage one another to go beyond the limits of conventional expectations. As a member of the dynamic Hitachi Solutions team, you will be challenged, pushed, and unconditionally supported in your efforts to drive the business forward.\n** THIS ROLE IS OPEN TO ANY CANDIDATES FROM WITHIN CANADA**\n\nJob Description\n\nAs an Analytics Data Engineer for Hitachi\u2019s Azure Cloud Enablement Team you will be responsible to deliver high quality modern data solutions while being part of a dynamic and fast-growing team consisting of endless opportunities.\nThe successful candidate will be a self-motivated, passionate individual who strives to be the best at what he/she does and creates a trail of ecstatic Customers for life. This person will love to learn, contribute to the team and wants to be part of something great.\nKnowledge and Experience\nHands-on experience with the Azure Data Platform (Data Factory, Data Lake, Data Warehouse, Blob Storage, SQL DB, Analysis Services)\nData quality (profiling, cleansing, enriching)\nData Modeling \u2013 including design from conceptual to logical to physical data models\nConsidered to be an expert in T-SQL\nHands-on experience with MPP database technologies such as Azure SQL DW, Teradata Netezza, etc.\nExperience with multiple components listed, required:\nPower BI including DAX\nDatabase migration from legacy systems to new solutions\nDevOps\nInterpreted languages (i.e. python, C-sharp, Java, Scala, etc.)\nDatabricks\nLogicApps\nPowerApps\nHDInsight\nD365FO / CE experience as it pertains to data extraction\nKnowledge of Cap Theorum and Distributed Database Management Systems, iis considered an asset.\nOpportunity for a career path into a Data Scientist role if desired\n\nQualifications\n\nRequired skills / qualifications\nProven ability to engage customers to understand customer challenges and needs to develop technical solutions\n3+ years of hands on experience working with the Azure Platform and its relevant components\nProven experience of architecting Azure services into a solution platform on Microsoft Azure for Analytics\nMinimum 5 years hands on development experience with ELT/ETL using MS SQL Server, Oracle or similar RDBMS Platform\nFamiliarity with data visualization tools (e.g. PowerBI, Tableau etc.)\nExperience or desire to coach, mentor and provide leadership to team members\nPost-secondary degree/diploma in Business, Computer Science or a related discipline;\nStrong communication skills, both written and verbal\nPrepared for domestic and US travel as required\nPreferred considered an asset, NOT required:\nProject management experience\nDatabricks and Spark SQL\nPrevious Consulting experience\nAdditional Information\n\nOpportunity Benefits:\nMedical and Dental Benefit Package (including Long Term and Short Term Disability)Base salary plus targeted bonus package\nThis position can be based anywhere in Canada, though travel might be required.", "job_collect_date": "2020-08-16T16:17:36.000Z"}, "13": {"job_id": "6752644600b05d88", "job_title": "Senior Data Engineer", "job_employer": "AstraNorth", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/company/AstraNorth/jobs/Senior-Data-Engineer-6752644600b05d88?fccid=8116154355837827&vjs=3", "job_description": "Key Accountabilities: Perform technical systems and data flow development in a variety of projects for complex front, middle and back office applications, with a focus on the reporting and analytics environment; this environment is built on a Microsoft Azure cloud and is underpinned by a Hortonworks Hadoop stackPerform technical systems and data flow design for small-to-medium sized projectsWork with multiple project execution and deployment teams (e.g. Development Architecture, Release Management, Production Support)Work closely with source system SMEs to produce source to target mappingsTranslate business requirements to technical specificationsWork closely with the technical leads and architecture teams, and align solutions that meet the departmental architectural visionAble to handle multiple priorities seamlesslyRequirements: 8+ years of hands-on development experience in multiple projects, with progressively increasing responsibility5+ years of hands-on experience working with data warehousing like applications and big data.Experience leveraging big data technologies (One or more of Hadoop, Python, Spark) is required.Exposure to Microsoft Azure (or other cloud) platforms is preferredExperience working with various data exchange formats (JSON, CSV, XML etc.)Solid understanding of relational and dimensional database design and knowledge of logical and physical data models is preferredExperience with SDLC and/or Agile methodologies for project development, and participation in all phases of project development, is requiredExcellent knowledge of SQL and Linux shell scriptingExperience in deploying and managing SQL and NoSQL databases is preferredExperience with job scheduling (TIDAL, CAWLA, Oozie) and file transfer (e.g. SFTP)Excellent diagnostic, analytical and problem-solving skills are preferredExperience with continuous delivery tools (Jenkins, Bamboo, Circle CI), and an understanding of the principles and pragmatics for build pipelines, artefact repositories, zero-downtime deployment, etc. is preferredExperience building real-time data pipelines using Kafka or spark streaming is preferredJob Types: Contract, PermanentSchedule:Monday to FridayWork remotely:Temporarily due to COVID-19", "job_collect_date": "2020-08-16T16:17:37.000Z"}, "14": {"job_id": "e8f01b72a171ac29", "job_title": "Data Engineer", "job_employer": "goeasy", "job_location": "Mississauga, ON", "job_link": "https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DbNJQGwjC-NEA80-qlVyEGmSv34Xx0eW4t4u9rXSv_7_tlvMU2e-Gw_wrZ47s1zPCnrySU2yuEL07wtSyx7XOwYh5VPcVL7CxRhOYwHzR-CdmyBrwbeqommOn6y9FD_uKIYpUCb9-PC_XgwGzBDrPy7SD7JQndrQdnAb_vhLXKdYMfZPomYJ-75Mth0uAWSx10A1uBYum3zMwR-2O6zQrEhp3pJygJjHncBh8aqAlvW--Pi34nPsTy_0s3Yuk0wkWLj-XZuA3ImMV5jvGqzz2DJVviDI88sa42k9ZZZzeT5cxs_ls835W3yR-DyzNIsMyYgYU0o6FEpRo978BuYPPkTWzd5qtKusdX4Ujm0ynV4a6CjYQ3dBmal7RkkG4TTNU9aVKqfiZ4lkwT-YLjCeW4OHhYSNW1SDhcU4zEH4l6Sg0rNYzahdSHe3iQ7ypdWuDYn0KLx5m6mrGIELGsmjdBw9uE6NBjISMAb7AIhdXhnA==&p=13&fvj=0&vjs=3", "job_description": "If you are looking to join one of Canada\u2019s fastest growing companies, goeasy Ltd. is the place for you! Recognized as one of Canada\u2019s Most Admired Corporate Cultures, one of Canada\u2019s Top 50 Fintech\u2019s and one of North America\u2019s Most Engaged Workplaces, we want the best and brightest to join our team.\n\nWe are a publicly traded company on the TSX with over 4000% shareholder return since 2001, goeasy operates two main business units. easyfinancial is our consumer lending business that offers secured and unsecured installment loans of up to $35,000 and easyhome is Canada\u2019s largest merchandise lease-to-own company. It is our mission to provide everyday Canadians the chance for a better tomorrow, today by giving them access to the credit they need and by offering them a second chance when they have been turned down by banks and traditional lenders. With a retail network of nearly 400 locations across Canada and over 1900 employees, we are able to build lasting relationships with our customers as we help them rebuild their credit and graduate towards prime rates and a brighter financial future.\n\nThe Data Engineer on the Data Science and Business Insights team will build, integrate data from various resources, manage data in goeasy operational data store and enterprise data warehouse. This position will develop ETL (Extract, Transform and Load) with various tools on large datasets to ensure data is easily accessible, works smoothly, as well as maintain and expand the data warehouse for reporting and analysis. The Data Engineer will also work closely with the data architect on the design and architecture of our enterprise data warehouse.\n\nResponsibilities:\n\nDevelop data set processes for data modeling, mining and production\nDevelop and maintain ETL processes using SSIS, Scripting and data replication technologies\nParticipate in development of datamarts for reports and data visualization solutions\nResearch opportunities for data acquisition and new uses for existing data\nIntegrate new data management technologies and software engineering tools into existing structures\nSupport the translation of business requirements for data acquisition/manipulation and provide detailed specifications that can be passed downstream for use\nDevelop detailed technical specifications and operational support documentation in collaboration with Business Systems Analysts, BI Engineers and Architects.\nIdentify and communicate technical problems, process and solutions\nCreate Ad-Hoc queries and reports as needed along with providing on-going analytical support for these requests\nAssist in the collection and documentation of user\u2019s requirements\nEnsure that existing business processes dependent on the ODS/EDW are monitored and respond quickly to bug fixes, enhancement requests and production ETL related issues.\nDealing with the database users on a daily basis to ensure that problems are dealt with promptly and that appropriate fixes are made to resolve any problems.\nRecommend ways to improve data reliability, efficiency and quality\nEnsure systems meet business requirements and industry practices\nWork effectively with the Business Intelligence and Data Solutions Architects, Data and BI Engineers to ensure that all approved development and deployment procedures are followed.\n\nQualifications:\n\nBachelor\u2019s Degree in Computer Science, MIS, Computer Engineering or other Information Technology related degree\n4+ years working with SQL Server or comparable relational database system\n3+ years of extensive ETL development experience with SSIS and/or ADF\n4+ years of experience troubleshooting within a Data Warehouse environment\nExpert domain knowledge & experience in Data warehousing, encompassing data model design, dimensional modeling, naming conventions, cross-cutting concerns, common integration technologies, patterns & standards and emerging technologies.\nExpert Knowledge of SQL skills to build, debug, and optimize (developing procedures, functions, SQL queries, etc.) and working with large data sets and to perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.\n2+ years SQL Server Database administration experience\nCloud experience (Azure) is highly preferred\nExposure and experience with Python, R, Hadoop, Azure, other Big data and advanced analytics\nKnowledge of AI and ML developments/solutions/implementations\nExperience with multiple programming languages (PowerShell scripting, C#, others) with basic scripting skills.\nHigh level of technical aptitude\n\nInclusion and Equal Opportunity Employment\n\ngoeasy is an equal opportunity employer. In addition, goeasy is committed to providing accommodations for applicants upon request at any stage of the recruitment process in accordance with all legislative requirements throughout Canada. Please let us know if you require an accommodation due to a disability during any aspect of the recruitment process and we will work with you to address your needs.\n\nAdditional Information:\n\nAll candidates considered for hire must successfully pass a criminal background check, credit check, and validation of their work experience to qualify for hire. We thank all interested applicants, however we will only be contacting those for interview who possess the skills and qualifications outlined above.\n\nWhy should you work for goeasy?\n\nTo learn more about our great company please click the links below:\n\nPAID1234", "job_collect_date": "2020-08-16T16:17:37.000Z"}, "15": {"job_id": "54e0fcc38385e3a2", "job_title": "Data Engineer/Integration Consultant", "job_employer": "Copperstone Connect", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Ceb7pqR8iGmLuXIZQklF6pqed3xddONNM6Jjumj-ndyGK0kULQUmQ2REqfJ5KAzFVeK_es1WVg0s1sLgKW09UsvVRp0-dSizFQr1oNyK_VIqdjla00VU9GNVvu2x4v9p3hsL7NeF-6hBksbAaSgKazsaYTODGMKQQeVuYcxoOuxvFKuD0HRLD9ybgbcXTn1LNAtTW6U8Sx1DLV_KdN7s9mh81Jjks8_PKSYw2wASupkkcdyQj_q0bLciLzCs3gEiKSYRbX1MOrJH6ZB7qJXuZMUYLUoc3dhqqloMhUCtBBQ8ud03ADhPSWQSzBRWeqC9nAJE-w1uaAMXadCfyqnnS9lGUuOrF-rn8Tk4F07HXS6nyvagwom94GbzjaMpfYAo79If-xpOfqwtSNgDdaryzm4-hIQFTiPVFoUjpg3Wdz9gsskEMjKIqjBeEktQjRTq0vqzGZKxHcICbAVRY39AcHYdY2mQyhz19raFlha662gMGRjGTCArLUIUujiXFneOyFTvI47ucDTpJnDJ5LLCTVHfOj8IgZe4W7cto8CRXhJG3tyPvJPoHVL6h6hdYH5ouRUP1trNJln3Js2kfS7vmSOmO1uqjrduXrLrKkZMHpe_33vqyuA74EFriKmaIm3d4T-aQ2SvQaMVZO9dQDUG4ye-cziBEnTwHlUuJvPr7e_A==&p=14&fvj=0&vjs=3", "job_description": "Our client, a boutique-size consulting firm, prides itself on being at the forefront of innovation in the Big Data space. Founded in 2010, they provide thought leadership and implementation excellence within the ever-growing data and analytics world.\nThey take pride in having some of the most highly trained and experienced consultants in the industry which translates into optimal value for their clients. They were one of the first companies to provide analytics and data as a service, via the cloud, as early as 2010. They strive to make sure their customers are well-positioned with the best technologies/tools in the industry, constantly evaluating new and existing technology partnerships. Some of the more prominent companies they have partnered with include; Snowflake, DataRobot, MicroStrategy, Informatica, Amazon AWS & Microsoft. They continue to invest in their most valuable resource, their people. They do this through extensive training both on the job and through various educational programs.\nDue to growth, they are seeking to hire an Intermediate - Senior Integration Consultant.\n\nDesired Skills and Experience\n: University/College degree in Computer Science, Mathematics, Data Science and/or Relevant Degree\n: 5+ years hands-on development, configuration, scripting and administration experience with Data Integration platforms. (i.e. Informatica, Talend, DataStage, SSIS)\n: BI Experience (MicroStrategy, Looker, Tableau, PowerBI) considered a nice to have\n: Extensive theoretical and practical knowledge of data warehousing principles/concepts and practical development experience in all areas of the data warehousing life cycle\n: Experience with Data Management, ETL, Cloud Data (AWS), Data Integration\n: Knowledge of OLAP-related principles and concepts\n: Strong grasp of data modeling techniques and concepts (Normalized/Denormalized, Conceptual/Logical/Physical, Star, Snowflake, Data Vault)\n: Strong knowledge and experience with relational databases such as Snowflake, SQL Server, Oracle (Advanced knowledge of reading and writing SQL, Performance analysis and tuning)\n: Knowledge and experience with key Big Data technologies (Hive, Presto, Spark, Kafka, NoSQL databases, Semi-structured data access patterns (Json, Parquet, XML, etc.))\n: Strong Python scripting skills\n: Excellent communication skills\n: Great problem-solving skills\n: Leadership and good client management skills\n\nDay to Day Activities Would Include\n: Conduct relevant customer interviews to determine key business requirements and objectives\n: Build appropriate analytical data models based on outcomes of user interviews\n: Analyze and profile data systems to build source to target data mappings\n: Review ETL performance and conducts performance tuning as required on mappings/workflows or SQL\n: Administration and support of data integration infrastructure\n: 2nd level on-call support of ETL services as required\n\nYou will be responsible for attaining the following goals:\n: Attaining a minimum of 1 new accreditation/certification per year\n: Spending 80% or more of their time on billable work\n: Completing 90% or more of their agile delivery tasks on time\n: Demonstrating competency in 1 new relevant technology every year", "job_collect_date": "2020-08-16T16:17:39.000Z"}, "16": {"job_id": "997673b8effa647f", "job_title": "Data Engineer (Intermediate 4-6 years) - Contract - Remote during COVID", "job_employer": "CorGTA", "job_location": "North York, ON", "job_link": "https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Dkh866153otbRJ8nVMuEXnXPd315JS377__3VyD6HnNQkmcaOHMemmy5pLrNV6j0WaVWEWWabnp4PP6-2u77DLfT_FCIGvCW8CQmQW140Z7EVU32Jnwl42FALsxnbc4n5M6FOTfoW2OoTMOcGG21HZGOoXL7fWT8eaT67VlcClh4sHQcJsHCX_LKe755wJkSEAHtN7OwQTHx-TQFMKZRSnKowLbAXnbWphx83wrji9zmILMu56J3itBeyoRGU0PnFHfzspq9Yfyr46Il2YAIjmr-w6Z3DisBXg6z4at0mMT-HHK8tttOvmvcOQcDeHhs51IJyidO38-LtHMigovBdSjbQRZj6pZi6G5PlKS7JZoxUaAeJaU31ywZcvnJncG7BxARO3P9G9MSRiC_lSoDmNhTFsUVA5BvnA-le9U8MtoNAQ1Ao8hl_9pXVfQ_7HuGWUjVoOwl19ugBbNz3gJyuDhq4w_009dG5OKNsG2GXD6LrkUx61QJVA&p=0&fvj=1&vjs=3", "job_description": "Role: Data Engineer (Intermediate 4-6 years)Structure: Contract (6 months initially)Location: North York, ONHours: Monday - Friday 40 hours (remote during COVID-19)Pay: $50.00 p/h inc.The role: We are looking for Intermediate Data Engineers who have roughly between 4-6 years experience who ideally has experience moving data from Google cloud over to Big query.However, if you are someone who has good experience with ETL and Data Pipeline Design & Data Validation experience and can script in Python and create strong SQL Queries then this will also be a potential match for this role.Ideal Qualifications to be successful in this role: - Degree in Computer Science, Engineering, Information Systems (or equivalent combination of skill and experience)- 4-6 years of experience working with data architecture projects- Excellent SQL coding and experience with a broad array of development tools and platforms including exposure to a big data environment tools/languages, such as SQL, R, SAS, Python, etc.- Experience creating relational database design and data models- Experience with various analytical data platforms and technologies- Experience with using REST API, Cloud (GCP Ideally)- Experience in custom or structured data integration design, implementation, and maintenance- Experience with business intelligence tools such as Qlikview, Tableau and Microstrategy (Nice to have for Data Visualizations)- Previous experience working in retail or eCommerce is highly preferred- Ability to curate data to tell stories and provide business insightsPlease apply with an updated resume and ensure the required skills you are able to speak to for this position are included.For more roles like this please go to www.corgta.com/find-a-job/Job Type: Full-timeSalary: $45.00-$50.00 per hour", "job_collect_date": "2020-08-16T16:17:41.000Z"}, "17": {"job_id": "e8f01b72a171ac29", "job_title": "Data Engineer", "job_employer": "goeasy", "job_location": "Mississauga, ON", "job_link": "https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DbNJQGwjC-NEA80-qlVyEGmSv34Xx0eW4t4u9rXSv_7_tlvMU2e-Gw_wrZ47s1zPCnrySU2yuEL07wtSyx7XOwYh5VPcVL7CxRhOYwHzR-CdmyBrwbeqommOn6y9FD_uKIYpUCb9-PC_XgwGzBDrPy7SD7JQndrQdnAb_vhLXKdVu-YLetkhNBwF8J_l3_eQorXWsn4hswr0IjP6FOTDJfJdbcwZ81bf-enYgjlHlngdz7QrOXmbz76RUYA6o91gxrY6hCpEhA3eaGrK_Y92ZOyK-ZcmCkdq54pKFddIKW8UPiIYHATSK0eKQkdkKIK8L15hm9obWUIDeUHfecV9ba0HYuO6Nnzy-yTck0BOEE12-2Id8bKoCKv--zjFA3MLZSc3OmYG2_9ge7ZZtbPsGpVmS25Qq8h1HtzQf1B2lxtNTmoWEjuOVBgiWZ1-eU8BblhqnA1o-8VeJhIMnTofK4BDb9DcUEODgH9V02KFlwSA==&p=1&fvj=0&vjs=3", "job_description": "If you are looking to join one of Canada\u2019s fastest growing companies, goeasy Ltd. is the place for you! Recognized as one of Canada\u2019s Most Admired Corporate Cultures, one of Canada\u2019s Top 50 Fintech\u2019s and one of North America\u2019s Most Engaged Workplaces, we want the best and brightest to join our team.\n\nWe are a publicly traded company on the TSX with over 4000% shareholder return since 2001, goeasy operates two main business units. easyfinancial is our consumer lending business that offers secured and unsecured installment loans of up to $35,000 and easyhome is Canada\u2019s largest merchandise lease-to-own company. It is our mission to provide everyday Canadians the chance for a better tomorrow, today by giving them access to the credit they need and by offering them a second chance when they have been turned down by banks and traditional lenders. With a retail network of nearly 400 locations across Canada and over 1900 employees, we are able to build lasting relationships with our customers as we help them rebuild their credit and graduate towards prime rates and a brighter financial future.\n\nThe Data Engineer on the Data Science and Business Insights team will build, integrate data from various resources, manage data in goeasy operational data store and enterprise data warehouse. This position will develop ETL (Extract, Transform and Load) with various tools on large datasets to ensure data is easily accessible, works smoothly, as well as maintain and expand the data warehouse for reporting and analysis. The Data Engineer will also work closely with the data architect on the design and architecture of our enterprise data warehouse.\n\nResponsibilities:\n\nDevelop data set processes for data modeling, mining and production\nDevelop and maintain ETL processes using SSIS, Scripting and data replication technologies\nParticipate in development of datamarts for reports and data visualization solutions\nResearch opportunities for data acquisition and new uses for existing data\nIntegrate new data management technologies and software engineering tools into existing structures\nSupport the translation of business requirements for data acquisition/manipulation and provide detailed specifications that can be passed downstream for use\nDevelop detailed technical specifications and operational support documentation in collaboration with Business Systems Analysts, BI Engineers and Architects.\nIdentify and communicate technical problems, process and solutions\nCreate Ad-Hoc queries and reports as needed along with providing on-going analytical support for these requests\nAssist in the collection and documentation of user\u2019s requirements\nEnsure that existing business processes dependent on the ODS/EDW are monitored and respond quickly to bug fixes, enhancement requests and production ETL related issues.\nDealing with the database users on a daily basis to ensure that problems are dealt with promptly and that appropriate fixes are made to resolve any problems.\nRecommend ways to improve data reliability, efficiency and quality\nEnsure systems meet business requirements and industry practices\nWork effectively with the Business Intelligence and Data Solutions Architects, Data and BI Engineers to ensure that all approved development and deployment procedures are followed.\n\nQualifications:\n\nBachelor\u2019s Degree in Computer Science, MIS, Computer Engineering or other Information Technology related degree\n4+ years working with SQL Server or comparable relational database system\n3+ years of extensive ETL development experience with SSIS and/or ADF\n4+ years of experience troubleshooting within a Data Warehouse environment\nExpert domain knowledge & experience in Data warehousing, encompassing data model design, dimensional modeling, naming conventions, cross-cutting concerns, common integration technologies, patterns & standards and emerging technologies.\nExpert Knowledge of SQL skills to build, debug, and optimize (developing procedures, functions, SQL queries, etc.) and working with large data sets and to perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.\n2+ years SQL Server Database administration experience\nCloud experience (Azure) is highly preferred\nExposure and experience with Python, R, Hadoop, Azure, other Big data and advanced analytics\nKnowledge of AI and ML developments/solutions/implementations\nExperience with multiple programming languages (PowerShell scripting, C#, others) with basic scripting skills.\nHigh level of technical aptitude\n\nInclusion and Equal Opportunity Employment\n\ngoeasy is an equal opportunity employer. In addition, goeasy is committed to providing accommodations for applicants upon request at any stage of the recruitment process in accordance with all legislative requirements throughout Canada. Please let us know if you require an accommodation due to a disability during any aspect of the recruitment process and we will work with you to address your needs.\n\nAdditional Information:\n\nAll candidates considered for hire must successfully pass a criminal background check, credit check, and validation of their work experience to qualify for hire. We thank all interested applicants, however we will only be contacting those for interview who possess the skills and qualifications outlined above.\n\nWhy should you work for goeasy?\n\nTo learn more about our great company please click the links below:\n\nPAID1234", "job_collect_date": "2020-08-16T16:17:42.000Z"}, "18": {"job_id": "ed86ae879d5f5db2", "job_title": "Senior Big Data Engineer (f/m/x)", "job_employer": "Avira Operations GmbH & Co. KG", "job_location": "Canada", "job_link": "https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DVAdzec4bw6rQ4FO3PDdwz3gCZ43d7Gsr17jnu3o1g5CsoUOgyHaCzZOjoCGyji8ntYJxUdGd2FmwbktcVakqfF59toD5WP-Ro_DzTHrpq6YpYqUfP4QH5sxN4c835IUO3mqkddooJyzOy5iObgoXm6nmIVAYiD64A4oqzCBZE9BYxg9Mfju7kPPPrGDAUXFbENQ2FPMzwtKyVouX2eKl_BzR_2MqXrhEC6URzMH46M_gimTMzeskn1bcVsVzdc0DFfNUf06U1Eo0SSU5oP1sYsM55zuksXIkwVKsDC2MlcROIlPx71xas2apd2LxWcJ5c4C1xT5LgRk0Ww-_Hg7WUAB_IzA_DmZxWJ-Y39PtaAwxO3NH2WLywPd2m_MvAf-brQLz_-qNc2shIsEZcpjD6eXpxRKTCC_7pG3LO71PlGKK8atxsaqR0Zzk3CqUZzkfhHJlLk_9Ahg==&p=2&fvj=0&vjs=3", "job_description": "Senior Big Data Engineer (f/m/x)\n\nLocation: Tettnang\n\nThe Challenge\n\nAs a Data Engineer in the Threat Intelligence team, you will design, implement and maintain the Threat Intelligence Platform that transforms massive amounts of (real-time) data from various sources into descriptive knowledge about emerging threats. You challenge our status quo and define standards for data-intensive analytic pipelines. Your projects will collect and process security-related intelligence and will then provide that data to the company as a whole so Avira can protect its users on a global scale.\n\nThe team\nWe are an international team of engineers and researchers. We are a self-organized and result-oriented team, always looking to perfect the art of automation. Our systems and services are integrated within the Avira Protection Cloud that protect consumers and businesses around the world.\n\nWhat you will do\nStarting from day one you will familiarize yourself with our big data ecosystem and tool stack, as well as the main data source systems.\n\nWithin the first 3 months, you are fully integrated into the Threat Intelligence Platform development and maintenance. You are taking responsibilities for certain data stream integrations.\n\nWithin the first 6 months you are an expert for the Threat Intelligence Platform having a complete understanding of its integration into Avira\u2019s protection technologies and architecture landscape.\n\nWithin the first year you will be truly integrated within a team that delivers the most up-to-date Threat Intelligence solutions to millions of customers. You take full ownership of cross-department projects and collaborate efficiently with other teams. You are keen to improve existing technologies and implement new and innovative features. You also play a big part in securing and supporting the digital lifestyle of our customers and making the Internet a more secure place.\n\nYour Qualifications\n3+ years' experience in software development, with excellent development skills in PythonIndustrial experience with data-intensive projects in the Hadoop ecosystem, Spark, Kafka, and AirflowExperience in building data ingestion pipelinesGood knowledge in designing, building, using and maintaining REST APIsExperience with SQL/NoSQL databases, especially creating scalable, multi-node deploymentsExperience with AWS components and principles are a plusStrong analytical, technical, organizational and communication skills\nThe position is based in Tettnang, Germany, near lake Constance.\n\nBenefits and perks:\nNew Work\nStylish building with roof terracesCanteen and ChocaViraModern office concept\n\nLearning & Development\nUnlimited access to UdemyTrainings & Conferences\nSpecialist Career\n\nHealth & Wellbeing\nGym and fitness courses\u201cJobRad\u201d bike leasingMedical checkups\n\nFamily & Living\nRelocation PackageVacation child careAvira Prime licences\n\nEvents\nOnboarding eventsMonthly Employee MeetingsSummer & Christmas parties\n\nAN OPPORTUNITY TO MAKE A DIFFERENCE\nUpdate: Although there\u2019s a lot of disruption nowadays due to Covid-19, we at Avira are continuing to run our daily business activities so that we stay true to the promise to our customers - now even more than ever: Protecting people in the connected world.\n\nAnd we are doing this from the safeness of our own homes. Among other things, this means we are still hiring, but we have moved all our interviews online and all our colleagues are being onboarded remotely\n\nSo join us and you will be able to work from home until the danger is over.\n\nWe\u2019re an international software company at the forefront of imagining the future of digital security. Avira\u2019s award-winning products and technology protect over 500 million users in the connected world.\n\nWhat makes us special? First and foremost \u2013 it\u2019s the authentic people at Avira. We have a great community feeling that fosters your uniqueness and offers the space to reflect, the feedback to grow, and the freedom to innovate. If you are looking for a culture that also encourages aspiration and professional excellence, get in touch with us and discover the Avira experience firsthand.\n\nLena Komarek\nHR Recruiter\nAvira Operations GmbH & Co. KG\nHuman Resources\nKaplaneiweg 1\nD-88069 Tettnang, Germany\nPhone: +49 (0) 7542-500 -2207\nE-Mail: lena.komarek(at)avira.com", "job_collect_date": "2020-08-16T16:17:43.000Z"}, "19": {"job_id": "874eae48579a7736", "job_title": "Big Data Engineer", "job_employer": "Rackspace", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=874eae48579a7736&fccid=b60c9324aeb7df96&vjs=3", "job_description": "As a full spectrum AWS integrator, we assist hundreds of companies to realize the value, efficiency, and productivity of the cloud. We take customers on their journey to enable, operate, and innovate using cloud technologies \u2013 from migration strategy to operational excellence and immersive transformation.\n\nIf you like a challenge, you\u2019ll love it here, because we\u2019re solving complex business problems every day, building and promoting great technology solutions that impact our customers\u2019 success. The best part is, we\u2019re committed to you and your growth, both professionally and personally.\n\nOverview\n\nOur Big Data Engineers are experienced technologists with technical depth and breadth, along with strong interpersonal skills. In this role, you will work directly with customers and our team to help enable innovation through continuous, hands-on, deployment across technology stacks. You will work to build data pipelines and by developing data engineering code ( as well as writing complex data queries and algorithms.\n\nIf you get a thrill working with cutting-edge technology and love to help solve customers\u2019 problems, we\u2019d love to hear from you. It\u2019s time to rethink the possible. Are you ready?\nWhat You\u2019ll Be Doing:\nBuild complex ETL code\nBuild complex SQL queries using MongoDB, Oracle, SQL Server, MariaDB, MySQL\nWork on Data and Analytics Tools in the Cloud\nDevelop code using Python, Scala, R languages\nWork with technologies such as Spark, Hadoop, Kafka, etc.\nBuild complex Data Engineering workflows\nCreate complex data solutions and build data pipelines\nEstablish credibility and build impactful relationships with our customers to enable them to be cloud advocates\nCapture and share industry best practices amongst the community\nAttend and present valuable information at Industry Events\nTraveling up to 50% of the time\nQualifications & Experience:\n3+ years design & implementation experience with distributed applications\n2+ years of experience in database architectures and data pipeline development\nDemonstrated knowledge of software development tools and methodologies\nPresentation skills with a high degree of comfort speaking with executives, IT management, and developers\nExcellent communication skills with an ability to right level conversations\nTechnical degree required; Computer Science or Math background desired\nDemonstrated ability to adapt to new technologies and learn quickly\n\n\nAbout Rackspace Technology\nWe are the multicloud solutions experts. We combine our expertise with the world\u2019s leading technologies \u2014 across applications, data and security \u2014 to deliver end-to-end solutions. We have a proven record of advising customers based on their business challenges, designing solutions that scale, building and managing those solutions, and optimizing returns into the future. Named a best place to work, year after year according to Fortune, Forbes and Glassdoor, we attract and develop world-class talent. Join us on our mission to embrace technology, empower customers and deliver the future.\n\n\nMore on Rackspace Technology\nThough we\u2019re all different, Rackers thrive through our connection to a central goal: to be a valued member of a winning team on an inspiring mission. We bring our whole selves to work every day. And we embrace the notion that unique perspectives fuel innovation and enable us to best serve our customers and communities around the globe. We welcome you to apply today and want you to know that we are committed to offering equal employment opportunity without regard to age, color, disability, gender reassignment or identity or expression, genetic information, marital or civil partner status, pregnancy or maternity status, military or veteran status, nationality, ethnic or national origin, race, religion or belief, sexual orientation, or any legally protected characteristic. If you have a disability or special need that requires accommodation, please let us know.", "job_collect_date": "2020-08-16T16:17:44.000Z"}, "20": {"job_id": "9b3e8aaf625b3563", "job_title": "Data Engineer - Hardware", "job_employer": "Square", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=9b3e8aaf625b3563&fccid=09abad886b83c501&vjs=3", "job_description": "Company Description\n\nSquare builds common business tools in unconventional ways so more people can start, run, and grow their businesses. When Square started, it was difficult and expensive (or just plain impossible) for some businesses to take credit cards. Square made credit card payments possible for all by turning a mobile phone into a credit card reader. Since then Square has been building an entire business toolkit of both hardware and software products including Square Capital, Square Terminal, Square Payroll, and more. We\u2019re working to find new and better ways to help businesses succeed on their own terms\u2014and we\u2019re looking for people like you to help shape tomorrow at Square.\n\nJob Description\n\nSquare began with a simple yet revolutionary piece of hardware\u2014the Square Reader. We have expanded that vision to give our merchants access to the latest secure payment technologies (contactless and chip cards) and easy-to-use Point of Sale products. Today our hardware carries over $30 billion in transactions annually. Measuring the performance of our hardware, and the journey in getting the hardware to our merchants, is essential in ensuring that our merchants\u2019 experience with Square is as seamless as possible.\nWe need your help to collect data about our hardware, organize that data, and make it available to analysts across Square. As a member of the Hardware data engineering team, you will define, develop, and manage a variety of data infrastructure components and pipelines so that our analytics teams and collaborators have trusted data to inform decisions and insights.\nYou will:\nJoin the small but mighty Hardware data engineering team that partners with internal Hardware data consumers to understand their needs and to source the right data sets to work on\nWork in a remote environment that allows you to build scalable Hardware data pipelines and tools to ingest data from internal/external sources to our cloud data stack (Snowflake / AWS)\nDevelop data structures to support flexible analysis of this data, including creating data models, structuring optimized ETLs, designing validation scripts\nTroubleshoot technical issues with platforms, data discrepancies, alerts, etc.\nBe a voice between the Hardware team and Square\u2019s data community. Help the hardware team\u2019s data producers embrace best-practices, represent the hardware team\u2019s voice in data infrastructure planning discussions.\nReport into a data engineering manager on the Platform Infrastructure Engineering team\n\nQualifications\n\nYou have:\n3+ years building and supporting reporting data systems built on columnar oriented RDBMS systems (e.g. Snowflake, BigQuery, Redshift, Vertica, etc.)\nStrong experience building data pipelines from heterogeneous data sources (e.g. Event streams, Flat Files, RDBMS, REST APIs, SFTP, etc.) to support real-time operational and analytical workloads\nTechnical accomplishments working with SQL, ETL, and Apache Airflow; and knowledge of at least one mature programming language (Ruby (and Rails), Python, Java, Go, or similar)\nExperience with Linux/OSX command line, version control software (git), and general software development\nExperience with cloud based data tools and services (e.g. AWS Lambda/S3/Transfer, GCP Cloud Functions/GCS, Fivetran, etc.)\nAdditional Information\n\nAt Square, our purpose is to empower \u2013 within and outside of our walls. In order to build the best tools for the businesses and customers we support all over the world, we have to start at home with a workforce as diverse and empowered as our sellers. To this end, we take great care to evaluate all employees and job applicants equally, based on merit, competence, and qualifications. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, disability status, or any other characteristic protected by law. We encourage candidates from all backgrounds to apply. Applicants in need of special assistance or accommodation during the interview process or in accessing our website may contact us by sending an email to assistance(at)squareup.com. We will treat your request as confidentially as possible. In your email, please include your name and preferred method of contact, and we will respond as soon as possible.\n\nPerks\n\nAt Square, we want you to be well and thrive. Our global benefits package includes:\n\nHealthcare coverage\nRetirement Plans\nEmployee Stock Purchase Program\nWellness perks\nPaid parental leave\nFlexible time off\nLearning and Development resources", "job_collect_date": "2020-08-16T16:17:45.000Z"}, "21": {"job_id": "1e5daca6232e90a6", "job_title": "Data Engineer", "job_employer": "SDK", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/company/SDK/jobs/Data-Engineer-1e5daca6232e90a6?fccid=185dce58b577e4a6&vjs=3", "job_description": "Job DescriptionSDK is looking for Big Data Engineers that will work on the collecting, storing, processing, and analyzing of huge sets of data. The Data Engineer must also have exceptional analytical skills, showing fluency in the use of tools such as MySQL and strong Python, Shell, Java, PHP, and T-SQL programming skills. He must also be technologically adept, demonstrating strong computer skills. The candidate must additionally be capable of developing databases using SSIS packages, T-SQL, MSSQL, and MySQL scripts.The candidate will also have an ability to design, build, and maintain the business\u2019s ETL pipeline and data warehouse. The candidate will also demonstrate expertise in data modeling and query performance tuning on SQL Server, MySQL, Redshift, Postgres or similar platforms.Experience:ETL: 5 years (Preferred)Software Development: 5 years (Preferred)Data Integration: 5 year (Preferred)Spark Programming (Azure Databricks preferable)Python, Java & SQLKnowledge of Azure Cloud (Data Platform Technologies)Experience and commitment to development and testing best practices.Manage high volume, high traffic GDPR solutions buildSCALA a nice to haveSDK is looking for Big Data Engineers that will work on the collecting, storing, processing, and analyzing of huge sets of data. The Data Engineer must also have exceptional analytical skills, showing fluency in the use of tools such as MySQL and strong Python, Shell, Java, PHP, and T-SQL programming skills. He must also be technologically adept, demonstrating strong computer skills. The candidate must additionally be capable of developing databases using SSIS packages, T-SQL, MSSQL, and MySQL scripts.The candidate will also have an ability to design, build, and maintain the business\u2019s ETL pipeline and data warehouse. The candidate will also demonstrate expertise in data modeling and query performance tuning on SQL Server, MySQL, Redshift, Postgres or similar platforms.Base Qualifications3+ years of demonstrated data engineering experience or development experience3+ years of experience with Big Data Technologies like Hadoop or Hive3+ years' experience in custom ETL design, implementation and maintenanceProficient designing and implementing data models and data integrationExperienced deploying Azure SQL Database, Azure Data Factory and well-acquainted with other Azure services including Azure Data Lake and Azure MLExperience implementing REST API calls and authenticationExperienced working with agile project management methodologiesPreferred QualificationsAt SDK we believe \u201cperfection\u201d is a process. We hire for fit and invest in training, so our people continue to be the best for themselves, SDK, and SDK's customers.EducationComputer Science Degree/DiplomaMicrosoft Certified: Azure Data Engineer Associate:Job Types: Looking for full-time employees only. No Contractors. Must be eligible to work in Canada.Job Types: Full-time, ContractExperience:Data Engineering: 5 years (Preferred)Location:Toronto (Required)Work remotely:Temporarily due to COVID-19", "job_collect_date": "2020-08-16T16:17:46.000Z"}, "22": {"job_id": "4326ff472bad16d1", "job_title": "Data Engineer Mgr - Platforms", "job_employer": "Rogers Communications", "job_location": "Brampton, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=4326ff472bad16d1&fccid=b5583ca8df6a68c3&vjs=3", "job_description": "At Rogers, we connect Canadians to a world of possibilities and the memorable moments that matter most in their lives. Every day we wake up with one purpose in mind. To bring loved ones together from across the globe. To connect people to each other and the world around them. To help an entrepreneur realize their dream. A sports fan celebrate a special moment.\n\nBecause we believe connections unite us, possibilities fuels us, and moments define us.\n\nRogers is seeking a Database and Platform Administrator as part of the Enterprise Data and Analytics Team who will be operating in a senior role managing a team of Big Data, Cloud and other Database Administrators. The Database and Platform Administrator will serve as a key member of the Data Engineering team and will be responsible for the design, deployment, and operations of all of the database infrastructure in Development, Pre-Production and Production environments.\n\nResponsibilities:\nResponsible for Management of all database infrastructure including: transactional databases, data warehouses, and cloud infrastructure as well as data replication and extractions from these database infrastructures.\nDesign of database structures and replication, failover, backup, and extraction functions.\nProduct evaluation and selection for all database infrastructure requirements.\nCreation of operations documentation and scripts for all database infrastructures.\nDeployment and tuning of monitoring for critical database availability and performance thresholds and alarms.\nEnsure database servers are backed up and meet the Enterprise Recovery Time Objectives\nTuning of all databases in Development, Pre-Production, and Production environments.\nCreate performance baseline metrics and monitor performance problems.\nApplication database setup in cloud environment. Setup/configuration of database backup/recovery, database monitoring, database performance tuning, Security, problem diagnosis\nDatabase design and DDL deployments to cloud environment and working closely with application teams\nEvaluating and testing new database features, documenting and presenting to team\n\nWhat You Need to Have:\n8+ years of experience as an Oracle DBA supporting prod/QA/Dev environment\nExperience migrating the On-Prem Databases (Oracle/Big Data) to Cloud (Azure and AWS)\nExperience of all key DBA roles in Cloud (backup, recovery, alert monitoring, performance monitoring, security administration, performance tuning, other database maintenance \u2013 statistics, re-indexing, etc.\nExperience in automation and scheduling of database backups and other database maintenance\nKnowledge of availability zones, HA, DR solutions in the Cloud will be nice to have\nExperience with Cloud portals to create and manage databases and resources\n\nSchedule: Full time\nShift: Day\nLength of Contract: No Selection\nWork Location: 8200 Dixie Road (101), Brampton, ON\nTravel Requirements: Up to 25%\nPosting Category/Function: Technology & Information Technology\nRequisition ID: 201652\n\nTogether, we'll make more possible, and these six shared values guide and define our work:\n\nOur people are at the heart of our success\nOur customers come first. They inspire everything we do\nWe do what\u2019s right, each and every day\nWe believe in the power of new ideas\nWe work as one team, with one vision\nWe give back to our communities and protect our environment\n\nWhat makes us different makes us stronger. Rogers has a strong commitment to diversity and inclusion. Everyone who applies for a job will be considered. We recognize the business value in creating a workplace where each team member has the tools to reach their full potential. At Rogers, we value the insights and innovation that diverse teams bring to work. We work with our candidates with disabilities throughout the recruitment process to ensure that they have what they need to be at their best. Please reach out to our recruiters and hiring managers to begin a conversation about how we can ensure that you deliver your best work. You matter to us! For any questions, please visit the Rogers FAQ.\n\nPosting Notes: Information Technology & Engineering", "job_collect_date": "2020-08-16T16:17:46.000Z"}, "23": {"job_id": "980a5785f8688836", "job_title": "Staff Data Engineer (Remote)", "job_employer": "FreshBooks", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=980a5785f8688836&fccid=785af18d53962443&vjs=3", "job_description": "FreshBooks has an ambitious vision. We launched in 2003 but we're just getting started and there's a lot left to do. We're a high-performing team working towards a common goal: building an elite online accounting application to help small businesses better handle their finances. Known for extraordinary product and customer service experiences and based in Toronto, Canada, FreshBooks serves paying customers in over 120 countries.\n\nThe Opportunity - Staff Data Engineer (Remote)\n\nWe're looking for a strong technical leader who has gravitas to influence and has deep knowledge of building and designing scalable data platforms. An effective communicator, a mentor who can think on their feet and be able to come up with practical, simple solutions to complex problems. As a Staff Data Engineer, you are someone who can redefine data engineering capability and constantly push boundaries. You will co-own FreshBooks' data platform to ensure scalability and elasticity.\n\nIf this appeals to you, please come and chat with us to learn more about how you can become a part of the intrinsically motivated teams of data engineers!\n\nWhat you'll do in your first twelve months at FreshBooks:\n\nCollaborate with analytics, engineering and business teams working on the product for our customers.\nBe the technical lead and advocate best practices, contribution to roadmaps, developing the domain breadth and/or depth, stewardship.\nContribute towards defining technical vision and challenge across different levels of business including Senior Executives and Senior Leadership group.\nHelp in growing the technical expertise of the team as they continue to work and touch services outside their realm.\nDemonstrate experience coaching and mentoring data engineering teams, and growing the overall technical maturity of our Data organization.\nEvolve our data platform architecture to scale with our rapidly growing customer community.\nDevelop a deep understanding of multiple parts of our stack as well as the processes and technologies relevant to our tech space.\nBe capable of supporting the data platform from end to end.\nRaise the bar for our entire data engineering team through best practices, automation, documentation, and hiring.\nLevel up our operational excellence, and drive our team to maintain it, so that common regressions are root caused.\nSupport operational excellence and make measurable improvements to our support processes.\n\nWhat you bring:\n\nAt least 10 years of combined experience in software and data engineering (Agile or Lean environment)\nStrong programming skills in Python or similar language with deep refactoring skills.\nFive or more years with cloud based data platforms and technologies such as AWS or Google Cloud Technologies, Elastic MapReduce, S3, EC2, and Kinesis.\nExperience in architecting and building large-scale batch or stream processing data pipelines.\nExperience architecting scalable ETLs with inputs from multiple data sources.\nExperience with data warehousing technologies such as Amazon RedShift or Google BigQuery.\nExperience in Airflow or other data infrastructure job scheduling software.\nExperience writing complex SQL queries.\nAbility to troubleshoot and determine root causes of issues.\nExperience working with large codebases, writing robust and testable code\nExperience ensuring security and governance of data.\nA passion for keeping up to date in current technologies and future trends\nA deep understanding of test-driven (and behavioural test driven) development, and of building substantially complete test code, and not just for the happy path\nFamiliarity with continuous integration (or better, continuous delivery) and automated build pipelines.\nThe ability to balance desire to ship code quickly to our customers with the responsibility of making good technical decisions.\nA long-standing habit of continuous learning, and of applying new technologies, architectures, and methodologies to improve the code and Engineering organization.\n\nWhat you might bring:\n\nWe're looking for a variety of talented technical leadership and know that a mix of skills and experience is useful. Even having a couple of the skills from the list below would be a strong asset.\n\nExpertise in the core areas of business of FreshBooks (accounting, payments, small business solutions)\nA background in DevOps and service ownership, and a clear understanding of bounded contexts and how they map onto microservices.\nStrong pair programming both as a mechanism for producing better code, and for teaching skills.\nExperience with Docker, Kubernetes, Ansible, Terraform, or other similar tools.\nExperience with Redis / Elasticsearch & RabbitMQ.\n\nWhy Join Us\n\nWe're a motivated bunch, with our eye's laser-focused on shipping extraordinary experiences to businesses. You will be surrounded by hardworking team members who share a common vision for what an amazing software company could be and have the opportunity to help build an elite one, right here in downtown Toronto.\n\nApply Now\n\nHave we got your attention? Submit your application today and a member of our recruitment team will be in touch with you shortly!\n\nFreshBooks is an equal opportunity employer. We do not discriminate based on gender, religion, race, mental disability, sexual orientation, age, or any other status. All applicants are considered based on their qualifications and merits. At FreshBooks, we inspire an environment of mutual respect and we believe diversity and inclusion are crucial to our success.\n\nHere at FreshBooks, we welcome and encourage applications from people with disabilities. Should you require any accommodations during the recruitment process, please advise your recruiter on how we can meet your needs to ensure a fair and equitable selection process in a confidential manner.", "job_collect_date": "2020-08-16T16:17:47.000Z"}, "24": {"job_id": "d40c380c285457aa", "job_title": "Data Engineer", "job_employer": "Gensquared", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=d40c380c285457aa&fccid=ae47c2169a389ac0&vjs=3", "job_description": "Gensquared prides itself on being at the forefront of innovation in the Big Data space.\n\nFounded in 2010, Gensquared provides thought leadership and implementation excellence within the ever-growing data and analytics world. The volume of data is expected to grow to 5x what it is today, and Gensquared helps its customers to be well-positioned for success to use this data to their advantage. Companies that use data have been proven to outperform their peers by as much as 85% (McKinsey Group 2017).\n\nWe take pride in having some of the most highly trained and experienced consultants in the industry which translates into optimal value for our clients. We were one of the first companies to provide analytics and data as a service, via the cloud, as early as 2010. We strive to make sure our customers are well-positioned with the best technologies/tools in the industry, constantly evaluating new and existing technology partnerships. Some of the more prominent companies we have partnered with include; Snowflake, DataRobot, MicroStrategy, Informatica, Amazon AWS & Microsoft. We continue to invest in our most valuable resource, our people. We do this through extensive training both on the job and through various educational programs.\n\nOur consultants are family and valuing each other is one of our most important core values. We are looking to grow our family as we search for a Data Integration Consultant.\n\nThis individual will be responsible for attaining the following goals:\n\nAttaining a minimum of 1 new accreditation/certification per year\nSpending 80% or more of their time on billable work\nCompleting 90% or more of their agile delivery tasks on time\nDemonstrating competency in 1 new relevant technology every year\n\n\nDay to day activities for this role would include:\n\nConduct relevant customer interviews to determine key business requirements and objectives\nBuild appropriate analytical data models based on outcomes of user interviews\nAnalyze and profile data systems to build source to target data mappings\nBuild required ETL to populate target designed data warehouse and/or data lake\nReview ETL performance and conducts performance tuning as required on mappings/workflows or SQL\nAdministration and support of data integration infrastructure\n2nd level on-call support of ETL services as required\n\nDesired Skills and Experience\n\nUniversity/College degree in Computer Science, Mathematics, Data Science and/or Relevant Degree\n5+ years hands-on development, configuration, scripting and administration experience with Data Integration platforms. (i.e. Informatica, Talend, DataStage, SSIS)\nBI Experience (MicroStrategy, Looker, Tableau, PowerBI) considered a nice to have\nExtensive theoretical and practical knowledge of data warehousing principles/concepts and practical development experience in all areas of the data warehousing life cycle\nExperience with Data Management, ETL, Cloud Data (AWS), Data Integration\nKnowledge of OLAP-related principles and concepts\nStrong grasp of data modeling techniques and concepts (Normalized/Denormalized, Conceptual/Logical/Physical, Star, Snowflake, Data Vault)\nStrong knowledge and experience with relational databases such as Snowflake, SQL Server, Oracle (Advanced knowledge of reading and writing SQL, Performance analysis and tuning)\nKnowledge and experience with key Big Data technologies (Hive, Presto, Spark, Kafka, NoSQL databases, Semi-structured data access patterns (Json, Parquet, XML, etc.))\nStrong Python scripting skills\nExcellent communication skills\nGreat problem-solving skills\nLeadership and good client management skills", "job_collect_date": "2020-08-16T16:17:48.000Z"}, "25": {"job_id": "55e49e6c1c9af157", "job_title": "Senior Data Engineer", "job_employer": "RBC", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=55e49e6c1c9af157&fccid=537b899e30af3338&vjs=3", "job_description": "What is the Opportunity?\n\nThe DNA (Data & Analytics) group is responsible for enabling RBC to become a data-driven organization. As part of this mission, DNA works with various lines of business (Personal & Commercial Banking, Wealth Management, Insurance, Capital Markets, etc\u2026) to create and build data-driven solutions to serve our clients better. You will be part of a strong team of developers building out our reusable, core data services to deliver value to business partners through data, insights and AI.\nThe DNA Data Services team will build data-driven products and services/API\u2019s, tackle challenging and interesting data-related problems using RBC's massive internal datasets (client relationships, user behavior across channels, transactions, etc\u2026.) and strategically partner with the business to enable client interactions to be informed by Artificial Intelligence (AI).\n\nWhat will you do?\n\n Build large-scale real-time data pipelines using the latest technologies.\n Apply design thinking and an agile mindset in working with other engineers, data scientists and business stakeholders to continuously experiment, iterate and deliver on new initiatives.\n Leverage best practices in continuous integration and delivery.\n Help drive transformation by continuously looking for ways to automate existing processes and testing and optimize data quality.\n Explore new capabilities and technologies to drive innovation\n\nWhat do you need to succeed?\n\nMust-have\n Bachelor\u2019s degree in computer science, software engineering, or equivalent\n 5+ years development experience in Java\n 2+ years\u2019 experience with streaming or messaging technologies (Kafka, etc\u2026)\n 2+ years\u2019 experience in Big Data environments (Spark, Hadoop, etc\u2026)\n Experience building operational REST APIs\n Strong foundational knowledge of relational databases (MySQL, SQL Server, etc\u2026) and NoSQL stores (Elasticsearch, Neo4j, MongoDB, etc\u2026)\n\nNice-to-have\nKnowledge of public cloud environments (AWS, Azure)\n A passion for simplifying and automating work, making things better, continuous learning, solving open-ended problems, improving efficiency and helping others\n\nWhat\u2019s in it for you?\n\n We thrive on the challenge to be our best, progressive thinking to keep growing, and working together to deliver trusted advice to help our clients thrive and communities prosper. We care about each other, reaching our potential, making a difference to our communities, and achieving success that is mutual.\nA comprehensive Total Rewards Program including bonuses and flexible benefits, competitive compensation, commissions, and stock where applicable\nLeaders who support your development through coaching and managing opportunities\nAbility to make a difference and lasting impact\nWork in a dynamic, collaborative, progressive, and high-performing team\nA world-class training program in financial services\nFlexible work/life balance options\nOpportunities to do challenging work\nOpportunities to take on progressively greater accountabilities\nOpportunities to building close relationships with clients\nAccess to a variety of job opportunities across business and geographies", "job_collect_date": "2020-08-16T16:17:48.000Z"}, "26": {"job_id": "5929006533b711f6", "job_title": "Data Engineer- Permanent, downtown Toronto", "job_employer": "IT Connex", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=5929006533b711f6&fccid=580f6b9a5ada66c2&vjs=3", "job_description": "Must have:\nStrong Spark, Kafka and are familiar with Flink/Druid/Ignite/Presto/Athena\n\nYou are proficient in Java/Scala/Python/Spark", "job_collect_date": "2020-08-16T16:17:50.000Z"}, "27": {"job_id": "fa77394a330e96c4", "job_title": "Data Engineer (Cloud)", "job_employer": "StackPros Inc.", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/company/StackPros-Inc./jobs/Data-Engineer-fa77394a330e96c4?fccid=efee8d7cab418992&vjs=3", "job_description": "StackPros Inc is seeking a candidate for a full-time role within our Data Systems Team in Toronto, Ontario.The *Cloud Data Engineer* will play a key role at StackPros, required to help create and maintain industry-leading quality and efficiency of service and software delivery.StackPros will rely on the Data Engineer to support the Data Systems team, in both data engineering and data science-related workflows. The Data Engineer will be expected to meet and exceed StackPros' quality standards, while helping the organization rapidly expand complex Machine Learning and related applications.*Key Responsibilities*:*Data Engineering-Specific Responsibilities*Participate in continuous delivery pipeline to fully automate deployment of the highly available cloud platform that supports multiple projectsDesign and develop ETL workflows and datasets to be used in data visualization toolsWrite complex SQL queries with multiple joins to automate and manipulate data extractsPerform end to end Data Validation to maintain accuracy of data setsBuild tools for deployment, monitoring and operationsTroubleshoot and resolve issues in the development, test and production environmentsDevelop re-useable processes that can be leveraged and standardized for multiple instancesPrepare technical specifications and documentation for projectsStay up-to-date on relevant technologies, plug into user groups, understand trends and opportunities to ensure we are using the best possible techniques and toolsUnderstand, implement, and automate security controls, governance processes, and compliance validationDesign, manage, and maintain tools to automate operational processes*Data Science-Specific Responsibilities*Perform exploratory data analysis to identify patterns from historical data, generate and test hypotheses, and provide product owners with actionable insightsDesign experiments for product initiatives and perform statistical analysis of the results with recommendations for next steps and future experimentsCreate and design dashboards by using different data visualization tools to present reports and insights, and support business decision makingHelp the StackPros Data Systems team adopt and evolve Predictive Modeling, Machine Learning and Deep Learning processes to deliver to clients in the future*Company-Wide Responsibilities: *Maintain and exceed client satisfaction with StackPros Inc.\u2019s deliverables, day-to-day work and overall value as a partnerCultivate opportunities for company growth, always seek areas where StackPros Inc.\u2019s role could be expandedAdapt to ever-changing client needs and expectationsMaintain dedication toward achieving excellence in StackPros Inc.\u2019s delivery against client needs, and overall success as an organizationBe an enthusiastic, positive and generally awesome team mate, mentor & constantly curious learner*Qualifications: *4+ years experience in Data EngineeringUnderstanding of digital ecosystems including online data collection, cloud systems and analytics tools (Google Stack, Facebook, AWS, Salesforce, Adobe Suite etc.) a strong assetStrong technical understanding of a range of marketing concepts such as cookie-based data collection, setting and leveraging audience segments, attribution modelling, AB/N & multivariateExcellent written & verbal communication skills are essential; candidate should be comfortable presenting and participating in group discussions of concepts with internal and external stakeholdersCandidate must exhibit an analytical, detail-oriented approach to problem solvingExperience with Jira / Atlassian project management tools is an asset*WHAT\u2019S IN IT FOR YOU?*100% employer-paid benefits packageMonthly yoga and meditation classes onsiteRegular Lunch and Learns from your Team MatesStanding desksEntertainment and Games area, including pool tableFully-loaded kitchen: snacks/fruit/drinks/beerFun Employee Events and ActivitiesParticipation in Community EngagementJob Type: Full-time", "job_collect_date": "2020-08-16T16:17:51.000Z"}, "28": {"job_id": "24716d17b181ff38", "job_title": "Intermediate Data Engineer", "job_employer": "Hitachi Solutions", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=24716d17b181ff38&fccid=28f79c18789111e8&vjs=3", "job_description": "Company Description\n\nThe Company\nHitachi Solutions is an impact-driven global leader in consulting dedicated to delivering competitive, end-to-end solutions based on the Microsoft Cloud. Our deeply connected teams are unified by our values and our commitment to helping clients succeed and compete with the largest global enterprises. We are a division of the 38th largest company in the world and carry the strength of a vast network of interconnected Hitachi companies all while remaining nimble, agile, and ready to pivot at a moment\u2019s notice.\nHitachi Solutions started as three founding partners and transformed into nearly 2,000 consultants, developers, and support personnel all around the globe. With 36 Microsoft Partner of the Year awards, Hitachi Solutions has established itself as a leading partner in the ever-growing landscape of technology consulting.\nThe Culture\nOur team is a mix of curious, fun, and get it done. We celebrate collaborative thought leadership and individual talents so that our ideas are translated into real-world results.\nEach day, our team members show up to work as a blank slate with the sole purpose of coloring their canvases with knowledge. We are a complex group of resilient learners and encourage one another to go beyond the limits of conventional expectations. As a member of the dynamic Hitachi Solutions team, you will be challenged, pushed, and unconditionally supported in your efforts to drive the business forward.\n** THIS ROLE IS OPEN TO ANY CANDIDATES FROM WITHIN CANADA**\n\nJob Description\n\nAs an Analytics Data Engineer for Hitachi\u2019s Azure Cloud Enablement Team you will be responsible to deliver high quality modern data solutions while being part of a dynamic and fast-growing team consisting of endless opportunities.\nThe successful candidate will be a self-motivated, passionate individual who strives to be the best at what he/she does and creates a trail of ecstatic Customers for life. This person will love to learn, contribute to the team and wants to be part of something great.\nKnowledge and Experience\nHands-on experience with the Azure Data Platform (Data Factory, Data Lake, Data Warehouse, Blob Storage, SQL DB, Analysis Services)\nData quality (profiling, cleansing, enriching)\nData Modeling \u2013 including design from conceptual to logical to physical data models\nConsidered to be an expert in T-SQL\nHands-on experience with MPP database technologies such as Azure SQL DW, Teradata Netezza, etc.\nExperience with multiple components listed, required:\nPower BI including DAX\nDatabase migration from legacy systems to new solutions\nDevOps\nInterpreted languages (i.e. python, C-sharp, Java, Scala, etc.)\nDatabricks\nLogicApps\nPowerApps\nHDInsight\nD365FO / CE experience as it pertains to data extraction\nKnowledge of Cap Theorum and Distributed Database Management Systems, iis considered an asset.\nOpportunity for a career path into a Data Scientist role if desired\n\nQualifications\n\nRequired skills / qualifications\nProven ability to engage customers to understand customer challenges and needs to develop technical solutions\n3+ years of hands on experience working with the Azure Platform and its relevant components\nProven experience of architecting Azure services into a solution platform on Microsoft Azure for Analytics\nMinimum 5 years hands on development experience with ELT/ETL using MS SQL Server, Oracle or similar RDBMS Platform\nFamiliarity with data visualization tools (e.g. PowerBI, Tableau etc.)\nExperience or desire to coach, mentor and provide leadership to team members\nPost-secondary degree/diploma in Business, Computer Science or a related discipline;\nStrong communication skills, both written and verbal\nPrepared for domestic and US travel as required\nPreferred considered an asset, NOT required:\nProject management experience\nDatabricks and Spark SQL\nPrevious Consulting experience\nAdditional Information\n\nOpportunity Benefits:\nMedical and Dental Benefit Package (including Long Term and Short Term Disability)Base salary plus targeted bonus package\nThis position can be based anywhere in Canada, though travel might be required.", "job_collect_date": "2020-08-16T16:17:51.000Z"}, "29": {"job_id": "a32b7c599156a218", "job_title": "BHJOB15656_15145 - Data Engineer w/ DevOps - Azure Cloud", "job_employer": "Myticas Consulting", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Bt274jo0r_e0cCCqhskoQvohXG3bfDAo8rIdE-Msu3yuVfyF2oiDFKHFOU-vxVulAfe8kPL7riFwoXa1bsfN-szjxO_8qUQWbPsJOnTg0EkutIoK2S4NNLL24JQh9e4DsE9hv907k2_zDbvj0a8WAj5oPSV5J0Yhgrx-GCyclKc1wKxOx6QDqBiLqHy-gwJa-QMxALgtwXTILgzo_AuOrx7IjJuJhtVFR3hH1p7VFZdYrkNsJ5ctIBRo94OMRTOnrWnTAWh-mGR8Uvpq4O3NUARGG8G3MQfancBXU6pzdB5FdtpF4X7UxT9-FNNJg_ciNzmTpj5hRBQLdELOj7zGjwWeXiuaeCNZdcrzlCwR5y9ZFHKOaxXAS9GPe2DDTwKIkGeNXlYj-UVr-xQ-y04oh9SS3Bz2n9DuS2CdREWR-gXjb_Ulhitt6kKCLlMzOqWEt4SiPP0MTWPQ==&p=13&fvj=0&vjs=3", "job_description": "The growing team here at Myticas is looking for an experienced Data Engineer who would be interested in a remote contract opportunity within the Ottawa, Toronto and Montreal regions.\n\nAccountabilities:\n\nDefining and reviewing security design requirements for cloud infrastructure and application components.\nEvaluating architecture patterns from security perspective.\nBuilding and implementing security controls to enable enforcement of compliance with Cloud Control Objectives, using custom Azure policies and integrated controls in DevOps processes\n\nRequirements:\n\nStrong Data Engineer w/ DevOps expertise + Azure Cloud Experience\nMust know how to code and stand up scripts.\nExperience with Data Digestions\nExperience writing scripts to automate (infrastructure)\nARM Templating Expertise\nAzure Synapse Expertise\nSupport developing automated DevOps processes and procedures for the following Azure components:\nAzure Synapse (Azure DW) & Studio (private preview)\nAzure Data Catalog Gen 2 (Babylon \u2013 private preview)\nAzure Data Lake Storage Gen 2\nAzure ML\nML Flow\nAzure SQL Analysis Service\nAzure Databricks\nADF data pipelines for data loading to AzSQL/Synapse\nADF data pipelines for connecting to on-prem data sources for data\n\nCandidates looking to apply for this role are to send us an updated version of their resume in confidence. Our team will be sure to review all applicants and follow up accordingly at the conclusion of the review process.\n\nJob is also known as: Data Engineer, DevOps Engineer, Azure Cloud Engineer, Azure Engineer, Cloud Engineer\n\nINDMY", "job_collect_date": "2020-08-16T16:17:52.000Z"}, "30": {"job_id": "54e0fcc38385e3a2", "job_title": "Data Engineer/Integration Consultant", "job_employer": "Copperstone Connect", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Ceb7pqR8iGmLuXIZQklF6pqed3xddONNM6Jjumj-ndyGK0kULQUmQ2REqfJ5KAzFVeK_es1WVg0s1sLgKW09UsvVRp0-dSizFQr1oNyK_VIqdjla00VU9GNVvu2x4v9p3hsL7NeF-6hBksbAaSgKazsaYTODGMKQQeVuYcxoOuxvFKuD0HRLD9ybgbcXTn1LPgjmJaFROSikUe7kl9Tc081jZAD-jovTgRv_fWIw1UGwfmFPqTvOFkRsyyLoZXNKB67JjUWpga9Y0on1RmX2D-w1eNjLYDOTmz9k6Q7AqruIWoevnvhXMqr_n_Q9cqNDTgTVScPtD98ib2L0kTdabcABjqN-i_IlsDMsUzSMqQOHMozIgA42k-wjmiMkbxa8O8fUwvZBwS3ABFJcT_EEl-SxO8mTId_3hhcxmZ-5kUt3jzBBFaJoI1Qdirv_2s56E51EOcdh22yRm0iMkn26T4v4_qUdt01QEsuoJLpQuh_Z1VNewLVMO7mvX2Lsut9bfTtalNMPh3lQjHdIkCOTE5Sfg2paU-YkCbcF75fr6UDQGVGALgS8C4DzKgqYq88xwpw2dEgoVtV4lQYGGMmx6uhN8-RR_EjUPfSZ5698dRP5_pld40k_fdc8MtQtPRAwuty2IM0YaB7wW2ilJizi7l6qbF6wNOOI9nr1TDq2fKgA==&p=14&fvj=0&vjs=3", "job_description": "Our client, a boutique-size consulting firm, prides itself on being at the forefront of innovation in the Big Data space. Founded in 2010, they provide thought leadership and implementation excellence within the ever-growing data and analytics world.\nThey take pride in having some of the most highly trained and experienced consultants in the industry which translates into optimal value for their clients. They were one of the first companies to provide analytics and data as a service, via the cloud, as early as 2010. They strive to make sure their customers are well-positioned with the best technologies/tools in the industry, constantly evaluating new and existing technology partnerships. Some of the more prominent companies they have partnered with include; Snowflake, DataRobot, MicroStrategy, Informatica, Amazon AWS & Microsoft. They continue to invest in their most valuable resource, their people. They do this through extensive training both on the job and through various educational programs.\nDue to growth, they are seeking to hire an Intermediate - Senior Integration Consultant.\n\nDesired Skills and Experience\n: University/College degree in Computer Science, Mathematics, Data Science and/or Relevant Degree\n: 5+ years hands-on development, configuration, scripting and administration experience with Data Integration platforms. (i.e. Informatica, Talend, DataStage, SSIS)\n: BI Experience (MicroStrategy, Looker, Tableau, PowerBI) considered a nice to have\n: Extensive theoretical and practical knowledge of data warehousing principles/concepts and practical development experience in all areas of the data warehousing life cycle\n: Experience with Data Management, ETL, Cloud Data (AWS), Data Integration\n: Knowledge of OLAP-related principles and concepts\n: Strong grasp of data modeling techniques and concepts (Normalized/Denormalized, Conceptual/Logical/Physical, Star, Snowflake, Data Vault)\n: Strong knowledge and experience with relational databases such as Snowflake, SQL Server, Oracle (Advanced knowledge of reading and writing SQL, Performance analysis and tuning)\n: Knowledge and experience with key Big Data technologies (Hive, Presto, Spark, Kafka, NoSQL databases, Semi-structured data access patterns (Json, Parquet, XML, etc.))\n: Strong Python scripting skills\n: Excellent communication skills\n: Great problem-solving skills\n: Leadership and good client management skills\n\nDay to Day Activities Would Include\n: Conduct relevant customer interviews to determine key business requirements and objectives\n: Build appropriate analytical data models based on outcomes of user interviews\n: Analyze and profile data systems to build source to target data mappings\n: Review ETL performance and conducts performance tuning as required on mappings/workflows or SQL\n: Administration and support of data integration infrastructure\n: 2nd level on-call support of ETL services as required\n\nYou will be responsible for attaining the following goals:\n: Attaining a minimum of 1 new accreditation/certification per year\n: Spending 80% or more of their time on billable work\n: Completing 90% or more of their agile delivery tasks on time\n: Demonstrating competency in 1 new relevant technology every year", "job_collect_date": "2020-08-16T16:17:53.000Z"}, "31": {"job_id": "997673b8effa647f", "job_title": "Data Engineer (Intermediate 4-6 years) - Contract - Remote during COVID", "job_employer": "CorGTA", "job_location": "North York, ON", "job_link": "https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Dkh866153otbRJ8nVMuEXnXPd315JS377__3VyD6HnNQkmcaOHMemmy5pLrNV6j0WaVWEWWabnp4PP6-2u77DLfT_FCIGvCW8CQmQW140Z7EVU32Jnwl42FALsxnbc4n5M6FOTfoW2OoTMOcGG21HZGOoXL7fWT8eaT67VlcClh4sHQcJsHCX_LRRvgH2PH-pPOgJzcdM0GKSzbYUlcSxRjGJaAv1JbtvQ8SYHY4MJadfj6puz-rKQ0-nGe_FLo8Oem4EJsPKSz2C9RqLKwGqJEq7ablCi84knGpsY-KGsZmA2qqrb8-b8uEtwEbOQ52vq7gFGSorS63Npqbv3jyPphKIdcUMjDMUk3eh2DGuXWaxAZT-UeEdmVSCEpJDr9lwDRtKyPJQ7MrZa8arb6lrDUwmfSheDv4xON_QijLpXIva8CRCPZsehq0T_OWkC2FP2kDWpirO252HHcI6cSk_W2V2dOEhzte7GC5CJ68GB4jCiDDJd3HyU&p=0&fvj=1&vjs=3", "job_description": "Role: Data Engineer (Intermediate 4-6 years)Structure: Contract (6 months initially)Location: North York, ONHours: Monday - Friday 40 hours (remote during COVID-19)Pay: $50.00 p/h inc.The role: We are looking for Intermediate Data Engineers who have roughly between 4-6 years experience who ideally has experience moving data from Google cloud over to Big query.However, if you are someone who has good experience with ETL and Data Pipeline Design & Data Validation experience and can script in Python and create strong SQL Queries then this will also be a potential match for this role.Ideal Qualifications to be successful in this role: - Degree in Computer Science, Engineering, Information Systems (or equivalent combination of skill and experience)- 4-6 years of experience working with data architecture projects- Excellent SQL coding and experience with a broad array of development tools and platforms including exposure to a big data environment tools/languages, such as SQL, R, SAS, Python, etc.- Experience creating relational database design and data models- Experience with various analytical data platforms and technologies- Experience with using REST API, Cloud (GCP Ideally)- Experience in custom or structured data integration design, implementation, and maintenance- Experience with business intelligence tools such as Qlikview, Tableau and Microstrategy (Nice to have for Data Visualizations)- Previous experience working in retail or eCommerce is highly preferred- Ability to curate data to tell stories and provide business insightsPlease apply with an updated resume and ensure the required skills you are able to speak to for this position are included.For more roles like this please go to www.corgta.com/find-a-job/Job Type: Full-timeSalary: $45.00-$50.00 per hour", "job_collect_date": "2020-08-16T16:17:55.000Z"}, "32": {"job_id": "05f7bf2a7458c7fc", "job_title": "Data Engineer", "job_employer": "TES - The Employment Solution", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DQQnUBuQBSuyaIQhpC59TW7hrTbBg8v-nGtzzV8dunbQHh9VTp-k89gJ8q3B9UEXLHcYbAG67d8jpOK4E3ok8olcOhfyyiChWMI3BPvUFdra3ljwgLS_vFsJXlW6jHt2NpQMzd2p9F6BXGm9ND3feUN42yGy4tVXCEXqAGgdFldxFSpEwQ46aMQiEwWkEt4yY8A_va9dqU40pjaAtQeLNN2H32ABkmCFUGs5qFl1ozZz4lY_voY7j5TrB4ZAF28vIAJ6yXkiDCOsGYVawaxwzrk1WwbusWOEwszGGzy-UomoRBT_e9xhYPgg9hVdNpv5jwUyHSMhmLzGE5AcDe6qB4ywzUUQ_AAEEOhiN6FTKH-EWXekJ_ckQMNJp2EJah5Wk69eluVS83KBA8JznTF6V71J1NIuFGJYeiPtAHVvY61T4G6hXedEQk4VV608RQeqMKcAkGKuGvRTyaFPrrpBFGUhHwCXuASqza8qp_ELXi5XCW0zTFrLFriTL8krtvvDKHNw7GDmqKWV-YuLpYq3KHmITfYBWJYyv3FmKDVI7NdcWoSNVug5jOzlq6-sKtIu9x0045xJN0I40-GbC39Oc9JLQe9fmuiQlvWL1EX2ED3DQ_7nHf14OdJ4keirFpfOheBY4X2eUoPLb0wGNCYsagqTxOhyoIv2ifEEWbGj10dnh8b7ObPjlQ2-SGlUiMH3HzUR1IcOLyVr6_gBj2nppSa6RqnuiI7xP7CyKGsbmLlev5cu7jiKO8Qzkx4ID-GNOHt5I-Ujde6RcWE2IRy9Gd6W2FEQeRGplpp3Ht6vaJDwXCQvo6QeawlxlvZAjhJj2Ml6paTO_u2M0KAXme94IF0ouzj1s1oNc=&p=1&fvj=0&vjs=3", "job_description": "Job Title- Data Engineer\nLocation- Downtown, Toronto\nType - Full Time Permanent\nSalary - Negotiable + Benefits\n\n\n\nFocus on data architecture, best practices, reliability, security, and complianceImprove and extend ETL, data processing, and analytics processesFacility with PowerBI, including creating dashboards and data sourcesDeveloping high complexity, fast performing SELECT queries.Developing T-SQL procedures, functions, triggers, jobs, scripts, etc.Development of Advanced T-SQL such as temporal tables, PIVOTs, recursive table expressions and more.Modeling and implementing Data Mart solution for Power BI analytics\nManaging indexes, statistics, query plans alerts, database activity, and overall performance activity.In-depth experience working with relational databases, such as Microsoft SQL Server or PostgreSQLEnthusiasm for applying good data design, testing, documentation, and support practicesExperience building and optimizing data pipelines, architectures, and data setsKnowledge of message queueing, stream processing, and data stores/warehousesWorking knowledge of AWS products related to data engineeringBachelor's degree in Computer Science, Software Engineering or an equivalent\nExcellent communication skills - both written and verbal; ability to speak in Spanish is a bonus\n\nTo apply please send an email to sheetalk@tes.net", "job_collect_date": "2020-08-16T16:17:56.000Z"}, "33": {"job_id": "24716d17b181ff38", "job_title": "Intermediate Data Engineer", "job_employer": "Hitachi Solutions", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=24716d17b181ff38&fccid=28f79c18789111e8&vjs=3", "job_description": "Company Description\n\nThe Company\nHitachi Solutions is an impact-driven global leader in consulting dedicated to delivering competitive, end-to-end solutions based on the Microsoft Cloud. Our deeply connected teams are unified by our values and our commitment to helping clients succeed and compete with the largest global enterprises. We are a division of the 38th largest company in the world and carry the strength of a vast network of interconnected Hitachi companies all while remaining nimble, agile, and ready to pivot at a moment\u2019s notice.\nHitachi Solutions started as three founding partners and transformed into nearly 2,000 consultants, developers, and support personnel all around the globe. With 36 Microsoft Partner of the Year awards, Hitachi Solutions has established itself as a leading partner in the ever-growing landscape of technology consulting.\nThe Culture\nOur team is a mix of curious, fun, and get it done. We celebrate collaborative thought leadership and individual talents so that our ideas are translated into real-world results.\nEach day, our team members show up to work as a blank slate with the sole purpose of coloring their canvases with knowledge. We are a complex group of resilient learners and encourage one another to go beyond the limits of conventional expectations. As a member of the dynamic Hitachi Solutions team, you will be challenged, pushed, and unconditionally supported in your efforts to drive the business forward.\n** THIS ROLE IS OPEN TO ANY CANDIDATES FROM WITHIN CANADA**\n\nJob Description\n\nAs an Analytics Data Engineer for Hitachi\u2019s Azure Cloud Enablement Team you will be responsible to deliver high quality modern data solutions while being part of a dynamic and fast-growing team consisting of endless opportunities.\nThe successful candidate will be a self-motivated, passionate individual who strives to be the best at what he/she does and creates a trail of ecstatic Customers for life. This person will love to learn, contribute to the team and wants to be part of something great.\nKnowledge and Experience\nHands-on experience with the Azure Data Platform (Data Factory, Data Lake, Data Warehouse, Blob Storage, SQL DB, Analysis Services)\nData quality (profiling, cleansing, enriching)\nData Modeling \u2013 including design from conceptual to logical to physical data models\nConsidered to be an expert in T-SQL\nHands-on experience with MPP database technologies such as Azure SQL DW, Teradata Netezza, etc.\nExperience with multiple components listed, required:\nPower BI including DAX\nDatabase migration from legacy systems to new solutions\nDevOps\nInterpreted languages (i.e. python, C-sharp, Java, Scala, etc.)\nDatabricks\nLogicApps\nPowerApps\nHDInsight\nD365FO / CE experience as it pertains to data extraction\nKnowledge of Cap Theorum and Distributed Database Management Systems, iis considered an asset.\nOpportunity for a career path into a Data Scientist role if desired\n\nQualifications\n\nRequired skills / qualifications\nProven ability to engage customers to understand customer challenges and needs to develop technical solutions\n3+ years of hands on experience working with the Azure Platform and its relevant components\nProven experience of architecting Azure services into a solution platform on Microsoft Azure for Analytics\nMinimum 5 years hands on development experience with ELT/ETL using MS SQL Server, Oracle or similar RDBMS Platform\nFamiliarity with data visualization tools (e.g. PowerBI, Tableau etc.)\nExperience or desire to coach, mentor and provide leadership to team members\nPost-secondary degree/diploma in Business, Computer Science or a related discipline;\nStrong communication skills, both written and verbal\nPrepared for domestic and US travel as required\nPreferred considered an asset, NOT required:\nProject management experience\nDatabricks and Spark SQL\nPrevious Consulting experience\nAdditional Information\n\nOpportunity Benefits:\nMedical and Dental Benefit Package (including Long Term and Short Term Disability)Base salary plus targeted bonus package\nThis position can be based anywhere in Canada, though travel might be required.", "job_collect_date": "2020-08-16T16:17:57.000Z"}, "34": {"job_id": "a677b4cff4d9abf1", "job_title": "Software Engineer, Data", "job_employer": "Fathom Health", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=a677b4cff4d9abf1&fccid=9e88642c8c797c72&vjs=3", "job_description": "We\u2019re on a mission to understand and structure the world\u2019s medical data, starting by making sense of the terabytes of clinician notes contained within the electronic health records of the world\u2019s largest health systems.\n\nWe\u2019re seeking exceptional Data Engineers to work on data products that drive the core of our business-a backend expert able to unify data, and build systems that scale from both an operational and an organizational perspective.\nAs a Data Engineer you will:\nDevelop data infrastructure to ingest, sanitize and normalize a broad range of medical data, such as electronics health records, journals, established medical ontologies, crowd-sourced labelling and other human inputs.\nBuild performant and expressive interfaces to the data\nBuild infrastructure to help us not only scale up data ingest, but large-scale cloud-based machine learning\n\nWe\u2019re looking for teammates who bring:\nExperience building data pipelines from disparate sources\nHands-on experience building and scaling up compute clusters\nExcitement about learning how to build and support machine learning pipelines that scale not just computationally, but in ways that are flexible, iterative, and geared for collaboration.\nA solid understanding of databases and large-scale data processing frameworks like Hadoop or Spark. You\u2019ve not only worked with a variety of technologies, but know how to pick the right tool for the job.\nA unique combination of creative and analytic skills capable of designing a system capable of pulling together, training, and testing dozens of data sources under a unified ontology.\n\nBonus points if you have experience with:\nDeveloping systems to do or support machine learning, including experience working with NLP toolkits like Stanford CoreNLP, OpenNLP, and/or Python\u2019s NLTK.\nExpertise with wrangling healthcare data and/or HIPAA.\nExperience with managing large-scale data labelling and acquisition, through tools such as through Amazon Turk or DeepDive.", "job_collect_date": "2020-08-16T16:17:58.000Z"}, "35": {"job_id": "e8f693d8ceb0d39b", "job_title": "Big Data Engineer", "job_employer": "The Property Registry", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=e8f693d8ceb0d39b&fccid=f9ed656b63c620fc&vjs=3", "job_description": "Teranet is seeking an experienced Big Data Engineer to support its Data Analytics program. In this role, the successful candidate will be responsible for the ongoing monitoring and maintenance of Teranet\u2019s data lake and the in-house developed and third-party software tools used to maintain it. In addition, the Big Data Engineer will work with consumers of the Teranet data lake to develop data views and data feeds according to their business needs and requirements. As part of this effort, the Big Data Engineer will be responsible for publishing data sets and views to the Teranet Data Analytics visualization platform so the data are accessible to downstream consumers. The Big Data Engineer will also collaborate with Teranet\u2019s security team to ensure proper data access controls are in place, data is properly secured, and access activities are auditable.\nKey responsibilities include:\nMaintain and monitor Teranet\u2019s data lake feeds\nManage, maintain and oversee Teranet\u2019s data lake Big Data platform (Cloudera)\nPrepare the data views for downstream data lake consumers (e.g. Data Scientists and Analysts)\nOptimization of Teranet\u2019s Big Data environment, applications and data views\nDesign, build and test data queries for data views and feeds for downstream data lake consumers\nDesign, build and integrate additional data sources into the Teranet data lake\nDesign, build and test analysis/data models to support downstream data lake consumers\nYou are someone who:\nContinuously seeks to improve your knowledge of data analytics technologies and best practices\nStrives to understand business drivers and strategy in order to understand business requirements\nTakes a collaborative approach to assignments and works well with others\nIs a good communicator with strong written and oral communication skills\nClearly documents your work so that it can be readily understood by others\nTakes ownership and accountability for your assignments and responsibilities\nTakes pride in delivering detail-oriented, thoughtful, thorough, and quality results\nKey qualifications:\nBachelor\u2019s degree in Computer, a quantitative field, or equivalent practical experience\n3-4 years working with Hadoop related technologies (Spark, Hive, MapReduce, Sqoop, Impala)\nAdvanced software development experience with Scala, Java, SQL and Python\nFamiliarity with visualization and statistical modeling tools and languages (Tableau, R)", "job_collect_date": "2020-08-16T16:17:58.000Z"}, "36": {"job_id": "fa77394a330e96c4", "job_title": "Data Engineer (Cloud)", "job_employer": "StackPros Inc.", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/company/StackPros-Inc./jobs/Data-Engineer-fa77394a330e96c4?fccid=efee8d7cab418992&vjs=3", "job_description": "StackPros Inc is seeking a candidate for a full-time role within our Data Systems Team in Toronto, Ontario.The *Cloud Data Engineer* will play a key role at StackPros, required to help create and maintain industry-leading quality and efficiency of service and software delivery.StackPros will rely on the Data Engineer to support the Data Systems team, in both data engineering and data science-related workflows. The Data Engineer will be expected to meet and exceed StackPros' quality standards, while helping the organization rapidly expand complex Machine Learning and related applications.*Key Responsibilities*:*Data Engineering-Specific Responsibilities*Participate in continuous delivery pipeline to fully automate deployment of the highly available cloud platform that supports multiple projectsDesign and develop ETL workflows and datasets to be used in data visualization toolsWrite complex SQL queries with multiple joins to automate and manipulate data extractsPerform end to end Data Validation to maintain accuracy of data setsBuild tools for deployment, monitoring and operationsTroubleshoot and resolve issues in the development, test and production environmentsDevelop re-useable processes that can be leveraged and standardized for multiple instancesPrepare technical specifications and documentation for projectsStay up-to-date on relevant technologies, plug into user groups, understand trends and opportunities to ensure we are using the best possible techniques and toolsUnderstand, implement, and automate security controls, governance processes, and compliance validationDesign, manage, and maintain tools to automate operational processes*Data Science-Specific Responsibilities*Perform exploratory data analysis to identify patterns from historical data, generate and test hypotheses, and provide product owners with actionable insightsDesign experiments for product initiatives and perform statistical analysis of the results with recommendations for next steps and future experimentsCreate and design dashboards by using different data visualization tools to present reports and insights, and support business decision makingHelp the StackPros Data Systems team adopt and evolve Predictive Modeling, Machine Learning and Deep Learning processes to deliver to clients in the future*Company-Wide Responsibilities: *Maintain and exceed client satisfaction with StackPros Inc.\u2019s deliverables, day-to-day work and overall value as a partnerCultivate opportunities for company growth, always seek areas where StackPros Inc.\u2019s role could be expandedAdapt to ever-changing client needs and expectationsMaintain dedication toward achieving excellence in StackPros Inc.\u2019s delivery against client needs, and overall success as an organizationBe an enthusiastic, positive and generally awesome team mate, mentor & constantly curious learner*Qualifications: *4+ years experience in Data EngineeringUnderstanding of digital ecosystems including online data collection, cloud systems and analytics tools (Google Stack, Facebook, AWS, Salesforce, Adobe Suite etc.) a strong assetStrong technical understanding of a range of marketing concepts such as cookie-based data collection, setting and leveraging audience segments, attribution modelling, AB/N & multivariateExcellent written & verbal communication skills are essential; candidate should be comfortable presenting and participating in group discussions of concepts with internal and external stakeholdersCandidate must exhibit an analytical, detail-oriented approach to problem solvingExperience with Jira / Atlassian project management tools is an asset*WHAT\u2019S IN IT FOR YOU?*100% employer-paid benefits packageMonthly yoga and meditation classes onsiteRegular Lunch and Learns from your Team MatesStanding desksEntertainment and Games area, including pool tableFully-loaded kitchen: snacks/fruit/drinks/beerFun Employee Events and ActivitiesParticipation in Community EngagementJob Type: Full-time", "job_collect_date": "2020-08-16T16:17:59.000Z"}, "37": {"job_id": "e493781e9354119e", "job_title": "Senior Data Engineer (Remote)", "job_employer": "FreshBooks", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=e493781e9354119e&fccid=785af18d53962443&vjs=3", "job_description": "FreshBooks has a big vision. We launched in 2003 but we're just getting started and there's a lot left to do. We're a high performing team working towards a common goal: building an elite online accounting application to help small businesses better handle their finances. Known for extraordinary customer service and based in Toronto, Canada, FreshBooks serves paying customers in over 120 countries.\n\nThe Opportunity - Senior Data Engineer\n\nFreshBooks is seeking a Senior Data Engineer to join our team. You will help build new features and update existing ones in our current data pipeline infrastructure. If you're committed to great work and are constantly looking for ways to improve the systems you're responsible for, we'd love to chat with you!\n\nWhat you'll do:\n\nCollaborate with data engineers and full-stack developers on cross-functional agile teams working on features for our stakeholders.\nWork closely with our analytics, data science, product and other internal business teams to ensure their data needs are met.\nParticipate and share your ideas in technical design and architecture discussions.\nShip your code with our continuous integration process.\nProvide coaching to data engineers and share and learn from your peers.\nDevelop your craft and build your expertise in data engineering.\n\nWhat you bring:\n\nEnthusiasm for data engineering!\nExperience creating and maintaining data pipelines\nExperience with AWS, or another major cloud provider such as Google Cloud Platform.\nExperience with Redshift, Big Query, or similar cloud storage technologies.\nStrong programming skills in Python or similar language\nA strong practitioner of test-driven (and behavioural test-driven) development\nExperience with Git workflows, continuous integration and automated build pipelines.\nExperience working in an Agile environment.\n\nWhat you might bring:\n\nA track record of staying at the forefront of data engineering technology.\nExperience with BI tools: Periscope, Looker.\nExperience with Spark, Kafka, Flink, Dataflow, or other streaming technologies.\nExperience with Docker, Kubernetes, Terraform, and other DevOps and infrastructure as code technologies.\nA limitless imagination for where data could go and what we can do with it to make our customers and our people awesome!\n\nWhy Join Us\n\nWe're an ambitious bunch, with our eyes laser-focused on shipping extraordinary experiences to small business owners. You'll be surrounded by talented team members who share a common vision for what an amazing software company could be, and have the opportunity to help build a world-class one, right here in Toronto, Canada.\n\nApply now\n\nHave we got your attention? Submit your application today and a member of our recruitment team will be in touch with you shortly!\n\nFreshBooks is an equal opportunity employer that embraces the differences in all of our employees. We celebrate diversity and are committed to creating an inclusive environment for all FreshBookers. All applicants are evaluated based on their experience and qualifications in relation to this position.\n\nFreshBooks provides employment accommodation during the recruitment process. Should you require any accommodation, please indicate this on your application and we will work with you to meet your accessibility needs. For any questions, suggestions or required documents regarding accessibility in a different format, please contact us at phone 416-780-2700 and/or accessibility@freshbooks.com", "job_collect_date": "2020-08-16T16:18:00.000Z"}, "38": {"job_id": "ad526aacc6864e5d", "job_title": "Data Engineer - Infosphere MDM", "job_employer": "Manulife", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=ad526aacc6864e5d&fccid=1747adf6142beb48&vjs=3", "job_description": "Are you looking for unlimited opportunities to develop and succeed? With work that challenges and makes a difference, within a flexible and supportive environment, we can help our customers achieve their dreams and aspirations.\nJob Description\nAre you looking for unlimited opportunities to develop and succeed? With work that challenges and makes a difference and a flexible and supportive environment, we can help our customers achieve their dreams and aspirations. We are currently seeking a Data Engineer to join one of our fast growing teams.\nA day to day breakdown of the role would be as follows: 60% Hands On Development and Analysis, 20% Business partner interaction, 20% Agile Team Collaboration.\nAdditional responsibilities include:\nDesigns and implements data architectures in production environments\nYou will develop solutions that process data real-time from a variety of sources and make data available to multiple partners ranging from other IT applications to business teams to end customers.\nYou will work jointly with the Front-End team to ensure the definition of API endpoints is well understood and their requirements are supported\nTranslates business needs into data architecture solutions\nDevelops data landscape modernization architectures and roadmaps\nReview and analyze the effectiveness and efficiency of existing systems and develop strategies for improving or further utilizing these systems.\nCreates, reviews, updates and presents systems models, specifications, diagrams and charts to provide direction to system programmers and manages third party vendor (managed services) relationships.\nDevelops standards and processes for coding, deployment, testing, and governance.\nYou would be a good for this this role if:\nYou work jointly with the Front-End team to ensure the definition of API endpoints is well understood and their requirements are supported\nYou value collaborative development, this can be read in the quality and readability of your code and in your thorough code reviews\nYou are obsessed with accurate, reliable customer data\nYou think about QA and testability before you implement anything\nYou are mindful your work doesn\u2019t stop when something works on your local computer: you work collaboratively with DevOps, think about migration, deployment, test coverage and documentation\nYou are promoting a culture of self-serve data analytics by minimizing technical barriers to data access and understanding\nQualifications for this role include:\nBSc in Computer Science, Statistics, Informatics, Information System, Mathematics or equivalent quantitative field preferred\nWell versed in software development methodologies, testing, release management, and maintenance. take pride in optimizing and cleaning code, and leave a program in a better shape than you found it\nAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL). A SQL veteran,\nGood understanding of Master Data Management and IBM InfoSphere.\nGood knowledge of DevOps and CI practices, ability to spec and setup the right environments and deployment procedures, proficiency with Docker and Jenkins.\nExcellent troubleshooting skills. Experience performing root cause analysis on internal and external data and processes to answer specific business questions and find opportunities for improvement.\nStrong analytic skills related to working with unstructured datasets.\nUnderstanding of relational and warehousing database technology working with at least one of the major databases platforms, preferably DB2, SQL Server\nPractical experience with big data processing frameworks and techniques such as Azure, Hortonworks, Spark, Hive, PCF,\nSolid working knowledge of data processing tools using SQL, Spark, Python or similar open source and commercial technologies\nKnowledge of Java/Scala especially in relation to big data open source software preferred.\nETL experience is a requirement.\nAgile project methodologies\nCollaborative attitude, willingness to work with team members; able to coach, participate in code reviews, share skills and methods\nConstantly learns from both success and failure\nGood organizational and problem-solving abilities that enable you to manage through creative abrasion\nGood verbal and written communication; effectively articulates technical vision, possibilities, and outcomes\nExperiments with emerging technologies and understanding how they will impact what comes next.\nWhat about Perks?\nManulife has lots of perks including, but not limited to:\nCompetitive compensation\nRetirement Savings Accounts including a RPP (Pension Plan), RRSP (Retirement Savings Plan), and TFSA (Tax Free Savings account)\nManulife Share Ownership Program with employer matching\nCustomizable Benefits Package including Health, Dental, Vision, and 100% of Mental Health expenses\nFinancial support for ongoing training, learning, and education\nMonthly Innovation Days (Hackathons)\nWearing jeans to work every day\nAn abundance of career paths and opportunities to advance\nThis is a full-time permanent role located in Toronto, Ontario.\nWe are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status\n\nIf you are ready to unleash your potential it\u2019s time to start your career with Manulife/John Hancock.\nAbout Manulife\nManulife Financial Corporation is a leading international financial services group that helps people make their decisions easier and lives better. With our global headquarters in Toronto, Canada, we operate as Manulife across our offices in Canada, Asia, and Europe, and primarily as John Hancock in the United States. We provide financial advice, insurance, and wealth and asset management solutions for individuals, groups and institutions. At the end of 2019, we had more than 35,000 employees, over 98,000 agents, and thousands of distribution partners, serving almost 30 million customers. As of March 31, 2020, we had $1.2 trillion (US$0.8 trillion) in assets under management and administration, and in the previous 12 months we made $30.4 billion in payments to our customers. Our principal operations are in Asia, Canada and the United States where we have served customers for more than 155 years. We trade as 'MFC' on the Toronto, New York, and the Philippine stock exchanges and under '945' in Hong Kong.\nManulife is an equal opportunity employer. We strive to attract, develop and retain a workforce that is as diverse as the customers we serve and to foster an inclusive work environment that embraces the strength of cultures and individuals. We are committed to fair recruitment, retention and advancement and we administer all of our practices and programs based on qualification and performance and without discrimination on any protected ground.\nIt is our priority to remove barriers to provide equal access to employment. A Human Resources representative will consult with applicants contacted to participate at any stage of the recruitment process who request any accommodation. Information received regarding the accommodation needs of applicants will be addressed confidentially.", "job_collect_date": "2020-08-16T16:18:00.000Z"}, "39": {"job_id": "67bbfacf7e5a2ca0", "job_title": "Data/Software Engineer Co-op", "job_employer": "Smart Nora", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=67bbfacf7e5a2ca0&fccid=e4a22b3d28be02c0&vjs=3", "job_description": "The Company\n\nSmart Nora is a wellness tech company based in Toronto that has improved sleep and relationships for tens of thousands of couples around the world. Our debut product is the world's most comfortable snoring solution and has been listed on Oprah\u2019s Favorite Things, as well as Good Morning America, TIME, TODAY and BBC to name a few.\n\nThe Team\n\nWe are an ambitious, tight-knit team with an open work environment and a self-directed approach. We typically work out of our office at King+Spadina or remotely in a weekly cadence of Monday kick-offs, Wednesday tea times, and Friday wrap-ups \u2014 taking a lot of pride in our individual work and holding one another accountable for producing great work.\n\nNote: Currently we are all working from home throughout the COVID-19 pandemic.\n\nThe Product\n\nSmart Nora is an over-the-counter, contact-free snoring solution relevant to the 40% of adults who snore. Smart Nora is loved by customers for its comfortable contact-free design that enables users to sleep without any attachments to their face or body.\n\nThe Role\n\nWe are looking for a Software / Data Engineer. This is a co-op placement, full-time from September - December ideally for a 4th-5th year student. We are flexible with an earlier starting date or longer term.\n\nLocation\n\nRemote\n\nWhat you would do\n\nSupport the current FW project conducted with our third party project partners by implementing structured testing and documentation\nBuild supporting tools in Python, Node, or other scripting languages to validate firmware and hardware\nActively participate in Mobile App development project with our third party project partners\nSupport acoustic performance optimization including microphone and codec gain settings, assisted by automated tests\nBe part of the Product team developing the next generation Smart Nora device\nWrangle data and decipher meaning from quantitative tests and analytics\nWrite clear documentation\n\nOur requirements\n\nComprehensive understanding of Software Development processes (SDLC)\nComprehensive understanding of Object Oriented Programming (OOP) principles\nExperience with Pandas, R, Tableau, or other data processing/visualization tools\nExperience with GitHub (send us your profile!)\nExposure to cloud (AWS) and APIs is a plus\nExperience in audio processing is a plus\nExperience in machine learning is a plus\nMajoring in Software Engineering, Computer Science, Industrial & Systems Engineering, or related field.\n\nHow to Apply\n\nPlease apply using our online application form to include your resume and links to your LinkedIn, GitHub (if have), and Kaggle (if have) profiles.\n\nAccommodations are available on request for candidates throughout the application process. Please let us know your needs so that we may accommodate. Email careers@smartnora.com if you would like to discuss this role before applying.", "job_collect_date": "2020-08-16T16:18:01.000Z"}, "40": {"job_id": "485168fc8141d4c6", "job_title": "Data Engineer (Cloud)", "job_employer": "StackPros", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=485168fc8141d4c6&fccid=efee8d7cab418992&vjs=3", "job_description": "StackPros Inc is seeking a candidate for a full-time role within our Data Systems Team in Toronto, Ontario.\nThe Cloud Data Engineer will play a key role at StackPros, required to help create and maintain industry-leading quality and efficiency of service and software delivery.\nStackPros will rely on the Data Engineer to support the Data Systems team, in both data engineering and data science-related workflows. The Data Engineer will be expected to meet and exceed StackPros\u2019 quality standards, while helping the organization rapidly expand complex Machine Learning and related applications.\nKey Responsibilities:\nData Engineering-Specific Responsibilities\nParticipate in continuous delivery pipeline to fully automate deployment of the highly available cloud platform that supports multiple projects\nDesign and develop ETL workflows and datasets to be used in data visualization tools\nWrite complex SQL queries with multiple joins to automate and manipulate data extracts\nPerform end to end Data Validation to maintain accuracy of data sets\nBuild tools for deployment, monitoring and operations\nTroubleshoot and resolve issues in the development, test and production environments\nDevelop re-useable processes that can be leveraged and standardized for multiple instances\nPrepare technical specifications and documentation for projects\nStay up-to-date on relevant technologies, plug into user groups, understand trends and opportunities to ensure we are using the best possible techniques and tools\nUnderstand, implement, and automate security controls, governance processes, and compliance validation\nDesign, manage, and maintain tools to automate operational processes\nData Science-Specific Responsibilities\nPerform exploratory data analysis to identify patterns from historical data, generate and test hypotheses, and provide product owners with actionable insights\nDesign experiments for product initiatives and perform statistical analysis of the results with recommendations for next steps and future experiments\nCreate and design dashboards by using different data visualization tools to present reports and insights, and support business decision making\nHelp the DRVN Intelligence Data Systems team adopt and evolve Predictive Modeling, Machine Learning and Deep Learning processes to deliver to clients in the future\nQualifications:\n3+ years experience in Data Engineering\nUnderstanding of digital ecosystems including online data collection, cloud systems and analytics tools (Google Stack, Facebook, AWS, Salesforce, Adobe Suite etc.)\nStrong technical understanding of a range of marketing concepts such as cookie-based data collection, setting and leveraging audience segments, attribution modelling, AB/N & multivariate\nExcellent written & verbal communication skills are essential; candidate should be comfortable presenting and participating in group discussions of concepts with internal and external stakeholders\nCandidate must exhibit an analytical, detail-oriented approach to problem solving\nExperience with Jira / Atlassian project management tools is an asset\nCompany-Wide Responsibilities:\nMaintain and exceed client satisfaction with StackPros Inc.\u2019s deliverables, day-to-day work and overall value as a partner\nCultivate opportunities for company growth, always seek areas where StackPros Inc.\u2019s role could be expanded\nAdapt to ever-changing client needs and expectations\nMaintain dedication toward achieving excellence in StackPros Inc.\u2019s delivery against client needs, and overall success as an organization\nBe an enthusiastic, positive and generally awesome team mate, mentor & constantly curious learner", "job_collect_date": "2020-08-16T16:18:02.000Z"}, "41": {"job_id": "d5e196c24ff9d6f9", "job_title": "Data Engineer", "job_employer": "Technology Development Corp", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=d5e196c24ff9d6f9&fccid=d5eebd59dbfe9145&vjs=3", "job_description": "Our client located in downtown Toronto is looking for a Big Data Engineer who will be responsible for developing new system stacks and tools for big data ingestion, processing, and analytics. This position is perfect for a developer whose passion is to apply cutting-edge technologies to solve complex business and engineering problems. This individual will work on a team of talented engineers responsible for the full life-cycle of production systems, software, tools, and flows.\n\nResponsibilities\n\nDesign, develop, and maintain the software and systems that make up the data platform that runs our entire business\nParticipate in multi-disciplinary projects\nPartner with the Data Science and Engineering teams who use our platform to by diagnosing, predicting and correcting scaling problems\nContribute to our teams growing set of development platforms, tools, and processes\n\nRequired Skills\n\nHands-on experience with Big data technologies (HBASE, HDFS, SPARK, and/or HADOOP)\nDemonstrated proficiency with Spark, Scala, Python\nExperience in building stream processing systems using spark streaming or Storm.\nExperience in integration of data from multiple sources\nExperience with NoSQL cluster databases, such as HBase, Cassandra, Druid\nExperience with various messaging systems such as KAFKA, or RabbitMQ\nProficient understanding of distributed computing principles.\n\nPreferred Skills\n\nExperience with other highly scalable, low latency big data systems is a plus\nExperience with Hortonworks / Cloudera distributions is a plus\nExperience with Cloud technology and containerization (docker / kubernetes) is a plus\n\nWho you are\n\nAble to learn and apply new technologies quickly\nProven excellent problem-solving abilities\nAble to work both independently and as part of a team\nAble to multi-task in a dynamic environment\nHave Excellent verbal and communication skills\n\nQualifications\n\nSoftware development experience using mainstream languages such as Java, Scala and/or C++\nExperience architecting, deploying and operating mission-critical big data clusters.", "job_collect_date": "2020-08-16T16:18:03.000Z"}, "42": {"job_id": "281e0356c31acae9", "job_title": "Data Engineer", "job_employer": "Capgemini", "job_location": "Brampton, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=281e0356c31acae9&fccid=105ecfd0283f415f&vjs=3", "job_description": "Job Description:\n3-6 yearsExpertise on Spark Scala.Ability to develop ETL jobs to implement business logic using Scala (Spark Framework)Conversant with Hive Database, Able to create HQL scripts and work on Hive tables for data analysisPerformance tuning of the existing Hadoop jobs, able to trouble shoot and fix existing bugs.Good understanding of Oracle Exadata RDBMS, able to profile telecom data residing in Exadata and derive business rules.Co lace with business, have working session with business to identify and freeze business logic.Understanding / experience working on scrum based Agile set up.\n\nThe Capgemini Freelancer Gateway is enabled by a cutting-edge software platform that leads the contingent labor world for technology innovation. The software platform leverages Machine Learning and Artificial Intelligence to make sure the right people end up in the right job.\n\nA global leader in consulting, technology services and digital transformation, Capgemini is at the forefront of innovation to address the entire breadth of clients\u2019 opportunities in the evolving world of cloud, digital and platforms. Building on its strong 50 year heritage and deep industry-specific expertise, Capgemini enables organizations to realize their business ambitions through an array of services from strategy to operations. Capgemini is driven by the conviction that the business value of technology comes from and through people. It is a multicultural company of over 200,000 team members in more than 40 countries. The Group reported 2018 global revenues of EUR 13.2 billion.", "job_collect_date": "2020-08-16T16:18:03.000Z"}, "43": {"job_id": "e8f01b72a171ac29", "job_title": "Data Engineer", "job_employer": "goeasy", "job_location": "Mississauga, ON", "job_link": "https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DbNJQGwjC-NEA80-qlVyEGmSv34Xx0eW4t4u9rXSv_7_tlvMU2e-Gw_wrZ47s1zPCnrySU2yuEL07wtSyx7XOwYh5VPcVL7CxRhOYwHzR-CdmyBrwbeqommOn6y9FD_uKIYpUCb9-PC_XgwGzBDrPy7SD7JQndrQdnAb_vhLXKdclnrHUyyqFOO2ZhZqkLhBg0Gn0NuAgAnXYz17xIE0vcHkZOpdy4oT918TvqBrnuvqbEYmlbrS73TUyD6cDDZ7ITBKAHbqh6RWSXtee8X6D-jYc-5H_QvqY2SyRa9oMZ5V0Zu2yFyUR46JSWk5HKQOfdDJwjGvIlSgkUykV9YcuqrW8G81uE_BzhAzc8Blzrpzq8dj3ZaM33Exazm9NL3Ab4vSPbWxZDfbVsHGYRjd8azqA1it8ZvdYs4VxtK-qrFZ-_8lHrtv_WEY9xOdjhplnTAvcTkkg8qf6SQAoAOmNIhg6pLPHgibnyph7JbL905g==&p=12&fvj=0&vjs=3", "job_description": "If you are looking to join one of Canada\u2019s fastest growing companies, goeasy Ltd. is the place for you! Recognized as one of Canada\u2019s Most Admired Corporate Cultures, one of Canada\u2019s Top 50 Fintech\u2019s and one of North America\u2019s Most Engaged Workplaces, we want the best and brightest to join our team.\n\nWe are a publicly traded company on the TSX with over 4000% shareholder return since 2001, goeasy operates two main business units. easyfinancial is our consumer lending business that offers secured and unsecured installment loans of up to $35,000 and easyhome is Canada\u2019s largest merchandise lease-to-own company. It is our mission to provide everyday Canadians the chance for a better tomorrow, today by giving them access to the credit they need and by offering them a second chance when they have been turned down by banks and traditional lenders. With a retail network of nearly 400 locations across Canada and over 1900 employees, we are able to build lasting relationships with our customers as we help them rebuild their credit and graduate towards prime rates and a brighter financial future.\n\nThe Data Engineer on the Data Science and Business Insights team will build, integrate data from various resources, manage data in goeasy operational data store and enterprise data warehouse. This position will develop ETL (Extract, Transform and Load) with various tools on large datasets to ensure data is easily accessible, works smoothly, as well as maintain and expand the data warehouse for reporting and analysis. The Data Engineer will also work closely with the data architect on the design and architecture of our enterprise data warehouse.\n\nResponsibilities:\n\nDevelop data set processes for data modeling, mining and production\nDevelop and maintain ETL processes using SSIS, Scripting and data replication technologies\nParticipate in development of datamarts for reports and data visualization solutions\nResearch opportunities for data acquisition and new uses for existing data\nIntegrate new data management technologies and software engineering tools into existing structures\nSupport the translation of business requirements for data acquisition/manipulation and provide detailed specifications that can be passed downstream for use\nDevelop detailed technical specifications and operational support documentation in collaboration with Business Systems Analysts, BI Engineers and Architects.\nIdentify and communicate technical problems, process and solutions\nCreate Ad-Hoc queries and reports as needed along with providing on-going analytical support for these requests\nAssist in the collection and documentation of user\u2019s requirements\nEnsure that existing business processes dependent on the ODS/EDW are monitored and respond quickly to bug fixes, enhancement requests and production ETL related issues.\nDealing with the database users on a daily basis to ensure that problems are dealt with promptly and that appropriate fixes are made to resolve any problems.\nRecommend ways to improve data reliability, efficiency and quality\nEnsure systems meet business requirements and industry practices\nWork effectively with the Business Intelligence and Data Solutions Architects, Data and BI Engineers to ensure that all approved development and deployment procedures are followed.\n\nQualifications:\n\nBachelor\u2019s Degree in Computer Science, MIS, Computer Engineering or other Information Technology related degree\n4+ years working with SQL Server or comparable relational database system\n3+ years of extensive ETL development experience with SSIS and/or ADF\n4+ years of experience troubleshooting within a Data Warehouse environment\nExpert domain knowledge & experience in Data warehousing, encompassing data model design, dimensional modeling, naming conventions, cross-cutting concerns, common integration technologies, patterns & standards and emerging technologies.\nExpert Knowledge of SQL skills to build, debug, and optimize (developing procedures, functions, SQL queries, etc.) and working with large data sets and to perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.\n2+ years SQL Server Database administration experience\nCloud experience (Azure) is highly preferred\nExposure and experience with Python, R, Hadoop, Azure, other Big data and advanced analytics\nKnowledge of AI and ML developments/solutions/implementations\nExperience with multiple programming languages (PowerShell scripting, C#, others) with basic scripting skills.\nHigh level of technical aptitude\n\nInclusion and Equal Opportunity Employment\n\ngoeasy is an equal opportunity employer. In addition, goeasy is committed to providing accommodations for applicants upon request at any stage of the recruitment process in accordance with all legislative requirements throughout Canada. Please let us know if you require an accommodation due to a disability during any aspect of the recruitment process and we will work with you to address your needs.\n\nAdditional Information:\n\nAll candidates considered for hire must successfully pass a criminal background check, credit check, and validation of their work experience to qualify for hire. We thank all interested applicants, however we will only be contacting those for interview who possess the skills and qualifications outlined above.\n\nWhy should you work for goeasy?\n\nTo learn more about our great company please click the links below:\n\nPAID1234", "job_collect_date": "2020-08-16T16:18:04.000Z"}, "44": {"job_id": "a32b7c599156a218", "job_title": "BHJOB15656_15145 - Data Engineer w/ DevOps - Azure Cloud", "job_employer": "Myticas Consulting", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Bt274jo0r_e0cCCqhskoQvohXG3bfDAo8rIdE-Msu3yuVfyF2oiDFKHFOU-vxVulAfe8kPL7riFwoXa1bsfN-szjxO_8qUQWbPsJOnTg0EkutIoK2S4NNLL24JQh9e4DsE9hv907k2_zDbvj0a8WAj5oPSV5J0Yhgrx-GCyclKc8LzaN40BKOLnRM7zjoDgSh_elnQz60vzZhQSfebwVn9k-pEdr0GmdPwD_FzR4x2erPkHS7kUxNDVwVDtbD6A-e7yc5jE2SL_kZM_ah0aRkKqVGPo34u14M0g4m4ziSxzwIfZppoa98OmSnR5An8u5cPCuQDRXcVROh8SbS3SGW0thh8__L1V8aoZ4kOFSrYkEuoxVENfUeEEr-5-I9Fu3f-MzWI5jWJOIoIXlIlFClivTMz7wnytABoPx1G-IRxatGexklvlSX3cq2fmNze2UD22iN9ZAkMWA==&p=13&fvj=0&vjs=3", "job_description": "The growing team here at Myticas is looking for an experienced Data Engineer who would be interested in a remote contract opportunity within the Ottawa, Toronto and Montreal regions.\n\nAccountabilities:\n\nDefining and reviewing security design requirements for cloud infrastructure and application components.\nEvaluating architecture patterns from security perspective.\nBuilding and implementing security controls to enable enforcement of compliance with Cloud Control Objectives, using custom Azure policies and integrated controls in DevOps processes\n\nRequirements:\n\nStrong Data Engineer w/ DevOps expertise + Azure Cloud Experience\nMust know how to code and stand up scripts.\nExperience with Data Digestions\nExperience writing scripts to automate (infrastructure)\nARM Templating Expertise\nAzure Synapse Expertise\nSupport developing automated DevOps processes and procedures for the following Azure components:\nAzure Synapse (Azure DW) & Studio (private preview)\nAzure Data Catalog Gen 2 (Babylon \u2013 private preview)\nAzure Data Lake Storage Gen 2\nAzure ML\nML Flow\nAzure SQL Analysis Service\nAzure Databricks\nADF data pipelines for data loading to AzSQL/Synapse\nADF data pipelines for connecting to on-prem data sources for data\n\nCandidates looking to apply for this role are to send us an updated version of their resume in confidence. Our team will be sure to review all applicants and follow up accordingly at the conclusion of the review process.\n\nJob is also known as: Data Engineer, DevOps Engineer, Azure Cloud Engineer, Azure Engineer, Cloud Engineer\n\nINDMY", "job_collect_date": "2020-08-16T16:18:05.000Z"}, "45": {"job_id": "54e0fcc38385e3a2", "job_title": "Data Engineer/Integration Consultant", "job_employer": "Copperstone Connect", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Ceb7pqR8iGmLuXIZQklF6pqed3xddONNM6Jjumj-ndyGK0kULQUmQ2REqfJ5KAzFVeK_es1WVg0s1sLgKW09UsvVRp0-dSizFQr1oNyK_VIqdjla00VU9GNVvu2x4v9p3hsL7NeF-6hBksbAaSgKazsaYTODGMKQQeVuYcxoOuxvFKuD0HRLD9ybgbcXTn1LNvtGLQfJlBChsbqxYk5nhyZw8ZAK-1xVtAQ0CWYcVFUCj_yukI13DVCwqq_du_-0biE4ooJedRzo5umWHSMtZBkCj4tdUuKA2xrdE5PZn0vXe9TdJ9XAKT8MtehNBrU0oM9R4iBviBYhO1FZF-YvIy6JNNDZCwRKEhyzJbfKl2yKuqLskhNoW3Qi00siXl9gr1rvGzZUZ-D5tueQb2bZuxxxnRx-HHGsll4GmuWV0BOyGbyYHIvfKeaC26wx3nkqGdByX9PV55328x68qklpMLaVIFxruyi6iil-pIqYI4IGdxYD-E2S50qJSkn6UUW4d-sbwY4028CeoafwYSfJgvR6aAxYcwd_-yQf4wUMrcWMSiwYJoPoY1fd0ggx0DJYLZTB6Ylrl7SUdpOsMy0Xj0C8aweN7yqnIJVhSPvcTR10f5VLa3BO6dWKb4VSZSrvYw-ivuO4kblZY5tGJLBO1gCYIbWKbMpIh9dO94-ipcdA==&p=14&fvj=0&vjs=3", "job_description": "Our client, a boutique-size consulting firm, prides itself on being at the forefront of innovation in the Big Data space. Founded in 2010, they provide thought leadership and implementation excellence within the ever-growing data and analytics world.\nThey take pride in having some of the most highly trained and experienced consultants in the industry which translates into optimal value for their clients. They were one of the first companies to provide analytics and data as a service, via the cloud, as early as 2010. They strive to make sure their customers are well-positioned with the best technologies/tools in the industry, constantly evaluating new and existing technology partnerships. Some of the more prominent companies they have partnered with include; Snowflake, DataRobot, MicroStrategy, Informatica, Amazon AWS & Microsoft. They continue to invest in their most valuable resource, their people. They do this through extensive training both on the job and through various educational programs.\nDue to growth, they are seeking to hire an Intermediate - Senior Integration Consultant.\n\nDesired Skills and Experience\n: University/College degree in Computer Science, Mathematics, Data Science and/or Relevant Degree\n: 5+ years hands-on development, configuration, scripting and administration experience with Data Integration platforms. (i.e. Informatica, Talend, DataStage, SSIS)\n: BI Experience (MicroStrategy, Looker, Tableau, PowerBI) considered a nice to have\n: Extensive theoretical and practical knowledge of data warehousing principles/concepts and practical development experience in all areas of the data warehousing life cycle\n: Experience with Data Management, ETL, Cloud Data (AWS), Data Integration\n: Knowledge of OLAP-related principles and concepts\n: Strong grasp of data modeling techniques and concepts (Normalized/Denormalized, Conceptual/Logical/Physical, Star, Snowflake, Data Vault)\n: Strong knowledge and experience with relational databases such as Snowflake, SQL Server, Oracle (Advanced knowledge of reading and writing SQL, Performance analysis and tuning)\n: Knowledge and experience with key Big Data technologies (Hive, Presto, Spark, Kafka, NoSQL databases, Semi-structured data access patterns (Json, Parquet, XML, etc.))\n: Strong Python scripting skills\n: Excellent communication skills\n: Great problem-solving skills\n: Leadership and good client management skills\n\nDay to Day Activities Would Include\n: Conduct relevant customer interviews to determine key business requirements and objectives\n: Build appropriate analytical data models based on outcomes of user interviews\n: Analyze and profile data systems to build source to target data mappings\n: Review ETL performance and conducts performance tuning as required on mappings/workflows or SQL\n: Administration and support of data integration infrastructure\n: 2nd level on-call support of ETL services as required\n\nYou will be responsible for attaining the following goals:\n: Attaining a minimum of 1 new accreditation/certification per year\n: Spending 80% or more of their time on billable work\n: Completing 90% or more of their agile delivery tasks on time\n: Demonstrating competency in 1 new relevant technology every year", "job_collect_date": "2020-08-16T16:18:06.000Z"}, "46": {"job_id": "a32b7c599156a218", "job_title": "BHJOB15656_15145 - Data Engineer w/ DevOps - Azure Cloud", "job_employer": "Myticas Consulting", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Bt274jo0r_e0cCCqhskoQvohXG3bfDAo8rIdE-Msu3yuVfyF2oiDFKHFOU-vxVulAfe8kPL7riFwoXa1bsfN-szjxO_8qUQWbPsJOnTg0EkutIoK2S4NNLL24JQh9e4DsE9hv907k2_zDbvj0a8WAj5oPSV5J0Yhgrx-GCyclKc2EhTZ45wozqvgTmF0RfZDZQ40_UeCFWGju36sjHfzxVJocTX0ya9jMVzymsab3Q_OCQ-97uj2BvF2-AVmIK4M0pa0_Kd7ZFuYKspHHwWSS1QGkAi1-aoOcbOhaFGYidCvqV8uA5lgLumyMrkrwBuuwHp7CFp_8Hzc32VLRTz88tV38ZV2qKc9O-Ic2SEZWrBt992tBtLuGfa2t9Rkd1b39mEH8DOxuP2iXSU-RQXVbaGQ9IkIixi8GsPzpPhMDf_jtZJPS2DZxI8bhlFsuIaaV0aCCv5i4kRg==&p=0&fvj=0&vjs=3", "job_description": "The growing team here at Myticas is looking for an experienced Data Engineer who would be interested in a remote contract opportunity within the Ottawa, Toronto and Montreal regions.\n\nAccountabilities:\n\nDefining and reviewing security design requirements for cloud infrastructure and application components.\nEvaluating architecture patterns from security perspective.\nBuilding and implementing security controls to enable enforcement of compliance with Cloud Control Objectives, using custom Azure policies and integrated controls in DevOps processes\n\nRequirements:\n\nStrong Data Engineer w/ DevOps expertise + Azure Cloud Experience\nMust know how to code and stand up scripts.\nExperience with Data Digestions\nExperience writing scripts to automate (infrastructure)\nARM Templating Expertise\nAzure Synapse Expertise\nSupport developing automated DevOps processes and procedures for the following Azure components:\nAzure Synapse (Azure DW) & Studio (private preview)\nAzure Data Catalog Gen 2 (Babylon \u2013 private preview)\nAzure Data Lake Storage Gen 2\nAzure ML\nML Flow\nAzure SQL Analysis Service\nAzure Databricks\nADF data pipelines for data loading to AzSQL/Synapse\nADF data pipelines for connecting to on-prem data sources for data\n\nCandidates looking to apply for this role are to send us an updated version of their resume in confidence. Our team will be sure to review all applicants and follow up accordingly at the conclusion of the review process.\n\nJob is also known as: Data Engineer, DevOps Engineer, Azure Cloud Engineer, Azure Engineer, Cloud Engineer\n\nINDMY", "job_collect_date": "2020-08-16T16:18:08.000Z"}, "47": {"job_id": "05f7bf2a7458c7fc", "job_title": "Data Engineer", "job_employer": "TES - The Employment Solution", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DQQnUBuQBSuyaIQhpC59TW7hrTbBg8v-nGtzzV8dunbQHh9VTp-k89gJ8q3B9UEXLHcYbAG67d8jpOK4E3ok8olcOhfyyiChWMI3BPvUFdra3ljwgLS_vFsJXlW6jHt2NpQMzd2p9F6BXGm9ND3feUN42yGy4tVXCEXqAGgdFld-XpVSYQ9qTBgM7GqD59qPJK_33jU3HjHWQBfFRxLk4LPOHhruQbIkH33I5E6ycn9koHOYlp39BSiBRLfhfchaMrKl6Z3ozn9rt05OomN5oU3Ejf_x5VV4oowtBRdoJA1lNZXq9rZre46YRpS5sb6eMuRM9paNyPh4vNSDs5h4ZERkKeFxOf8RpEpL-XWPUgDvCtuPeezsl-aDROIl5hWsYNnml3Na0Ie1MCKLrr0to3Xyy_NT8H-wGmZqc7eye-SNOfYCdsMIavurV0_NwVHKBCuy7r5nwfDF0mYgBUKuZvzDf1WLmUR6mqhF3cQ2nkq_ZPZqkTImnc9quo9qkr00zgbIcfdUOFIiGc_Nd4RkvX_pQQ2L4nrzYkdFnBuMngTeQ9dPbcYUdsDn04UX7qR7EKOaOy5CLiTTbV05FetLF3XS_1mDDB2EDamNerZEfDG6JgTmMTKob4-Z8qrQAaRFgVGCuDR6v8Sm1UfLiCDFQuaEhTTGgWJgqrDYphhhQ4ielxAu5JvRZOSxhDKcbip68hA9xSXFvjEHhV_zmO5ugG5RE4zHWqA9GyL5c5WjBuKKRuYS5tPXbfcPJKyFGZXSWfXgVqQfP6sgU1yZtniIUOJirLYeiPI90ros5z82-Gyea71uo9853zv-_bVR1vGyYlg7g-L2bGMfx20IpO3xSw1Tl4uB3Jh-s=&p=1&fvj=0&vjs=3", "job_description": "Job Title- Data Engineer\nLocation- Downtown, Toronto\nType - Full Time Permanent\nSalary - Negotiable + Benefits\n\n\n\nFocus on data architecture, best practices, reliability, security, and complianceImprove and extend ETL, data processing, and analytics processesFacility with PowerBI, including creating dashboards and data sourcesDeveloping high complexity, fast performing SELECT queries.Developing T-SQL procedures, functions, triggers, jobs, scripts, etc.Development of Advanced T-SQL such as temporal tables, PIVOTs, recursive table expressions and more.Modeling and implementing Data Mart solution for Power BI analytics\nManaging indexes, statistics, query plans alerts, database activity, and overall performance activity.In-depth experience working with relational databases, such as Microsoft SQL Server or PostgreSQLEnthusiasm for applying good data design, testing, documentation, and support practicesExperience building and optimizing data pipelines, architectures, and data setsKnowledge of message queueing, stream processing, and data stores/warehousesWorking knowledge of AWS products related to data engineeringBachelor's degree in Computer Science, Software Engineering or an equivalent\nExcellent communication skills - both written and verbal; ability to speak in Spanish is a bonus\n\nTo apply please send an email to sheetalk@tes.net", "job_collect_date": "2020-08-16T16:18:09.000Z"}, "48": {"job_id": "63c6f469ca57386c", "job_title": "Senior Data Engineer", "job_employer": "BenchSci", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=63c6f469ca57386c&fccid=cb94cfaf4856cc5b&vjs=3", "job_description": "BenchSci exponentially increases the speed and quality of life-saving research by empowering scientists with the world\u2019s most advanced biomedical artificial intelligence to run more successful experiments. Backed by F-Prime and Google\u2019s AI fund, Gradient Ventures, BenchSci uses machine learning to diagnose pharmaceutical R&D health from hidden patterns in procurement data. A turnkey application of AI with immediate, quantifiable impact, BenchSci now optimizes reagent procurement and experimental success in 15 of the top 20 pharmaceutical companies and over 4,300 leading academic centers globally.\n\nWe are currently seeking a Senior Data Engineer to join our Data Team. As part of the job, you will work on evolving our data models in several styles of datastores, improve internal tooling to allow data self-service, and operationalize production-grade data pipelines.\nWhat you\u2019ll do:\nScale data pipelines to allow data to go from research to platform as fast as possible\nDevelop data access mechanisms for downstream applications consumption\nManage sources which contain both semi-structured as well as unstructured data\nDevelop and apply suitable frameworks to detect data drift, and then calibrate and redeploy them to production seamlessly\nCollaborate closely with other engineers to solve interesting and challenging data problems\nWho we\u2019re looking for:\n5+ years working as a professional developer\nExpertise in Python\nExpertise with SQL\nExpertise in Spark 2.x, Dataset/DataFrame API and performance tuning\nExperience with cloud reference architectures and developing specialized stacks on cloud services\nExperience with Pandas\nYou have strong cross-team communication and collaboration skills\nA team player who strives to see teammates succeed together\nBonus points for:\nBackground in Life Science\nExperience with Airflow or other workflow management systems in a distributed setup\nExperience with graph data modelling and scaling graph databases\nExperience with Kubernetes in production\nExperience with technical design and applying architectural patterns\n\nHere at BenchSci, these are our core values:\n\nFocused: We focus on what will drive the greatest impact at all times.\nAdvancement: We believe in continuous growth, and discovering new ways to do things better. This applies to our product and business, but also to ourselves.\nSpeed: We recognize that without a sense of urgency, our team, our product and our mission lose their value.\nTenacity: What we\u2019re trying to do isn\u2019t easy, but we hire the best people, and give them the autonomy, tools, and resources to succeed. The hard work is up to them.\nTransparency: We believe that sharing diverse ideas and information creates strong teams. Our success stems from research, collaboration, feedback, and trust.\n\nBenchSci is an equal opportunity employer. We value diversity and are committed to fostering an inclusive environment. All four of our cofounders are immigrants to Canada, as are many of our employees. We welcome your fresh perspectives and ideas.", "job_collect_date": "2020-08-16T16:18:10.000Z"}, "49": {"job_id": "a677b4cff4d9abf1", "job_title": "Software Engineer, Data", "job_employer": "Fathom Health", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=a677b4cff4d9abf1&fccid=9e88642c8c797c72&vjs=3", "job_description": "We\u2019re on a mission to understand and structure the world\u2019s medical data, starting by making sense of the terabytes of clinician notes contained within the electronic health records of the world\u2019s largest health systems.\n\nWe\u2019re seeking exceptional Data Engineers to work on data products that drive the core of our business-a backend expert able to unify data, and build systems that scale from both an operational and an organizational perspective.\nAs a Data Engineer you will:\nDevelop data infrastructure to ingest, sanitize and normalize a broad range of medical data, such as electronics health records, journals, established medical ontologies, crowd-sourced labelling and other human inputs.\nBuild performant and expressive interfaces to the data\nBuild infrastructure to help us not only scale up data ingest, but large-scale cloud-based machine learning\n\nWe\u2019re looking for teammates who bring:\nExperience building data pipelines from disparate sources\nHands-on experience building and scaling up compute clusters\nExcitement about learning how to build and support machine learning pipelines that scale not just computationally, but in ways that are flexible, iterative, and geared for collaboration.\nA solid understanding of databases and large-scale data processing frameworks like Hadoop or Spark. You\u2019ve not only worked with a variety of technologies, but know how to pick the right tool for the job.\nA unique combination of creative and analytic skills capable of designing a system capable of pulling together, training, and testing dozens of data sources under a unified ontology.\n\nBonus points if you have experience with:\nDeveloping systems to do or support machine learning, including experience working with NLP toolkits like Stanford CoreNLP, OpenNLP, and/or Python\u2019s NLTK.\nExpertise with wrangling healthcare data and/or HIPAA.\nExperience with managing large-scale data labelling and acquisition, through tools such as through Amazon Turk or DeepDive.", "job_collect_date": "2020-08-16T16:18:10.000Z"}, "50": {"job_id": "cb99766691c3547f", "job_title": "Data Visualization Developer - Toronto, Canada", "job_employer": "New Signature", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=cb99766691c3547f&fccid=f537209340bad710&vjs=3", "job_description": "Join a team of passionate thought leaders in a dynamic and collaborative environment! New Signature's Applied Innovation team is growing fast and we're looking for our next Data Visualization Developer to join us.\n\n\nWhat impact will you have in this role?\n\n\nEvery role at New Signature is equally important in the grand scheme of things and everything we do is team work. Through team work, building relationships internally and at times with clients, understanding context at all times and what is important, and getting comfortable with influence and persuasion you will stand out as an expert in your field.\n\nAs we continue to scale we're looking for the market's best Azure Data & AI specialists to help us grow our business' fastest growing practice. You'll be working with the industry's biggest players, delivering innovative greenfield Data Platform builds, Data Integration programmes and implementing bespoke High-Level Data Architectural designs.\n\n\nWhat type of experience do you need to be successful in this role?\n\n\nStrong experience using the Microsoft Azure BI Stack, with a focus on front-end visualisation (PowerBI, ADFv2 , Azure SQL DB)\nAzure SQL Datawarehouse, Azure Data Lake, Azure Databricks, Analysis Services highly desirable\nKnowledge of C# essential\nAgile methodology experience essential\nCI/CD, Azure DevOps experience, highly desirable\nCustomer/client-facing consulting skills\nAbility to learn quickly and adapt to changes in the Azure data technology landscape\nAzure Data Engineer Associate certification desirable\n\n\nWhat personality traits and other capabilities are important for this role?\n\n\nOur consultants are self-motivated and pragmatic with strong problem-solving skills and a passion for crafting great solutions coming with a wealth of experience or talent that enables quality software delivery. They understand the best approach to architecture and development is through blending technologies and methodologies appropriate to the task at hand. The goal is to contribute to the clients' long lasting success to enable us to expand our business and clientele.\n\n\nSecurity Responsibility:\n\nAll employees must act in accordance with New Signature's corporate security standards.\n\nABOUT NEW SIGNATURE\n\nNew Signature is a cloud-first, full-service Microsoft partner committed to delivering innovative technology solutions that solve human challenges. Behind every interaction is our dedication to provide outstanding experiences and to build authentic relationships with those around us. We are passionate about driving transformational results for clients across all company sizes, geographies and industries. The New Signature team delivers full lifecycle solutions\u2014from project inception and planning, through deployment to ongoing support and maintenance.\n\nNew Signature was named the top Microsoft partner in the United States and the United Kingdom in 2014 and again in the United States in 2015\u2014becoming the first partner ever to win the prestigious US Partner of the Year award two years in a row. With over 600 individual technology certifications, New Signature is a recognized expert at the forefront of Microsoft advancements and couples these powerful technologies with exceptional services to empower our customers, colleagues, and community.\n\nOUR CORE VALUES\n\nOur employees are driven by our values and know that they make a positive difference every time that they help a customer to solve their challenges. Our focus on delivering great customer experiences empowers our people to build rewarding relationships that contribute to our positive work environment. You can learn more about our culture here: New Signature Culture\n\n\n\nHuman\n\nWe use our hearts and minds to collaborate for success.\n\nWe harness technology to drive business, but we never let that replace our human connections. We use our hearts and minds to collaborate for success and instill confidence in our customers through relationships forged from trust.\n\n\nGenerous\n\nWe are giving and respectful.\n\nWith our efforts to always be generous, we elevate our service level with empathetic and considerate communications and actions. We always find a way to support our customers and colleagues by giving of our time and talent and equally respecting the time and talent of others.\n\n\nAuthentic\n\nWe tell it as it is, with positive intent.\n\nBeing authentic helps to nurture our strong and trusted relationships. We are honest, transparent, and reliable. When you partner with New Signature, you are partnering with a group of purposeful, outcome-driven and results-oriented professionals.\n\n\nInnovative\n\nWe push the boundaries at the intersection of people, process and technology.\n\nFor us, there are no limit to our dreams. We continually innovate and push boundaries at the intersection of people, process, and technology to bring our customers and colleagues the best solutions first.\n\n\nEQUAL EMPLOYMENT OPPORTUNITY\n\nAs a Global Cloud Transformation Consultancy business, New Signature understands diversity and inclusion in the workplace brings benefits to our customers, our business and most importantly, our people. We are committed to being an inclusive employer and we provide equal employment opportunities to all employees and applicants for employment.\n\nNew Signature prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other factors protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including all aspects of the recruiting and employment life-cycle at New Signature.\n\n\nEMPLOYMENT ELIGIBILITY\n\nNew Signature requires the candidate to prove eligibility to work in the United States or Canada (depending on the location of the job) within three days of being employed. All final candidates will be asked to complete a background check in both the US and Canada. These record checks can include any or all of the following: education verification, employment verification, drug screening and criminal record check. Positions that require significant travel may also require a driving record check.\n\nFor US Applicants: New Signature is an E-Verify employer. E-Verify is a web-based system that allows enrolled employers to confirm the eligibility of their employees to work in the United States. E-Verify employers verify the identity and employment eligibility of newly hired employees by electronically matching information provided by employees on the Form I-9, Employment Eligibility Verification, against records available to the Social Security Administration (SSA) and the Department of Homeland Security (DHS). E-Verify is only used upon acceptance of a job offer and completion of the Form I-9.\n\nClick here for the E-Verify Participation Poster (available in English and Spanish)\n\nIf you require this notice in another language provided by DHS, please contact us at 202.452.5923\n\nClick here for more information on Your Right to Work or Visit the USCIS website here for more information on E-Verify\n\nFor Canadian Applicants: New Signature is committed to working with and providing reasonable accommodation to individuals with disabilities. In accordance with the Accessibility for Ontarians with Disabilities Act, 2005 and the Ontario Human Rights Code, New Signature will provide accommodations throughout the recruitment and selection process to applicants with disabilities. To request a reasonable accommodation, please call 416.971.4267. Please ensure to provide your name, the best way to contact you, a detailed description of the nature of any accommodation that you may require (including any materials or processes that can be used to ensure your equal participation).", "job_collect_date": "2020-08-16T16:18:11.000Z"}, "51": {"job_id": "485168fc8141d4c6", "job_title": "Data Engineer (Cloud)", "job_employer": "StackPros", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=485168fc8141d4c6&fccid=efee8d7cab418992&vjs=3", "job_description": "StackPros Inc is seeking a candidate for a full-time role within our Data Systems Team in Toronto, Ontario.\nThe Cloud Data Engineer will play a key role at StackPros, required to help create and maintain industry-leading quality and efficiency of service and software delivery.\nStackPros will rely on the Data Engineer to support the Data Systems team, in both data engineering and data science-related workflows. The Data Engineer will be expected to meet and exceed StackPros\u2019 quality standards, while helping the organization rapidly expand complex Machine Learning and related applications.\nKey Responsibilities:\nData Engineering-Specific Responsibilities\nParticipate in continuous delivery pipeline to fully automate deployment of the highly available cloud platform that supports multiple projects\nDesign and develop ETL workflows and datasets to be used in data visualization tools\nWrite complex SQL queries with multiple joins to automate and manipulate data extracts\nPerform end to end Data Validation to maintain accuracy of data sets\nBuild tools for deployment, monitoring and operations\nTroubleshoot and resolve issues in the development, test and production environments\nDevelop re-useable processes that can be leveraged and standardized for multiple instances\nPrepare technical specifications and documentation for projects\nStay up-to-date on relevant technologies, plug into user groups, understand trends and opportunities to ensure we are using the best possible techniques and tools\nUnderstand, implement, and automate security controls, governance processes, and compliance validation\nDesign, manage, and maintain tools to automate operational processes\nData Science-Specific Responsibilities\nPerform exploratory data analysis to identify patterns from historical data, generate and test hypotheses, and provide product owners with actionable insights\nDesign experiments for product initiatives and perform statistical analysis of the results with recommendations for next steps and future experiments\nCreate and design dashboards by using different data visualization tools to present reports and insights, and support business decision making\nHelp the DRVN Intelligence Data Systems team adopt and evolve Predictive Modeling, Machine Learning and Deep Learning processes to deliver to clients in the future\nQualifications:\n3+ years experience in Data Engineering\nUnderstanding of digital ecosystems including online data collection, cloud systems and analytics tools (Google Stack, Facebook, AWS, Salesforce, Adobe Suite etc.)\nStrong technical understanding of a range of marketing concepts such as cookie-based data collection, setting and leveraging audience segments, attribution modelling, AB/N & multivariate\nExcellent written & verbal communication skills are essential; candidate should be comfortable presenting and participating in group discussions of concepts with internal and external stakeholders\nCandidate must exhibit an analytical, detail-oriented approach to problem solving\nExperience with Jira / Atlassian project management tools is an asset\nCompany-Wide Responsibilities:\nMaintain and exceed client satisfaction with StackPros Inc.\u2019s deliverables, day-to-day work and overall value as a partner\nCultivate opportunities for company growth, always seek areas where StackPros Inc.\u2019s role could be expanded\nAdapt to ever-changing client needs and expectations\nMaintain dedication toward achieving excellence in StackPros Inc.\u2019s delivery against client needs, and overall success as an organization\nBe an enthusiastic, positive and generally awesome team mate, mentor & constantly curious learner", "job_collect_date": "2020-08-16T16:18:11.000Z"}, "52": {"job_id": "25dbbd3fbbdb8ba2", "job_title": "Data Engineer - BI & Data Science", "job_employer": "Paytm", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=25dbbd3fbbdb8ba2&fccid=4e917d9a3b14765d&vjs=3", "job_description": "About Paytm Labs:\nAt Paytm Labs, we build technologies that powers Paytm India, the world's fastest growing mobile payments and commerce ecosystem. We use our skills and our biggest asset \u2013 data, to make our dent in this universe. We are committed to offering the most transparent, secure, and personalized consumer experience to over 400 million users. We believe that this kind of scale, and the unique problems that it presents attracts curious candidates like yourself.\n\nJob Description:\nWe are seeking a Data Engineer with a focus on BI, for our personalization and marketing automation team. This team has an impact on every pixel in our mobile app and drives customer engagement and growth by showing relevant products, services and messaging to our customers. Candidates should have technical skills to build real-time and batch data pipelines, and strong analytical capabilities to delve into and understand trends and patterns in the data. You will be working closely with world class software engineers, machine learning engineers and business teams, and the work you do will have a direct impact on the roadmap of our product.\nResponsibilities:\nBuild and maintain batch and real-time data pipelines to power our analytical reports and dashboards\nAnalysis of historical data to identify trends and support decision making, including written and verbal presentation of results and recommendations\nSet up A/B tests and ensure metrics are reported accurately\nIdentifying data needs and driving data quality improvement projects\nEvangelizing data driven decision making within the team and to business & product owners\nUnderstanding the broad range of Paytm data resources, and knowing the right ones to use for the problems at hand\nTechnial Requirements:\nProficient in at least one programming language like Scala, Java or Python\nExperienced in Apache Spark, or other big data processing frameworks\nProficient in SQL\nProficient in software development, and source code management (Git etc)\nQualifications:\nBS degree or higher in computer science engineering, statistics, mathematics, econometrics, or a similar quantitative field\n3+ years work experience in data engineering, software engineering and/or data analysis\nPrior success in working with extremely large datasets using big data technologies\nDemonstrated ability to directly partner with business owners to understand product requirements\nEffective spoken and written communication to senior audiences, including strong data presentation and visualization skills\nDetail-oriented, with an aptitude for solving unstructured problems\nNice to Haves:\nFamiliarity with AWS services\nExperience with data streaming technologies like Kafka, Spark Streaming\nWhat we Offer!\nWe are proud to announce that we have been certified as a Great Place to Work!\nA collaborative, open work environment that fosters ownership, creativity, and urgency\nEnrolment in the Group Health Benefits plan right from Day 1, no waiting period\nFuel for the day: Weekly delivery of groceries and all types of snacks to our office\nAll types of signature drinks from coffee to lattes to cappuccinos\nCatered lunch and desserts on a monthly basis!\nPing Pong and Pool: Become the next Paytm Labs Table Tennis/ Pool champ!\nAnd so much more!\nNotice for Job Applicants\n\nFollowing the advice of Canadian health authorities, to mitigate the risk of potential spread of COVID-19 and support social distancing, all recruiting activities including interviews and new hire onboarding will be conducted remotely. While we are doing our best to ensure reasonable response times, please expect potential delays during the recruiting process due to the current situation. Thank you for your patience and understanding during these challenging times.\n\nDon't have Paytm Canada App yet?\nCheck us out in the Google Play or App Store.\n\nWe thank all applicants, however, only those selected for an interview will be contacted.\n\nPaytm Labs is committed to meeting the accessibility needs of all individuals in accordance with the Accessibility for Ontarians with Disabilities Act (AODA) and the Ontario Human Rights Code (OHRC). Should you require accommodations during the recruitment and selection process, please let us know. Paytm Labs is an equal opportunity employer.", "job_collect_date": "2020-08-16T16:18:12.000Z"}, "53": {"job_id": "281e0356c31acae9", "job_title": "Data Engineer", "job_employer": "Capgemini", "job_location": "Brampton, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=281e0356c31acae9&fccid=105ecfd0283f415f&vjs=3", "job_description": "Job Description:\n3-6 yearsExpertise on Spark Scala.Ability to develop ETL jobs to implement business logic using Scala (Spark Framework)Conversant with Hive Database, Able to create HQL scripts and work on Hive tables for data analysisPerformance tuning of the existing Hadoop jobs, able to trouble shoot and fix existing bugs.Good understanding of Oracle Exadata RDBMS, able to profile telecom data residing in Exadata and derive business rules.Co lace with business, have working session with business to identify and freeze business logic.Understanding / experience working on scrum based Agile set up.\n\nThe Capgemini Freelancer Gateway is enabled by a cutting-edge software platform that leads the contingent labor world for technology innovation. The software platform leverages Machine Learning and Artificial Intelligence to make sure the right people end up in the right job.\n\nA global leader in consulting, technology services and digital transformation, Capgemini is at the forefront of innovation to address the entire breadth of clients\u2019 opportunities in the evolving world of cloud, digital and platforms. Building on its strong 50 year heritage and deep industry-specific expertise, Capgemini enables organizations to realize their business ambitions through an array of services from strategy to operations. Capgemini is driven by the conviction that the business value of technology comes from and through people. It is a multicultural company of over 200,000 team members in more than 40 countries. The Group reported 2018 global revenues of EUR 13.2 billion.", "job_collect_date": "2020-08-16T16:18:12.000Z"}, "54": {"job_id": "93a28de264933288", "job_title": "Senior Data Engineer", "job_employer": "Nomis Solutions", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=93a28de264933288&fccid=13785db0e9e260f0&vjs=3", "job_description": "Senior Data Engineer\n\nNomis is looking for an outstanding data expert to join our team. The Data Engineer will collaborate closely with our client services team to process critical data while working to power advanced analytics and enable the integration of data science across the company. You are ready to be flexible and nimble in your work, from constructing ETL pipelines for customer delivery to participating in exploratory data analysis with our Analytics team.\n\nWho We Are & What We Build\n\n\nWe partner with Banks and FinTechs on their journey to best-in-class pricing technology and analytics so that they deliver more value to their customers, employees and shareholders. Our top-notch people, proven technology, and innovative analytics are tackling big data challenges at banks and lenders every day. We deliver market-leading cloud-based Pricing & Profitability Management solutions and insights for the Banking & Financial Services industry leveraging cutting-edge behavioral data science. We are a Blue Chip venture-backed company with the vision to transform the consumer banking landscape.\n\nResponsibilities\n\nEstablish and maintain big data processing platform\nBuild data management applications and microservices on AWS\nDesign and implement Hive/Greenplum/RedShift distributed data warehouses and standard schemas\nDesign, develop, maintain cross-platform ETL processes and MapReduce/Hive/Spark data processing workflows\nManage and maintain reference data securely on S3 and other storage systems\nSupport client services teams by\nManage, customize, and automate cloud-based (AWS) data processing supporting multiple clients\nAdministration of relational databases, capacity plans, infrastructure and storage design\nOversee and execute data migration from existing data stores\nApplication/implementation of custom analytics applications and datasets\nDevelop code standards, guidelines, and automated test suites to ensure highest data quality and integrity\n\nDesired Skills and Requirement\n\nExperience with building distributed systems, query processing, and the Hadoop ecosystem\nUnderstanding of Data warehousing - architect and design data warehouse\nExpertise with data schema - logical and physical data modeling\nKnowledge of ETL processes and tools\nExperience with AWS or a major cloud platform such as GCP\nProficiency in: Python, SQL, Java\n\nStrong pluses:\n\nExperience of Business Intelligence tooling such as Tableau\nExperience with data mining techniques and analytics functions\nPredictive analytics experience is a PLUS\nExperience with Spark 2, Apache Airflow and other modern data engineering tooling a strong plus\nExperience with streaming architectures and MPP databases such as Greenplum a strong plus\nUp-to-date with the open-source community w.r.t. data engineering\nExperience with the following services in AWS a strong plus: EMR, Lambda, Kinesis, Firehose, S3", "job_collect_date": "2020-08-16T16:18:13.000Z"}, "55": {"job_id": "668db5396375205e", "job_title": "Big Data Developer", "job_employer": "CGI", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=668db5396375205e&fccid=d2841a5c0380b93d&vjs=3", "job_description": "", "job_collect_date": "2020-08-16T16:18:14.000Z"}, "56": {"job_id": "33c214e6ad4ee9e2", "job_title": "Senior Big Data Engineer", "job_employer": "The Economical Insurance Group", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=33c214e6ad4ee9e2&fccid=d33243ac6c394a10&vjs=3", "job_description": "Senior Big Data Engineer\nLOVE WHAT YOU DO\nWork is a big part of our lives, so it makes sense to choose a company that offers truly rewarding work. At Economical Insurance\u00ae, your contributions, creativity, and energy won\u2019t go to waste. Our passionate culture and pioneering mentality infuse everything we do, which is why our one-million-plus policyholders know we\u2019ll protect their homes, businesses, farms, cars, and pets like they\u2019re our own.\nWe\u2019re not afraid to see how far we can push the envelope to make insurance better. Our family of companies includes Sonnet, the only Canadian home and auto insurer offering a fully online buying experience, and Petline, the largest Canadian pet insurance company. We\u2019re also preparing to become a publicly traded company, a once-in-a-lifetime career opportunity for everyone who joins our team.\nIf you\u2019re looking for a company that takes care of its people \u2014 and its customers \u2014 and has a track record of doing big things, get ready to love it here.\nThe Senior Big Data Engineer is accountable for the design and development of quality big data solutions for the operational and analytical business needs within Economical. Your expertise in Hadoop, Spark and related technologies will be used to optimize our current big data solution, as well as meet the future business needs. This role requires you to partner with IT, Analytics, Business Intelligence and the business lines to research, analyze, design and develop data solutions ranging in small to very highly complex.\nWhat can you expect in this role?\nDesign and develop Big Data solutions for both operational and analytical requirements using Hadoop Open Source frameworks\nDesign, develop and integrate data ingestion, ETL/ELT data pipelines and processing/transformation processes\nWork with senior stakeholders to develop a clear understanding of requirement drivers\nAddress non-functional requirements including performance, data governance, scalability, continuous integration, migration and compatibility\n\nWhat do you bring to the role?\nBSc in Computer Science, Engineering or related fields\nExpertise with Hadoop distributed frameworks handling large amount of data using Spark and Hadoop ecosystems\nExperience Hadoop technologies such as Oozie, Kafka, Druid, Hive, Storm, Ignite, Kudu, NiFi\nExperience with data loading tools like Flume, Sqoop, as well as different layers of Hadoop Framework \u2013 Storage (HDFS, HBASE), Analysis (Hive/Kudu/Druid/Impala etc.), Map Reduce Jobs\nMust have hands on Spark/Scala experience\nAdvanced programming skills (Python, Java, Scala or R preferred)\nStrong experience with ETL tools\nMinimum of 4 -5 years of experience handling variety of data (structured/unstructured), data formats (flat files, XML, JSON, relational, legacy) and data storage (HDFS, Hbase, NoSQL databases)\nStrong communication skills (verbal and written) with ability to communicate across teams, internal and external at all levels\nExperience in the financial sector, and specifically the P&C Insurance industry is considered a strong asset\nFirm understanding of database systems \u2013 data modelling, SQL and transactional processing\nExperience with API management best practices\nDevOps and Agile collaboration knowledge are necessary (i.e. security, testing, version control, continuous integration, testing, etc.)\nExperience with developing big data solutions in the cloud (AWS, GCP or Azure) would be great\nThis role can be in our Kitchener or Toronto location\n\nWe also take potential into consideration. If you don\u2019t have this exact experience, but you know you have what it takes, be sure to give us more insight through your application and cover letter.\nGo ahead and expect a lot \u2014 you deserve it.\nWe offer:\nCompetitive salaries, with potential for an annual raise and bonus\nPension and savings programs, with company-matched RRSP contributions\nGenerous time away, including vacation and personal needs days\nPaid volunteer days and company matching on charitable donations\nEducational resources, tuition assistance, and paid time off to study for exams\nTwo annual wellness campaigns \u2014 participants earn up to $300 each year to spend on almost anything supporting health and work-life balance (think things like spa days, daycare, pet grooming)\nAn unlimited employee referral bonus program\nFlexible work schedule\nDiscounts on products and services\n\nHOW TO APPLY\nTo complete the online application process, you\u2019ll need to upload your resume and cover letter in one document. The posting will close at midnight on the deadline date; in order to successfully apply, please ensure your application is submitted by 11:59 p.m. the day before the deadline.\nOur inclusive work environment welcomes diversity and supports accessibility. If you require accommodation at any time during the recruitment process, please let us know by contacting: hrsharedservices@economical.com.\nVisit economical.com to learn more about us and what we\u2019re up to.", "job_collect_date": "2020-08-16T16:18:17.000Z"}, "57": {"job_id": "f0322339724635eb", "job_title": "Data Engineer", "job_employer": "MobSquad", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=f0322339724635eb&fccid=e140fb6d05504c9a&vjs=3", "job_description": "ABOUT MOBSQUAD\n\nWe are a well-funded, hyper-growth, scale-up looking for an experienced Data Engineer. If you've ever dreamed of working with a top tier technology company scale-up, on leading edge technologies, backed by the very best venture capitalists in the world, then this is your chance.\n\nSome details about MobSquad:\n\nMobSquad solves the significant and growing technology talent shortage faced by US-based start-ups and scale-ups by enabling our clients to quickly have a turnkey \"virtual\" Canadian subsidiary, where Canadian-based software engineers serve our clients individually on an exclusive basis\nWe've been featured on the front page of The Washington Post, on NPR multiple times, The Financial Times (UK), The Globe and Mail, the Calgary Herald, BetaKit, CBC, Global News, and many other places. other media outlets\nWe're a Certified B Corporation, were recognized as the third Best Place to Work in Canada in 2020, and have made numerous contributions to charitable organizations as well as a financial commitment to the Upside Foundation. We believe we are playing a key role in enhancing Canada's innovation economy, and have received financial support from the Government of Canada, Province of Alberta, Province of Nova Scotia, and City of Calgary, to support this ambition\nYou can learn more about us on our website\n\nABOUT THE ROLE\n\nAs a Data Engineer, you will be part of a Canada-based team working remotely for a leading US scale-up. Your team will operate alongside many other talented developers and data scientists in Canada, and you will be an integral part of the tech community that MobSquad has built.\n\nThis role requires someone who has demonstrated an ability to develop, test, optimize, and maintain scalable databases, architectures, and pipelines that enable data scientists and software developers to easily analyze and work with data. The ideal candidate has worked closely with data scientists and data architects and is an expert at optimizing data flow for use across broader teams. The candidate should be able to generate ideas and create tools that add greater functionality and usability to data systems within the company.\n\nABOUT YOU\n\nYou have a bachelor's degree in Computer Science, Information Technology, Data Science, Applied Math, Physics, Engineering, or a comparable analytical field from an accredited institution\nYou are expert in modeling, working with database architectures, and relational databases\nYou have over three years of experience with big data tools, such as Hadoop (MapReduce, Hive, Pig), Spark, and Kafka\nYou have experience with SQL databases (PostgreSQL, MySQL) and NoSQL databases (Cassandra, MongoDB)\nYou have experience creating and working with ETL data transformation and integration processes\nYou have experience working with enterprise-grade cloud computing platforms such as Microsoft Azure, Amazon Web Services (EC2, EMR, RDS, Redshift), and Google Cloud\nYou have expertise in relevant programming languages (Python, R, C/C++, Java, Pearl, Scala)\nYou have a deep understanding of data modeling tools (ERWin, Enterprise Architect, Visio)\nYou have experience optimizing big data pipelines and extracting value from large disconnected datasets\nYou have the ability to develop high-quality code adhering to industry best practices (i.e., code review, unit tests, revision control)\n\nWHAT YOU'LL GET @MOBSQUAD\n\nA full-time position that offers competitive compensation\nA benefits program delivered through our bespoke digital platform, giving you control, choice, and flexibility. We give you the ability to build your package of benefits covering health (e.g., medical, dental, vision), wellness (e.g., gym, workout gear, massage, transit), and RRSP (retirement savings)\nA downtown office location with first-rate amenities, surrounded by great restaurants and easily-accessible transit\nFor international candidates, sponsorship for an immediate work permit, expedited permanent residency, and Canadian citizenship within four years\n\nAt MobSquad, we support and encourage building a work environment that is diverse, inclusive, and safe for all. We invite and welcome applicants of all backgrounds, regardless of race, religion, sexual orientation, gender identity, national origin, or disability.", "job_collect_date": "2020-08-16T16:18:18.000Z"}, "58": {"job_id": "e8f01b72a171ac29", "job_title": "Data Engineer", "job_employer": "goeasy", "job_location": "Mississauga, ON", "job_link": "https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DbNJQGwjC-NEA80-qlVyEGmSv34Xx0eW4t4u9rXSv_7_tlvMU2e-Gw_wrZ47s1zPCnrySU2yuEL07wtSyx7XOwYh5VPcVL7CxRhOYwHzR-CdmyBrwbeqommOn6y9FD_uKIYpUCb9-PC_XgwGzBDrPy7SD7JQndrQdnAb_vhLXKdWb4ncEcmJFloxMzLpi-yE5BjaU_ZnSw2JYV9YCrayBL3m2ZyLxTZRsGW7JRGGNr08YyZyigvQvXi_DutWTW39qAx8cNAkVl0DxQM96EWdTk-iK7b4I4jGA5i_95QcLow3uZegxPlhxrbW1haWrKGYlqJucOZe8RvGAVHJt4a6pTAEYu_LCHtBYmr-erqCnkIN5xweGMdLKcxUqLdAbnrqmII1f_t21c77rx68yOddH8MAFhRuz2k7KLCx8AivgskMV9OYB7PTYj-14ZdeEARWYjQHezBHvkgyTj6j4J4DFrwJhPaeX_XGQ0rczacoPFjQ==&p=12&fvj=0&vjs=3", "job_description": "If you are looking to join one of Canada\u2019s fastest growing companies, goeasy Ltd. is the place for you! Recognized as one of Canada\u2019s Most Admired Corporate Cultures, one of Canada\u2019s Top 50 Fintech\u2019s and one of North America\u2019s Most Engaged Workplaces, we want the best and brightest to join our team.\n\nWe are a publicly traded company on the TSX with over 4000% shareholder return since 2001, goeasy operates two main business units. easyfinancial is our consumer lending business that offers secured and unsecured installment loans of up to $35,000 and easyhome is Canada\u2019s largest merchandise lease-to-own company. It is our mission to provide everyday Canadians the chance for a better tomorrow, today by giving them access to the credit they need and by offering them a second chance when they have been turned down by banks and traditional lenders. With a retail network of nearly 400 locations across Canada and over 1900 employees, we are able to build lasting relationships with our customers as we help them rebuild their credit and graduate towards prime rates and a brighter financial future.\n\nThe Data Engineer on the Data Science and Business Insights team will build, integrate data from various resources, manage data in goeasy operational data store and enterprise data warehouse. This position will develop ETL (Extract, Transform and Load) with various tools on large datasets to ensure data is easily accessible, works smoothly, as well as maintain and expand the data warehouse for reporting and analysis. The Data Engineer will also work closely with the data architect on the design and architecture of our enterprise data warehouse.\n\nResponsibilities:\n\nDevelop data set processes for data modeling, mining and production\nDevelop and maintain ETL processes using SSIS, Scripting and data replication technologies\nParticipate in development of datamarts for reports and data visualization solutions\nResearch opportunities for data acquisition and new uses for existing data\nIntegrate new data management technologies and software engineering tools into existing structures\nSupport the translation of business requirements for data acquisition/manipulation and provide detailed specifications that can be passed downstream for use\nDevelop detailed technical specifications and operational support documentation in collaboration with Business Systems Analysts, BI Engineers and Architects.\nIdentify and communicate technical problems, process and solutions\nCreate Ad-Hoc queries and reports as needed along with providing on-going analytical support for these requests\nAssist in the collection and documentation of user\u2019s requirements\nEnsure that existing business processes dependent on the ODS/EDW are monitored and respond quickly to bug fixes, enhancement requests and production ETL related issues.\nDealing with the database users on a daily basis to ensure that problems are dealt with promptly and that appropriate fixes are made to resolve any problems.\nRecommend ways to improve data reliability, efficiency and quality\nEnsure systems meet business requirements and industry practices\nWork effectively with the Business Intelligence and Data Solutions Architects, Data and BI Engineers to ensure that all approved development and deployment procedures are followed.\n\nQualifications:\n\nBachelor\u2019s Degree in Computer Science, MIS, Computer Engineering or other Information Technology related degree\n4+ years working with SQL Server or comparable relational database system\n3+ years of extensive ETL development experience with SSIS and/or ADF\n4+ years of experience troubleshooting within a Data Warehouse environment\nExpert domain knowledge & experience in Data warehousing, encompassing data model design, dimensional modeling, naming conventions, cross-cutting concerns, common integration technologies, patterns & standards and emerging technologies.\nExpert Knowledge of SQL skills to build, debug, and optimize (developing procedures, functions, SQL queries, etc.) and working with large data sets and to perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.\n2+ years SQL Server Database administration experience\nCloud experience (Azure) is highly preferred\nExposure and experience with Python, R, Hadoop, Azure, other Big data and advanced analytics\nKnowledge of AI and ML developments/solutions/implementations\nExperience with multiple programming languages (PowerShell scripting, C#, others) with basic scripting skills.\nHigh level of technical aptitude\n\nInclusion and Equal Opportunity Employment\n\ngoeasy is an equal opportunity employer. In addition, goeasy is committed to providing accommodations for applicants upon request at any stage of the recruitment process in accordance with all legislative requirements throughout Canada. Please let us know if you require an accommodation due to a disability during any aspect of the recruitment process and we will work with you to address your needs.\n\nAdditional Information:\n\nAll candidates considered for hire must successfully pass a criminal background check, credit check, and validation of their work experience to qualify for hire. We thank all interested applicants, however we will only be contacting those for interview who possess the skills and qualifications outlined above.\n\nWhy should you work for goeasy?\n\nTo learn more about our great company please click the links below:\n\nPAID1234", "job_collect_date": "2020-08-16T16:18:18.000Z"}, "59": {"job_id": "997673b8effa647f", "job_title": "Data Engineer (Intermediate 4-6 years) - Contract - Remote during COVID", "job_employer": "CorGTA", "job_location": "North York, ON", "job_link": "https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Dkh866153otbRJ8nVMuEXnXPd315JS377__3VyD6HnNQkmcaOHMemmy5pLrNV6j0WaVWEWWabnp4PP6-2u77DLfT_FCIGvCW8CQmQW140Z7EVU32Jnwl42FALsxnbc4n5M6FOTfoW2OoTMOcGG21HZGOoXL7fWT8eaT67VlcClh4sHQcJsHCX_A9fpFnZYPmoqfiROhS65PnG7oKeU5Ex5jge2LOcXXrLpeY0SD4V8vlSjkePRWvVFy6tVozIM8VhLg81KqSfLljrvELDfjAjFDxS1FS-Q5yMpOwJORtl3E1dIUDwtGYSgvvjArbXMkkba8O-HfupVG1W4p2ykuIDfkJQN--y7AQ4_49yibxGA_pg0tSZvGTI9PArZjB_C0MvCzR8mBggt8WhMZgMYINYxYY6moDltgyltu2GTnwphFjvY2itzauFLTSATezrNm0LJlM8Vr1Xqr-6OT1EOI11dUnajYjEZiX2n0GqFZpPs5PhbzrWfnutG&p=13&fvj=1&vjs=3", "job_description": "Role: Data Engineer (Intermediate 4-6 years)Structure: Contract (6 months initially)Location: North York, ONHours: Monday - Friday 40 hours (remote during COVID-19)Pay: $50.00 p/h inc.The role: We are looking for Intermediate Data Engineers who have roughly between 4-6 years experience who ideally has experience moving data from Google cloud over to Big query.However, if you are someone who has good experience with ETL and Data Pipeline Design & Data Validation experience and can script in Python and create strong SQL Queries then this will also be a potential match for this role.Ideal Qualifications to be successful in this role: - Degree in Computer Science, Engineering, Information Systems (or equivalent combination of skill and experience)- 4-6 years of experience working with data architecture projects- Excellent SQL coding and experience with a broad array of development tools and platforms including exposure to a big data environment tools/languages, such as SQL, R, SAS, Python, etc.- Experience creating relational database design and data models- Experience with various analytical data platforms and technologies- Experience with using REST API, Cloud (GCP Ideally)- Experience in custom or structured data integration design, implementation, and maintenance- Experience with business intelligence tools such as Qlikview, Tableau and Microstrategy (Nice to have for Data Visualizations)- Previous experience working in retail or eCommerce is highly preferred- Ability to curate data to tell stories and provide business insightsPlease apply with an updated resume and ensure the required skills you are able to speak to for this position are included.For more roles like this please go to www.corgta.com/find-a-job/Job Type: Full-timeSalary: $45.00-$50.00 per hour", "job_collect_date": "2020-08-16T16:18:19.000Z"}, "60": {"job_id": "54e0fcc38385e3a2", "job_title": "Data Engineer/Integration Consultant", "job_employer": "Copperstone Connect", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Ceb7pqR8iGmLuXIZQklF6pqed3xddONNM6Jjumj-ndyGK0kULQUmQ2REqfJ5KAzFVeK_es1WVg0s1sLgKW09UsvVRp0-dSizFQr1oNyK_VIqdjla00VU9GNVvu2x4v9p3hsL7NeF-6hBksbAaSgKazsaYTODGMKQQeVuYcxoOuxvFKuD0HRLD9ybgbcXTn1LPRV14kT34pV9vwmDhKEnSO41Kf81Nxnx-Jud3NT0mbfEBy_I7AW-kmQNQOQ_or_zdqUBvyBEky4lxfBZr4QZxCnYS-DOkMaYB1tDzXYIFUec0exUHJgCcaI9Ee_5axAxgxCJxczLwZ23RkDYqq-a44We50GLzM1MWIAm1gczpAfvsfA36AC0A5BpNlCI8-brLzjwCf5vSoVx_9UhpLveZD1vaiHcg0afN9XU9WSrBc0wPLd0ko5c7ofzlWMK0McbQryAz_2yvVbnfzyWH2WKJ0dV4cg3wCqnvsJ1u2mMTwbdqMk43RR2QVP86TG2Hf0bUOxCzh9WPZ-IMnIkw4xHJ7Y3tBGjlt2XHjghppIEEHdlzNDnE-vDOn6RndI7S2UAwwPrcM_L1EyjE_E0xVDf6h09EKsvCeTHu2ghAdbhmOrpR4ALuujcI6AAjbBRCskv9glUB5jWH-jnfqotOb6E_fmRHZtE0NdG7MxTOl9GCQeA==&p=14&fvj=0&vjs=3", "job_description": "Our client, a boutique-size consulting firm, prides itself on being at the forefront of innovation in the Big Data space. Founded in 2010, they provide thought leadership and implementation excellence within the ever-growing data and analytics world.\nThey take pride in having some of the most highly trained and experienced consultants in the industry which translates into optimal value for their clients. They were one of the first companies to provide analytics and data as a service, via the cloud, as early as 2010. They strive to make sure their customers are well-positioned with the best technologies/tools in the industry, constantly evaluating new and existing technology partnerships. Some of the more prominent companies they have partnered with include; Snowflake, DataRobot, MicroStrategy, Informatica, Amazon AWS & Microsoft. They continue to invest in their most valuable resource, their people. They do this through extensive training both on the job and through various educational programs.\nDue to growth, they are seeking to hire an Intermediate - Senior Integration Consultant.\n\nDesired Skills and Experience\n: University/College degree in Computer Science, Mathematics, Data Science and/or Relevant Degree\n: 5+ years hands-on development, configuration, scripting and administration experience with Data Integration platforms. (i.e. Informatica, Talend, DataStage, SSIS)\n: BI Experience (MicroStrategy, Looker, Tableau, PowerBI) considered a nice to have\n: Extensive theoretical and practical knowledge of data warehousing principles/concepts and practical development experience in all areas of the data warehousing life cycle\n: Experience with Data Management, ETL, Cloud Data (AWS), Data Integration\n: Knowledge of OLAP-related principles and concepts\n: Strong grasp of data modeling techniques and concepts (Normalized/Denormalized, Conceptual/Logical/Physical, Star, Snowflake, Data Vault)\n: Strong knowledge and experience with relational databases such as Snowflake, SQL Server, Oracle (Advanced knowledge of reading and writing SQL, Performance analysis and tuning)\n: Knowledge and experience with key Big Data technologies (Hive, Presto, Spark, Kafka, NoSQL databases, Semi-structured data access patterns (Json, Parquet, XML, etc.))\n: Strong Python scripting skills\n: Excellent communication skills\n: Great problem-solving skills\n: Leadership and good client management skills\n\nDay to Day Activities Would Include\n: Conduct relevant customer interviews to determine key business requirements and objectives\n: Build appropriate analytical data models based on outcomes of user interviews\n: Analyze and profile data systems to build source to target data mappings\n: Review ETL performance and conducts performance tuning as required on mappings/workflows or SQL\n: Administration and support of data integration infrastructure\n: 2nd level on-call support of ETL services as required\n\nYou will be responsible for attaining the following goals:\n: Attaining a minimum of 1 new accreditation/certification per year\n: Spending 80% or more of their time on billable work\n: Completing 90% or more of their agile delivery tasks on time\n: Demonstrating competency in 1 new relevant technology every year", "job_collect_date": "2020-08-16T16:18:20.000Z"}, "61": {"job_id": "f0322339724635eb", "job_title": "Data Engineer", "job_employer": "MobSquad", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=f0322339724635eb&fccid=e140fb6d05504c9a&vjs=3", "job_description": "ABOUT MOBSQUAD\n\nWe are a well-funded, hyper-growth, scale-up looking for an experienced Data Engineer. If you've ever dreamed of working with a top tier technology company scale-up, on leading edge technologies, backed by the very best venture capitalists in the world, then this is your chance.\n\nSome details about MobSquad:\n\nMobSquad solves the significant and growing technology talent shortage faced by US-based start-ups and scale-ups by enabling our clients to quickly have a turnkey \"virtual\" Canadian subsidiary, where Canadian-based software engineers serve our clients individually on an exclusive basis\nWe've been featured on the front page of The Washington Post, on NPR multiple times, The Financial Times (UK), The Globe and Mail, the Calgary Herald, BetaKit, CBC, Global News, and many other places. other media outlets\nWe're a Certified B Corporation, were recognized as the third Best Place to Work in Canada in 2020, and have made numerous contributions to charitable organizations as well as a financial commitment to the Upside Foundation. We believe we are playing a key role in enhancing Canada's innovation economy, and have received financial support from the Government of Canada, Province of Alberta, Province of Nova Scotia, and City of Calgary, to support this ambition\nYou can learn more about us on our website\n\nABOUT THE ROLE\n\nAs a Data Engineer, you will be part of a Canada-based team working remotely for a leading US scale-up. Your team will operate alongside many other talented developers and data scientists in Canada, and you will be an integral part of the tech community that MobSquad has built.\n\nThis role requires someone who has demonstrated an ability to develop, test, optimize, and maintain scalable databases, architectures, and pipelines that enable data scientists and software developers to easily analyze and work with data. The ideal candidate has worked closely with data scientists and data architects and is an expert at optimizing data flow for use across broader teams. The candidate should be able to generate ideas and create tools that add greater functionality and usability to data systems within the company.\n\nABOUT YOU\n\nYou have a bachelor's degree in Computer Science, Information Technology, Data Science, Applied Math, Physics, Engineering, or a comparable analytical field from an accredited institution\nYou are expert in modeling, working with database architectures, and relational databases\nYou have over three years of experience with big data tools, such as Hadoop (MapReduce, Hive, Pig), Spark, and Kafka\nYou have experience with SQL databases (PostgreSQL, MySQL) and NoSQL databases (Cassandra, MongoDB)\nYou have experience creating and working with ETL data transformation and integration processes\nYou have experience working with enterprise-grade cloud computing platforms such as Microsoft Azure, Amazon Web Services (EC2, EMR, RDS, Redshift), and Google Cloud\nYou have expertise in relevant programming languages (Python, R, C/C++, Java, Pearl, Scala)\nYou have a deep understanding of data modeling tools (ERWin, Enterprise Architect, Visio)\nYou have experience optimizing big data pipelines and extracting value from large disconnected datasets\nYou have the ability to develop high-quality code adhering to industry best practices (i.e., code review, unit tests, revision control)\n\nWHAT YOU'LL GET @MOBSQUAD\n\nA full-time position that offers competitive compensation\nA benefits program delivered through our bespoke digital platform, giving you control, choice, and flexibility. We give you the ability to build your package of benefits covering health (e.g., medical, dental, vision), wellness (e.g., gym, workout gear, massage, transit), and RRSP (retirement savings)\nA downtown office location with first-rate amenities, surrounded by great restaurants and easily-accessible transit\nFor international candidates, sponsorship for an immediate work permit, expedited permanent residency, and Canadian citizenship within four years\n\nAt MobSquad, we support and encourage building a work environment that is diverse, inclusive, and safe for all. We invite and welcome applicants of all backgrounds, regardless of race, religion, sexual orientation, gender identity, national origin, or disability.", "job_collect_date": "2020-08-16T16:18:21.000Z"}, "62": {"job_id": "25bfaface01113b7", "job_title": "Intermediate Data Engineer", "job_employer": "QUESTRADE INC", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=25bfaface01113b7&fccid=dd16cd5f7dbbced2&vjs=3", "job_description": "The Intermediate Data Engineer works in one of the agile BI teams.\nThe ideal candidate will be an experienced Data Engineer that demonstrates in-depth knowledge and understanding of data warehousing, data integration, reporting and business intelligence. Open-minded and flexible and prepared to work in a very dynamic environment, supporting multiple business units.\nJOB RESPONSIBILITIES:\n Creating, supporting, and maintaining ongoing operational, managerial, and executive business intelligence infrastructure.\n Attention to detail, in particular as it relates to compliance and accuracy of data.\n Developing understanding of information sources and correct interpretation of data\n Gathering, documenting and analyzing requirements from stakeholders\n Meeting and interacting with all levels of management as needed to elicit, define, analyze and document requirements for new business intelligence initiatives.\n Designing the conceptual, logical and physical data models necessary to support new reporting and data analysis\n Developing data integration processes\nQUALIFICATION:\n Minimum 3 years of related experience.\n Understanding of Data Warehouse lifecycle is a must.\n Good knowledge in cloud technologies (preferably GCP)\n Advanced knowledge in Python scripting language\n Good knowledge in Message Broker systems (Kafka, RabbitMQ, PubSub)\n Excellent proficiency in writing SQL queries.\n Advanced proficiency with Microsoft BI Suite - SQL Server 2014-2019, SSIS, SSRS.\n Understanding relational and dimensional data modeling concepts.\n Strong knowledge and comprehension of technology and data management used in the process of collecting, storing and retrieving data.\n Post-secondary education, preferably in Math/Statistics or Computer Science.\n Superior writing, editing, and communication skills, capacity to interact with all levels of the organization.\n Knowledge of latest Microsoft self-service BI tools \u2013 Power BI (both desktop and cloud) an asset.\n Experience with DAX an asset\n Experience and/or personal interest in the financial industry an asset.", "job_collect_date": "2020-08-16T16:18:22.000Z"}, "63": {"job_id": "5328ae30e6b04e6f", "job_title": "Data Engineer", "job_employer": "Points International", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=5328ae30e6b04e6f&fccid=5c98f9ffc20e640f&vjs=3", "job_description": "Company Description\n\nWe\u2019re a technology company working in the loyalty e-commerce industry. Our solutions enhance the management and monetization of loyalty currencies for more than 50 of the world\u2019s largest loyalty brands, from frequent flyer miles and hotel points to retailer and credit card rewards. Supported by our unparalleled loyalty industry experience and technological expertise, we bring state-of-the-art loyalty commerce platforms and products to individuals and businesses in today\u2019s loyalty marketplace.\nOur casual, collaborative office is where our strong workplace culture begins. Our people are what make us great, so we empower them with the freedom to think big and the resources to make things happen. We communicate directly, lead by example, and make sure our team members know how much they are appreciated. Passion for life and work is important to us, and we want to see it in you, too!\n\nJob Description\n\nPoints is looking for a Data Engineer to join our Data Engineering team for a permanent position in our downtown Toronto office.\nWe\u2019re an industry-leading web-based organization that is continuously reshaping how consumers interact with their loyalty programs. We work with the world\u2019s largest airline, hotel, financial, and retail rewards programs, to tackle complex challenges and come up with innovative e-commerce solutions. If you\u2019d like to be a part of it, we\u2019d love to hear from you.\nReporting to the Team Lead, Data Engineering, you will:\nWork in a scrum based team that is passionate about data.\nDesign and develop scalable pipelines for data consumption by downstream applications and for reporting purposes.\nImprove upon existing ETL processes and monitoring to maintain data integrity and accuracy.\nAutomate the boring manual stuff, preferably using Python.\nSupport production systems to ensure a high degree of data availability.\n\nQualifications\n\nExperience using GUI ETL tools (we use Talend).\nStrong knowledge of SQL.\nExperience with pub/sub architectures, such as Kafka.\nExperience with containers and related infrastructure, such as Docker and Kubernetes.\nSelf-discipline and willingness to learn.\nNice to haves\nGood knowledge of general software engineering principles and practices.\nExperience with columnar-oriented databases, such as Vertica.\nExperience integrating with services, such as Dataiku and NetSuite.\nWorking knowledge of Continuous Integration and Continuous Deployment concepts.\nAdditional Information\n\nBuilding a great company culture is as vital to us as building a great business. Over the last 5 years Points has been the recipient of the following awards:\nBest Workplaces (Medium) in Canada\nBest Workplaces for Women.\nCanada\u2019s Top Small and Medium Employers\nGreater Toronto\u2019s Top Employers\nHere are some of the perks that are included in our Points culture:\nCentral downtown location in the Financial District\nConnected to the PATH network of shops/restaurants\nWe want to celebrate with you: all employees get an extra day off for their Birthdays!\nFlexible work hours and casual dress every day\nMarvelous Snack Cart Fridays: free refreshments and snacks!\nFree coffee, tea, juice, pop, and snacks\nMonthly subsidized lunch program\nGreen commuter and fitness subsidies\nSecure bike storage with showers and towel service\nCompany-sponsored activities: bowling, movies, sports, paintball, and more!\nPoints is an equal opportunity employer and is committed to providing an accessible recruitment process. Upon request we will provide accommodation for applicants with disabilities.\nAll your information will be kept confidential.\nNo agencies please.", "job_collect_date": "2020-08-16T16:18:23.000Z"}, "64": {"job_id": "20480654089beb5a", "job_title": "CATO - Big Data Engineer", "job_employer": "CAPCO", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=20480654089beb5a&fccid=c2a63affe8751868&vjs=3", "job_description": "Big Data Engineer\nLOCATION: TORONTO\n\nCapco \u2013 The Future. Now.\n\nCapco is a distinctly and positively different place to work. Much more than consultants, we are active participants in the global financial services industry. Our passionate business and technology professionals enjoy a unique environment where they are actively encouraged to apply intellect, innovation, experience and teamwork. We are dedicated to fully supporting our world class clients as they respond to challenges and opportunities in: Banking, Capital Markets, Finance Risk & Compliance, Insurance, and Wealth and Investment Management. Experience Capco for yourself at capco.com\n\nLet\u2019s Talk About You\n\nYou want to Own Your Career. You\u2019re serious about rising as far and as fast as your work and achievements can take you. And you\u2019re ready to write the next chapter of your career story: a challenging and rewarding role as a Capco Big Data Engineer.\n\n\nLet\u2019s Get Down To Business\n\nCapco is looking for talented, innovative, and creative people to join our development team to work on a number of projects and applications with a Data focus within the Digital practice.\n\nFitting that description, you will also need to be personally motivated to work in a team where clients become colleagues too.\n\nResponsibilities\n\nProduces high quality complex, deliverables with minimal input from stakeholders\nManage full software lifecycle for medium complexity projects from requirements, to design, to implementation, to testing\nDevelop and maintain back end solutions using cutting edge technologies and products\nWork with Scrum Masters and product owners to priorities and deliver solutions using an Agile environment\nBuild reusable code and libraries for future use and follow emerging technologies\nMentor and train junior developers\n\nEducation/Experience\n\nBachelor\u2019s degree (preference given to Computer Science, Engineering, Gaming and STEM-based majors) or equivalent experience\nFive (5) or more years of experience as a Full-stack Data Engineer/developer on Data driven projects\nStrong understanding of the full development lifecycle including requirements, architecture, design, development and testing\nStrong development experience with Scala/Spark\nExperience working with REST APIs/Springboot.\nFamiliarity working with Java and Hive.\nAbility to balance competing priorities in a very dynamic and fast-paced environment\nExcellent detail-oriented, problem solving skills and the ability to quickly learn and apply new concepts, principles and solutions\nMust have excellent communication skills (verbal and written)\n\nShow Us What You\u2019ve Got\n\nIt will be very useful if you have some or all of the following skills:\n\nUnderstanding of big data and distributed programming concepts\nExperience working with ASW, GCloud, Docker, Kubernetes\nExperience working with Microservices, CQRS, EventSourcing\nExperience working with Spring, Akka, Spark\nExperience working with Reactive Streams (Rx, Akka, Reactor)\nStrong organizational and communication skills\nExperience working in an Agile environment\nExperience working with code versioning tools\nExperience working with build, packaging and continuous integration tools and frameworks\n\n\nProfessional experience is important. But it\u2019s paramount you share our belief in disruptive innovation that puts clients ahead in a tough market. From day one, your key skill will be to perceive new and better ways of doing things to give your clients an unfair advantage.\n\n\nNow Take the Next Step\n\nIf you\u2019re looking forward to progressing your career with us, then we\u2019re looking forward to receiving your application.\n\nCapco is well known for its thought leadership and client-centric model that distinguishes it from other consulting firms. Capco\u2019s strong technology and digital knowledge base, it\u2019s global experience of the Financial Service enables us to deliver projects from strategy through to delivery. We are committed to providing new areas of expertise from which our clients will greatly benefit. We have:\n\nAccess to industry-focused talent globally\nAbility to leverage best-of-breed, innovative products and solutions for complex architecture and large-scale transformation\nExtended global geographic market reach\nAbility to capitalize on our client footprint and deep domain expertise within financial services\n\nFor more information about Capco, visit www.Capco.com.\n\nCapco is an equal opportunity employer. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, marital status, genetic information, national origin, disability, veteran status, and other protected characteristics.", "job_collect_date": "2020-08-16T16:18:23.000Z"}, "65": {"job_id": "750dbce57a60be39", "job_title": "Sr. Data Engineer", "job_employer": "SOTI Inc.", "job_location": "Mississauga, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=750dbce57a60be39&fccid=3d38508546395f15&vjs=3", "job_description": "SOTI is committed to providing its employees with endless possibilities; learning new things, working with the latest technologies and making a difference in the world.\nJob Title:\nSenior Data Engineer\nLocation:\nMississauga\nWho We Are\nAt SOTI, we are committed to delivering best in class mobile and IoT device management solutions. We are looking for out of the box thinkers that appreciate the art of creating great software. To us, being visionary is more important than doing things the way they\u2019ve always been done.\nWhat\u2019s in it for you?\nThe People - From our humble origins in our founder\u2019s basement, to our industry leading position today, SOTI has worked hard to foster a company culture that we can all believe in. A culture that emphasizes personal growth, continuous innovation and fun.\nThe Growth - Our environment fosters new ideas, fresh perspectives, and the ability to take them over the goal line. SOTI is a fast-paced environment with a global reach that encourages you to make your mark and be part of something big!\nThe Technology - You\u2019ll get the chance to work with leading edge technologies and take on complex and interesting projects, as part of highly collaborative and agile teams. You will work alongside SOTI\u2019s partners which include leading tech giants that will keep you on the cusp of emerging technologies.\nWhat You\u2019ll Do\nAbility to translate and document business requirements into technical documentation, supporting document management and knowledge sharing\nEnsure assigned deliverables are within business / audit control requirements\nTake ownership of end to end design and all aspects related to development and ensure design and development standards and followed\nCreate project documentation (Detailed design, Source-to-target mappings, Implementation plans, etc.)\nDevelop data and database-oriented solutions in order to solve complex business problems leading to data driven decision making\nDevelop data integration processes to integrate disparate data sets into a cohesive data model in support of BI and analytical requirements\nWorks closely with data architecture to ensure proper adherence to architectural guidelines and principles\nExperience You\u2019ll Bring:\n7+ years of hands-on advanced experience designing and developing BI Solutions and providing technical expertise\n7+ years hands-on advanced experience using MS SQL\n7+ years of experience with MS SQL Business Intelligence Stack (SSAS, SSIS, and SSRS)\n5+ years hands-on advanced experience using Power BI or similar BI platforms\nExperience using Cloud architecture, NoSQL databases and R/Python\nExperience using building data pipelines to integrate with unstructured data sources\nExperience in designing and building unstructured data stores using Azure or AWS technologies\nExperience with Data Warehouse concepts, including the use of Extract, Transform, and Load (ETL) tools\nExcellent analytical, troubleshooting, problem-solving and research skills\nMust be able to multitask and have experience with interacting within a diverse user/customer base\nExcellent written, verbal, and interpersonal communication skills\nAbout SOTI\nSOTI is the world's most trusted provider of mobile and IoT management solutions, with more than 17,000 enterprise customers and millions of devices managed worldwide. SOTI's innovative portfolio of solutions and services provide the tools organizations need to truly mobilize their operations and optimize their mobility investments. SOTI extends secure mobility management to provide a total, flexible solution for comprehensive management and security of all mobile devices and connected peripherals deployed in an organization.\nAt SOTI, we celebrate the uniqueness of our global teams and are proud to be an equal opportunity workplace. We are curious problem solvers who are committed to bringing the best mobile and IoT management solutions to market. We offer careers with #EndlessPossibilities.\nWhat are you waiting for? Apply today: https://www.soti.net/careers\nIf you want to bring your ideas to life, apply at SOTI today.\nWe are committed to providing accessible employment practices that are in compliance with the requirements under the Human Rights Code and the Accessibility for Ontarians with Disabilities Act (AODA). If you require accommodation during any stage of the recruitment process, please notify People & Culture at careers@soti.net .\nPlease note that SOTI does not accept unsolicited resumes from recruiters or employment agencies. In the absence of a signed Services Agreement with agency/recruiter, SOTI will not consider or agree to payment of any referral compensation or recruiter fee.", "job_collect_date": "2020-08-16T16:18:24.000Z"}, "66": {"job_id": "6efbc65db82f1b2d", "job_title": "Data Engineer", "job_employer": "Achievers", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=6efbc65db82f1b2d&fccid=a31d39aa57e3f105&vjs=3", "job_description": "Achievers delivers an Employee Success Platform\u2122 that enables social recognition, which dramatically increases employee engagement and drives business success. Designed specifically to meet the needs of today\u2019s workplace, it empowers employees to recognize each other in real time and aligns them to the goals of the company. With more than 5,000,000 annual recognitions, the Platform inspires brilliant performance in 110 countries. Visit us at www.achievers.com to learn more and join us in our mission to change the way the world works.\n\nWhy work with us?\nWe\u2019re a fun-loving, passionate, and highly collaborative team\nWe believe in moving quickly, failing fast, and adapting to change\nWe\u2019re committed to achieving technical excellence in everything we do\nWe value team work, learning from failure and innovation.\n\nWhy we'd want to work with you?\nYou have an undying passion for building world-class software\nYou\u2019re positive thinking, solution focused and find opportunities instead of problems\nYou have superb technical chops, but you\u2019re always striving to improve\nYou want to take ownership over your work and make challenging architecture decisions\nYou are ambitious and want to be involved in the strategic thinking of platform architecture\n\nWhat would a typical day look like, you ask? Something like this:\nDesign, develop, and maintain the software and systems that drive our back-end systems\nExtend our platform by researching and applying new technologies to solve business problems\nParticipate in multi-disciplinary projects with our applications and analytics teams\nContribute to our team\u2019s growing set of development platforms, tools, and processes\n\nDo you have what it takes? We will be responding to applicants that have:\nB.Sc. or Masters (preferred) in Computer Science or related field\n8+ years of relevant experience\nPrior experience with microservices architecture\nExperience with messaging systems, preferably Kafka or RabbitMQ\nExperience with Restful API\nProficient in web frameworks and PHP as well as other server-side language such as Python, Ruby, Java, JavaScript/React etc.\nAdvanced knowledge of data structures and security practices\nExcellent problem-solving abilities\nAbout Achievers:\n\nAs Achievers employees, we are passionate about disruptive technology, welcome constant change, and understand the value of employee success in the workplace. We enjoy coming to work every day because we believe in our product and love our culture. Achievers is more than just a software company; we are industry leaders in the HR space.\n\nOur headquarters are conveniently located in Liberty Village, close to bars, shops and restaurants and are highly accessible by TTC and GO Transit.\n\nWe have been recognized in numerous publications for our contributions to HR, for technical excellence and for our outstanding workplace culture:\n\nAchievers is ranked as number 32 on the list of Top 50 Best Workplaces in Canada.\nAchievers has been recognized on the 2020 list of Best Workplaces\u2122 for Women in Canada\nAchievers has been recognized on the 2020 list of Best Workplaces\u2122 for Inclusion\nAchievers has been named one of the Top 10 Employee Recognition Solution Providers Globally\n\nCheck out our platform in action here\n\nOur employees are a diverse and inclusive team of passionate, hardworking individuals. Achievers is committed to creating an environment where our employees can do the best work of their lives. We encourage all qualified candidates to apply to join our A-Player family. Accommodations are available on request for candidates taking part in all aspects of the selection process.", "job_collect_date": "2020-08-16T16:18:25.000Z"}, "67": {"job_id": "00df245dd038b055", "job_title": "Data Engineer - Applied AI", "job_employer": "Fiix", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=00df245dd038b055&fccid=87852d99a20def3f&vjs=3", "job_description": "Why Fiix?\nFiix has a big goal \u2013 to create a more sustainable world. Sounds lofty right? Our mission is to make every maintenance team successful by enabling the adoption of a CMMS and we\u2019re off to a great start. Teams that are part of the world\u2019s most well known brands (like Toyota, Siemens, and Sara Lee) manage their maintenance activities and achieve greater results with Fiix. But we\u2019re not stopping there. Our team is growing by leaps and bounds and we\u2019re conquering new challenges every day. We\u2019re looking for big thinkers with small egos to join us on our journey to create a more sustainable world.\n\nWhy we do it?\nWe\u2019re a team of market disrupting, like-minded individuals. We all do things our own way, but we come together each and every day to create and deliver the long awaited answer to an antiquated industry \u2013 and we have a lot of fun while we\u2019re at it.\n\nWe\u2019re looking for a Data Engineer to help take Fiix\u2019s explosive growth to a whole new level. We are looking for an experienced Data Engineer to build our data layer to support the delivery of machine learning-driven products. This is a unique opportunity to join a team of creative and passionate individuals committed to bringing AI to the predictive maintenance world.\n\nWhat You Will Do:\nWork with software engineering to design, build, maintain and optimize our data management and analytics pipeline\nPerform data analysis, quality assessments, cleaning, imputation and data aggregation tasks\nPerform feature engineering and selection to support machine learning activities\nBuild data processing pipelines and automate data pipelines in production environments\nWork with engineers and data scientists to deploy analytics capabilities and machine learning models in production\nWork with engineering and data teams to ingest and structure high throughput IoT data\nDevelop processes and frameworks to ensure data and model quality\nPerform code reviews and testing to ensure software quality is high and requirements are met\nDiagnose and repair data issues and assist customers with technical problems\nWhat We're Looking for:\n3+ years experience in a high growth software development environment developing data-driven products\nExperience working on ETL, data warehousing, data modeling, data architecting, data analysis\nExperience with at least some of: 1) Data streaming with Kinesis, Kafka or similar 2) ETL orchestration frameworks such as Airflow, Luigi or similar 3) Data warehouses such as Snowflake or similar\nDevelopment skills in Python, MySQL and other relational databases, NoSQL databases such as DynamoDB, Redis or similar\nExperience with AWS or other cloud providerEducation background:\nBachelor\u2019s Degree or higher in Computer Science or a related field\nEquity Statement\n\nAt Fiix, we recognize that people come with a wealth of experience and talent beyond just the technical requirements of a job. If your experience is close to what you see listed here, please still consider applying. Diversity of experience and skills combined with passion is a key to innovation and excellence. Therefore, we encourage people from all backgrounds to apply to our positions. Please let us know if you require accommodations during the interview process.", "job_collect_date": "2020-08-16T16:18:25.000Z"}, "68": {"job_id": "d21400c3ecab646b", "job_title": "Learn more about our Digital Analytics team!", "job_employer": "Scotiabank", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=d21400c3ecab646b&fccid=3002307a9e5b4706&vjs=3", "job_description": "Requisition ID: 65995\n\nAt Scotiabank Digital Factory, the reach and opportunity of a global organization meets the passion and drive of a startup. Reinvention starts here\u2014and it starts with you.\nWe believe that the pathway to success is paved with user feedback, so we\u2019re constantly asking our customers how they want to bank. When we find an opportunity to make things people will love, we assemble teams of specialists to collaborate in Digital Factories in Canada, Chile, Mexico, Peru, and Colombia. We work fast, and we work smart. Join us to create meaningful experiences and build the bank of tomorrow for our customers around the world.\nIf you\u2019d like to learn more about the Digital Factory workspace, our Digital Banking career opportunities, Communities of Practice, and open source projects, check out our microsite: https://digitalfactory.scotiabank.com/\n\nDisclaimer\n\u201cThis is for ongoing opportunities and not for an active or specific position. As such, you may not hear back from us immediately. In addition, this is a general overview of the Digital Analytics Community of Practice; therefore actual roles, in reality, will vary in their seniority and specific skill sets needed.\u201d\nWhat does Analytics at Scotia really mean?\nWe like to think of ourselves as superheroes, using the power of data to benefit the Bank and its customers! We make data-driven decisions that protect our customers, and we partner with internal clients, like our Marketing and Engineering teams, to lead data-driven initiatives that align with critical business objectives. Every project we\u2019re involved in benefits from our keen eye for detail and endless appetite to investigate the numbers we\u2019re working with. We focus on three major streams: Analytics and Reporting, Data Operations (Data Engineering), and Data Science.\n\nWho would I be working with and what are they like?\nIn addition to working on projects that directly correlate to business objectives, we\u2019re always striving to think of ways to improve the way our customer's bank through a good sense of precision, detail and carbon science. You\u2019ll get to work with a variety of tools, including open-source, as well as within a variety of environments depending on the business line and initiative you\u2019re focusing on.\nA couple of key projects we\u2019ve recently worked on include\u2026\nSearch Engine for Scotiabankers: We have a lot of smart people working together at Scotiabank, but sometimes it\u2019s hard to stay on top of information (not to mention acronyms)! We created a bank-wide search engine that links to the millions of pages that are buried in the background but hold valuable information.\n\n\nScotiabank\u2019s new mobile banking app: While this was a collaborative effort, the Analytics and Data Science teams played an integral role in the development of the new Scotia app. With a customer-first mindset and approach, we supported the teams by helping them target audiences more intelligently. We know that a student has different needs than a first time home buyer, and we\u2019re making sure that information is relevant based on personalization.\n\nSounds intriguing\u2026 but how do I fit in?\nBeing naturally curious and persuasive is how this team thrives!\nYou are able to walk us through your thought process and understand how to share your findings in business terms\nYou ache to solve the puzzle, and aim to present your insight once you solve it\nYou have an idea and you look both ways before crossing the street with it\nYou\u2019re not afraid to work hard and adhere to deadlines\nAt times things can change on the fly so it\u2019s important that you can adapt quickly to lots of moving parts: previous experience in an agile environment is always helpful\nOn the technical side, we are looking for domain knowledge, GCP, Python, R, or Hadoop\nKey areas we recruit for include\u2026\n\nData Scientist: You will create and deploy model and algorithm testing strategies to ensure model predictability holds.\nData Strategist: You will provide the insights, areas of opportunity and work with the stakeholders to implement the findings.\n\nData Engineer: You will be Integral to maintaining operational stability, and formulating techniques for quality data collection to ensure adequacy, accuracy, and legitimacy of data.\nWhat\u2019s in it for me?\nWe have an inclusive and collaborative working environment that encourages creativity, curiosity, and celebrates success!\nWe provide you with the tools and technology needed to create beautiful customer experiences\nYou'll get to work with and learn from diverse industry leaders, who have hailed from top technology companies around the world\nDress codes don't apply here, being comfortable does!\nFree shuttle service to and from Union Station, which means more money in your pocket\nOnsite subsidized cafeteria with a chef, so you can snack all day, every day\nOnsite fitness center, so you can feel your best\nAccess to thousands of online and in-person courses so you can brush up on skills, or learn new ones\nClear career mapping and progression. We hire you for your talent and not just for the job. We want to see you succeed not just in your role but in your career as a whole\nA competitive rewards package that includes a base salary, a performance bonus, company matching programs on pension and profit sharing, paid vacation, personal & sick days, medical, vision and dental benefits that start from day one and much more!\n\nLocation(s): Canada : Ontario : Toronto\nAs Canada's International Bank, we are a diverse and global team. We speak more than 100 languages with backgrounds from more than 120 countries. We value the unique skills and experiences each individual brings to the Bank, and are committed to creating and maintaining an inclusive and accessible environment for everyone. If you require accommodation (including, but not limited to, an accessible interview site, alternate format documents, ASL Interpreter, or Assistive Technology) during the recruitment and selection process, please let our Recruitment team know. If you require technical assistance please click here. Candidates must apply directly online to be considered for this role. We thank all applicants for their interest in a career at Scotiabank; however, only those candidates who are selected for an interview will be contacted.", "job_collect_date": "2020-08-16T16:18:26.000Z"}, "69": {"job_id": "71cd34c1d32326c5", "job_title": "Senior Data Engineer", "job_employer": "Manulife", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=71cd34c1d32326c5&fccid=1747adf6142beb48&vjs=3", "job_description": "Are you looking for unlimited opportunities to develop and succeed? With work that challenges and makes a difference, within a flexible and supportive environment, we can help our customers achieve their dreams and aspirations.\nJob Description\nManulife\u2019s Global Data Office (GDO) is seeking a Sr. Data Engineer reporting into the Director, Advanced Analytics and AI Engineering & Enablement Lead. Located in Toronto, Canada - the role will champion and support strategic and global data initiatives that strengthens Manulife\u2019s global data and advanced analytic capabilities, foster cross-segment collaboration and communication helping build an agile data insight driven culture, and lead and nurture open data design and architecture establishing conditions for successful technical and analytic innovation. The Sr. Data Engineer will develop, maintain, and test: data pipelines, application framework, infrastructure for data generation and work closely with Data Scientists to enable their work using modern data architecture and tools.\nJob Description\nManulife has a clear vision for a Global Data Strategy. By liberating and strengthening Manulife\u2019s data capabilities we will enable deeper insights, better product and service design, and more effective business processes. The result will be exceptional experiences for our customers.\n\nKey Responsibilities:\nLeveraging new & existing Big Data & Cloud technologies contributing to the innovative design, development and management of data analytics labs supporting to increase knowledge and insight from enterprise data\nPerform technical systems and data flow development in a variety of projects for complex front, middle and back office applications, with a focus on the reporting and analytics environment; this environment is built on a Microsoft Azure cloud and is underpinned by a Hortonworks Hadoop stack\nPerform technical systems and data flow design for small-to-medium sized projects\nWork with multiple project execution and deployment teams (e.g. Development, Business Analysis, Architecture, Release Management, Production Support)\nWork closely with the technical leads and architecture teams, and align solutions that meet the departmental architectural vision\nAssess the completeness and accuracy of data, identifying gaps in data, provide feedback to business and system owners with guidance and options to obtain missing information\nDesign, build and implement modern data architectures in development and production environments (data orchestration pipelines, data sourcing, cleansing, augmentation and quality control processes)\nTranslates business needs into data engineering and architecture solutions\nContributes to overall solution, integration and enterprise architectures\nBuild and support deploying machine learning models in development and production environments\nProvide proactive data ingestion and analysis of large structured and unstructured datasets involving a wide range of systems across Group Functions (i.e., Finance, Treasury, Risk, Human Resources, Brand & Communications)\nEvaluating existing and proposed data models and how to best access and query them as well as existing and proposed data interfaces and how to clearly document them, including specification of data flow models, data flow timing, data mapping, and data transformation rules including data validations and controls\n\nEducation, Experience & Skills:\nDemonstrated 2-5 years of professional experience in related industry experience in working in big data/data management & understanding big data analytic tooling and environments including a University degree and or Master\u2019s degree in Engineering, Computer Science or equivalent quantitative program\nExperience in Big Data, Analytics and Business Intelligence technologies to support design, build and implementation for advanced analytics and business intelligence reporting;\nExperience working with Cloudera and/or Hortonworks Hadoop stack\nExperience with big data processing frameworks and techniques such as HDFS, MapReduce, Syncsort, Sqoop, Oozie, Storage formats (Avro, Parquet), Stream processing (NiFi, Kafka), etc.\nUnderstanding of relational and warehousing database technology working with Hadoop and other major databases platforms (e.g., Hadoop, Oracle, SQLServer, Teradata, MySQL, or Postgres)\nExperience in data technologies and use of data to support software development, advanced analytics and reporting. Focus on Cloud (Azure), Hadoop-based technologies and programming or scripting languages like Java, Scala, Linux, C++, PHP, Ruby Python, R and SAS.\nKnowledge regarding different databases such as Hawq/HDB, MongoDB, Cassandra or Hbase.\nWorking knowledge of modern data streaming using Kafka, Apache Spark and data ingestion frameworks: NiFi, Hive and Pig\nExperience writing complex SQL and NoSQL jobs to analyze data in both traditional DBMS (MS-SQL, Oracle) and Big Data environments (i.e., HADOOP, SPARK, or similar open source and commercial technologies)\nKnowledge of non-relational (Cassandra, MongoDB) databases preferred\nPredictive analytics and machine learning experience (scikit-learn, Tensorflow, MLlib, recommendation systems) preferred\nExperience with integrating to back-end/legacy environments\nExperience integrating business and technology teams\nKnowledge and familiarity with machine learning models application and production pipelines\nCollaborative attitude, willingness to work with team members; able to coach, participate in code reviews, share skills and methods\nRemains current with emerging technologies, innovations and practices within the data and analytics industry\nGood organizational and problem-solving abilities that enable you to manage through creative abrasion\nGood verbal and written communication; effectively articulates technical vision, possibilities, and outcomes\nStrong work ethic, results oriented, and accuracy / attention to detail are critical; ability to work in agile or scrum delivery environments\nExceptional oral, written and interpersonal communication skills with the ability to simplify complex technical concepts into business & value-focused language. A key requirement is to communicate clearly and consistently keeping stakeholders well-informed of progress and challenges\nExcellent organizational and time management skills, strong business presence with ability to multi-task and effectively deal with competing priorities. Ability to work with minimal or no supervision while performing duties; has the ability and initiative to organize various functions and be a strong team player.\nWhat about Perks?\nManulife has lots of perks including, but not limited to:\nCompetitive compensation\nRetirement Savings Accounts including a RPP (Pension Plan), RRSP (Retirement Savings Plan), and TFSA (Tax Free Savings account)\nManulife Share Ownership Program with employer matching\nCustomizable Benefits Package including Health, Dental, Vision, and 100% of Mental Health expenses\nFinancial support for ongoing training, learning, and education\nMonthly Innovation Days (Hackathons)\nWearing jeans to work every day\nAn abundance of career paths and opportunities to advance.\nThis is a full-time, permanent role located in Toronto, Ontario.\n\nIf you are ready to unleash your potential it\u2019s time to start your career with Manulife/John Hancock.\nAbout Manulife\nManulife Financial Corporation is a leading international financial services group that helps people make their decisions easier and lives better. With our global headquarters in Toronto, Canada, we operate as Manulife across our offices in Canada, Asia, and Europe, and primarily as John Hancock in the United States. We provide financial advice, insurance, and wealth and asset management solutions for individuals, groups and institutions. At the end of 2019, we had more than 35,000 employees, over 98,000 agents, and thousands of distribution partners, serving almost 30 million customers. As of March 31, 2020, we had $1.2 trillion (US$0.8 trillion) in assets under management and administration, and in the previous 12 months we made $30.4 billion in payments to our customers. Our principal operations are in Asia, Canada and the United States where we have served customers for more than 155 years. We trade as 'MFC' on the Toronto, New York, and the Philippine stock exchanges and under '945' in Hong Kong.\nManulife is an equal opportunity employer. We strive to attract, develop and retain a workforce that is as diverse as the customers we serve and to foster an inclusive work environment that embraces the strength of cultures and individuals. We are committed to fair recruitment, retention and advancement and we administer all of our practices and programs based on qualification and performance and without discrimination on any protected ground.\nIt is our priority to remove barriers to provide equal access to employment. A Human Resources representative will consult with applicants contacted to participate at any stage of the recruitment process who request any accommodation. Information received regarding the accommodation needs of applicants will be addressed confidentially.", "job_collect_date": "2020-08-16T16:18:27.000Z"}, "70": {"job_id": "1f6d9f1ab7c60335", "job_title": "Senior Big Data Engineer", "job_employer": "Mosaic North America", "job_location": "Mississauga, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=1f6d9f1ab7c60335&fccid=1c4aa3d5a92746d4&vjs=3", "job_description": "The Senior Big Data Engineer DevOps role reports to the Director, Data Sciences DevOps. The role is part of the DevOps team in charge of the daily operations of various integration technologies. The role is responsible for supporting workloads running on the Hadoop environment and associated technologies. This position will be responsible for monitoring the batch jobs, resolving incidents, optimizing workloads, tuning jobs, and making job enhancements.\n\nThe role will focus on production support and will also take part in the DevOps rotation for making enhancements. Also, the role will be involved in R&D of emerging technologies with the application administrators and technical architects.\n\nStrategic:\nEvaluate tools and technologies in the context of the future state architecture, and evolving business requirements\nResponsible for review of project artifacts during the transition phase and ensure operational needs are met\nResearch & development about new Hadoop & Analytical technologies\nReview solution and technical designs\nPropose best practices/standards\nBenchmark the performance in line with the non-functional requirements\nPrevious experience in developing and deploying operational procedures, tuning guides and best practices documentation\nAttention to detail to review project deliverables for completeness, quality, and compliance with established project standards\n\nCandidate Profile:\n3+ years developing and supporting applications leveraging the Hadoop stack\n3+ of experience with data integration/ETL tools such as DataStage or alternatives\nSDLC knowledge in both waterfall and agile methodologies\nHands-on experience with source code management system (SVN, Git) and continuous integration tools (Jenkins)\nExperience on following tools: Hive, SQL, Spark, Kafka, Flume, Sqoop, HBase ,Pig, HDFS, R, NoSQL\nExperience on handling data processing, delivering distributed and highly scalable application\nExperience with HortonWorks Hadoop Distribution\nExperience with large scale domain or enterprise solution analysis development, selection and implementation\nExperience with high-volume, transaction processing software applications\nGood understanding of workload management, schedulers, scalability and distributed platform architectures\nExperience in software development and architecture experience using Java EE technologies (Application Server, Enterprise Service Bus, SOA, Messaging, Data Access Layers)\nExperience in scripting languages & automation such as bash, PERL, and Python\nExperience in data warehousing, analytics, and business intelligence/visualization/presentation\nExperience using SQL against relational databases\nWorking knowledge of search technologies like Lucene, Solr\n5+ years of hands-on experience on Linux, AIX, and z/OS\nProfessional Skill Requirements\nExcellent communication skills (both written and oral) combined with strong interpersonal skills\nStrong analytical skills and thought processes combined with the ability to be flexible and work analytically in a problem-solving environment\nAttention to detail\nStrong organizational & multi-tasking skills\nDISCLAIMER:\nAcosta/Mosaic North America is an Equal Opportunity Employer\nThe above statements are intended to describe the general nature and level of work being performed by people assigned to this classification. They are not intended to be construed as an exhaustive list of all responsibilities, duties, and skills required of personnel so classified. Mosaic reserves the right to modify all or part of any job descriptions at its discretion in order to meet and or exceed the needs of the business.\nWe are committed to providing accommodations for persons with disabilities. If you require accommodation, we will work with you to meet your needs, to the extent required by law.\n\nQualifications\n\n\nPrimary Location: CA-ON-Mississauga\nWork Locations: Acosta-Mosaic Mississauga Corporate Office 2700 Matheson Blvd. E. W. Tower 2nd Floor Mississauga L4W 4V9\nJob: Information Technology\nOrganization: CoE - Client Information Services - Canada\nShift: Standard\nJob Type: Full-time\n Day Job\nJob Posting: Aug 4, 2020, 10:54:13 AM", "job_collect_date": "2020-08-16T16:18:28.000Z"}, "71": {"job_id": "e8f01b72a171ac29", "job_title": "Data Engineer", "job_employer": "goeasy", "job_location": "Mississauga, ON", "job_link": "https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DbNJQGwjC-NEA80-qlVyEGmSv34Xx0eW4t4u9rXSv_7_tlvMU2e-Gw_wrZ47s1zPCnrySU2yuEL07wtSyx7XOwYh5VPcVL7CxRhOYwHzR-CdmyBrwbeqommOn6y9FD_uKIYpUCb9-PC_XgwGzBDrPy7SD7JQndrQfke5qYpMa2HgGSfMVA2_Hp0Cjw5tdgyghsqxMwx_hWpG_pLFpyFuohIu6k0ZY4sgc8e9ClWpE1wh07Z-ufHAYYOu3JuQjo5V72dyrxfLHxepp7Lab4MOOPxwlw5Ixb96mQkaaB9D-HFzvgOMMjdmrEoSpJ7dTD-k0S8RnGMp6R8mL3hFMg4Y6s9mMVc9VJ9Mqn_5q5zoKX3PfMMGRBg6U-n-Il-s5w3DwDIbXdpTbBQXKID3W6Xb_MD9GmY7e9tfqgIZoPWPBU7jjDnv_t2g56Zb8LzA2HBqoN7B2WVfknqVbTfs-pmyC7rrq3vVyXy0ZLoMvjcYtEbQ==&p=10&fvj=0&vjs=3", "job_description": "If you are looking to join one of Canada\u2019s fastest growing companies, goeasy Ltd. is the place for you! Recognized as one of Canada\u2019s Most Admired Corporate Cultures, one of Canada\u2019s Top 50 Fintech\u2019s and one of North America\u2019s Most Engaged Workplaces, we want the best and brightest to join our team.\n\nWe are a publicly traded company on the TSX with over 4000% shareholder return since 2001, goeasy operates two main business units. easyfinancial is our consumer lending business that offers secured and unsecured installment loans of up to $35,000 and easyhome is Canada\u2019s largest merchandise lease-to-own company. It is our mission to provide everyday Canadians the chance for a better tomorrow, today by giving them access to the credit they need and by offering them a second chance when they have been turned down by banks and traditional lenders. With a retail network of nearly 400 locations across Canada and over 1900 employees, we are able to build lasting relationships with our customers as we help them rebuild their credit and graduate towards prime rates and a brighter financial future.\n\nThe Data Engineer on the Data Science and Business Insights team will build, integrate data from various resources, manage data in goeasy operational data store and enterprise data warehouse. This position will develop ETL (Extract, Transform and Load) with various tools on large datasets to ensure data is easily accessible, works smoothly, as well as maintain and expand the data warehouse for reporting and analysis. The Data Engineer will also work closely with the data architect on the design and architecture of our enterprise data warehouse.\n\nResponsibilities:\n\nDevelop data set processes for data modeling, mining and production\nDevelop and maintain ETL processes using SSIS, Scripting and data replication technologies\nParticipate in development of datamarts for reports and data visualization solutions\nResearch opportunities for data acquisition and new uses for existing data\nIntegrate new data management technologies and software engineering tools into existing structures\nSupport the translation of business requirements for data acquisition/manipulation and provide detailed specifications that can be passed downstream for use\nDevelop detailed technical specifications and operational support documentation in collaboration with Business Systems Analysts, BI Engineers and Architects.\nIdentify and communicate technical problems, process and solutions\nCreate Ad-Hoc queries and reports as needed along with providing on-going analytical support for these requests\nAssist in the collection and documentation of user\u2019s requirements\nEnsure that existing business processes dependent on the ODS/EDW are monitored and respond quickly to bug fixes, enhancement requests and production ETL related issues.\nDealing with the database users on a daily basis to ensure that problems are dealt with promptly and that appropriate fixes are made to resolve any problems.\nRecommend ways to improve data reliability, efficiency and quality\nEnsure systems meet business requirements and industry practices\nWork effectively with the Business Intelligence and Data Solutions Architects, Data and BI Engineers to ensure that all approved development and deployment procedures are followed.\n\nQualifications:\n\nBachelor\u2019s Degree in Computer Science, MIS, Computer Engineering or other Information Technology related degree\n4+ years working with SQL Server or comparable relational database system\n3+ years of extensive ETL development experience with SSIS and/or ADF\n4+ years of experience troubleshooting within a Data Warehouse environment\nExpert domain knowledge & experience in Data warehousing, encompassing data model design, dimensional modeling, naming conventions, cross-cutting concerns, common integration technologies, patterns & standards and emerging technologies.\nExpert Knowledge of SQL skills to build, debug, and optimize (developing procedures, functions, SQL queries, etc.) and working with large data sets and to perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.\n2+ years SQL Server Database administration experience\nCloud experience (Azure) is highly preferred\nExposure and experience with Python, R, Hadoop, Azure, other Big data and advanced analytics\nKnowledge of AI and ML developments/solutions/implementations\nExperience with multiple programming languages (PowerShell scripting, C#, others) with basic scripting skills.\nHigh level of technical aptitude\n\nInclusion and Equal Opportunity Employment\n\ngoeasy is an equal opportunity employer. In addition, goeasy is committed to providing accommodations for applicants upon request at any stage of the recruitment process in accordance with all legislative requirements throughout Canada. Please let us know if you require an accommodation due to a disability during any aspect of the recruitment process and we will work with you to address your needs.\n\nAdditional Information:\n\nAll candidates considered for hire must successfully pass a criminal background check, credit check, and validation of their work experience to qualify for hire. We thank all interested applicants, however we will only be contacting those for interview who possess the skills and qualifications outlined above.\n\nWhy should you work for goeasy?\n\nTo learn more about our great company please click the links below:\n\nPAID1234", "job_collect_date": "2020-08-16T16:18:28.000Z"}, "72": {"job_id": "ed86ae879d5f5db2", "job_title": "Senior Big Data Engineer (f/m/x)", "job_employer": "Avira Operations GmbH & Co. KG", "job_location": "Canada", "job_link": "https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DVAdzec4bw6rQ4FO3PDdwz3gCZ43d7Gsr17jnu3o1g5CsoUOgyHaCzZOjoCGyji8ntYJxUdGd2FmwbktcVakqfF59toD5WP-Ro_DzTHrpq6YpYqUfP4QH5sxN4c835IUO3mqkddooJyzOy5iObgoXm6nmIVAYiD64A4oqzCBZE9JYD4xbdjSMdDWtxZsN0jxzZh2MZRbu8kewlmccWk6wRG9GgfcrJTeTSweVHplSvphfx2F0HA3r0aELcdCHKj20JbftGOe-CMJ45y3ehIf9Ynw-KME2lEx0Ws64kHXS9IpoIDIlbr6G8ckacwYqE4Dr7_7nZXYvU-7KGZl7bb94wSoTkCfEQzlZMgS4N4meWNxuwLlsaPMELvZCnhxEZQvW9CgCAN6Iv_RjQOTG4MNmk5gipud8rUW5hGm77a9V8Xhetge0TcVaw0ynCvWlIItYUnZIvpHoQuA==&p=11&fvj=0&vjs=3", "job_description": "Senior Big Data Engineer (f/m/x)\n\nLocation: Tettnang\n\nThe Challenge\n\nAs a Data Engineer in the Threat Intelligence team, you will design, implement and maintain the Threat Intelligence Platform that transforms massive amounts of (real-time) data from various sources into descriptive knowledge about emerging threats. You challenge our status quo and define standards for data-intensive analytic pipelines. Your projects will collect and process security-related intelligence and will then provide that data to the company as a whole so Avira can protect its users on a global scale.\n\nThe team\nWe are an international team of engineers and researchers. We are a self-organized and result-oriented team, always looking to perfect the art of automation. Our systems and services are integrated within the Avira Protection Cloud that protect consumers and businesses around the world.\n\nWhat you will do\nStarting from day one you will familiarize yourself with our big data ecosystem and tool stack, as well as the main data source systems.\n\nWithin the first 3 months, you are fully integrated into the Threat Intelligence Platform development and maintenance. You are taking responsibilities for certain data stream integrations.\n\nWithin the first 6 months you are an expert for the Threat Intelligence Platform having a complete understanding of its integration into Avira\u2019s protection technologies and architecture landscape.\n\nWithin the first year you will be truly integrated within a team that delivers the most up-to-date Threat Intelligence solutions to millions of customers. You take full ownership of cross-department projects and collaborate efficiently with other teams. You are keen to improve existing technologies and implement new and innovative features. You also play a big part in securing and supporting the digital lifestyle of our customers and making the Internet a more secure place.\n\nYour Qualifications\n3+ years' experience in software development, with excellent development skills in PythonIndustrial experience with data-intensive projects in the Hadoop ecosystem, Spark, Kafka, and AirflowExperience in building data ingestion pipelinesGood knowledge in designing, building, using and maintaining REST APIsExperience with SQL/NoSQL databases, especially creating scalable, multi-node deploymentsExperience with AWS components and principles are a plusStrong analytical, technical, organizational and communication skills\nThe position is based in Tettnang, Germany, near lake Constance.\n\nBenefits and perks:\nNew Work\nStylish building with roof terracesCanteen and ChocaViraModern office concept\n\nLearning & Development\nUnlimited access to UdemyTrainings & Conferences\nSpecialist Career\n\nHealth & Wellbeing\nGym and fitness courses\u201cJobRad\u201d bike leasingMedical checkups\n\nFamily & Living\nRelocation PackageVacation child careAvira Prime licences\n\nEvents\nOnboarding eventsMonthly Employee MeetingsSummer & Christmas parties\n\nAN OPPORTUNITY TO MAKE A DIFFERENCE\nUpdate: Although there\u2019s a lot of disruption nowadays due to Covid-19, we at Avira are continuing to run our daily business activities so that we stay true to the promise to our customers - now even more than ever: Protecting people in the connected world.\n\nAnd we are doing this from the safeness of our own homes. Among other things, this means we are still hiring, but we have moved all our interviews online and all our colleagues are being onboarded remotely\n\nSo join us and you will be able to work from home until the danger is over.\n\nWe\u2019re an international software company at the forefront of imagining the future of digital security. Avira\u2019s award-winning products and technology protect over 500 million users in the connected world.\n\nWhat makes us special? First and foremost \u2013 it\u2019s the authentic people at Avira. We have a great community feeling that fosters your uniqueness and offers the space to reflect, the feedback to grow, and the freedom to innovate. If you are looking for a culture that also encourages aspiration and professional excellence, get in touch with us and discover the Avira experience firsthand.\n\nLena Komarek\nHR Recruiter\nAvira Operations GmbH & Co. KG\nHuman Resources\nKaplaneiweg 1\nD-88069 Tettnang, Germany\nPhone: +49 (0) 7542-500 -2207\nE-Mail: lena.komarek(at)avira.com", "job_collect_date": "2020-08-16T16:18:29.000Z"}, "73": {"job_id": "997673b8effa647f", "job_title": "Data Engineer (Intermediate 4-6 years) - Contract - Remote during COVID", "job_employer": "CorGTA", "job_location": "North York, ON", "job_link": "https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Dkh866153otbRJ8nVMuEXnXPd315JS377__3VyD6HnNQkmcaOHMemmy5pLrNV6j0WaVWEWWabnp4PP6-2u77DLfT_FCIGvCW8CQmQW140Z7EVU32Jnwl42FALsxnbc4n5M6FOTfoW2OoTMOcGG21HZGOoXL7fWT8eaT67VlcClh4sHQcJsHCX_mVuwTGUlHvmzZOG0lt9m6pERSiNZTpJXhnbFZKfzEOzxebc-MZKFqINGnIM3FbIIXsQB63u7mLthrZ6CQxBUv_c9kW9FxtQ4Zw6pTWjVXgSuQTTvdubQ5xKenEd27ewy0oujKCH-hCaQSqAfl5v7rwpnwREq2g9ql6aeM7F-uzNP3q-yPw7mzUV2_KV5PLz5jTfnYnwP8PZywfT4nd9943cKlmCQmX92YonuybhB31fUS2JYoGRIIZ2pezUlnHdQkcSqUc6dBCA_2wpISun6HOiWdBgB3uC3EL1tO7PESROPQOWgUM8YcH-2v7VTucL9&p=12&fvj=1&vjs=3", "job_description": "Role: Data Engineer (Intermediate 4-6 years)Structure: Contract (6 months initially)Location: North York, ONHours: Monday - Friday 40 hours (remote during COVID-19)Pay: $50.00 p/h inc.The role: We are looking for Intermediate Data Engineers who have roughly between 4-6 years experience who ideally has experience moving data from Google cloud over to Big query.However, if you are someone who has good experience with ETL and Data Pipeline Design & Data Validation experience and can script in Python and create strong SQL Queries then this will also be a potential match for this role.Ideal Qualifications to be successful in this role: - Degree in Computer Science, Engineering, Information Systems (or equivalent combination of skill and experience)- 4-6 years of experience working with data architecture projects- Excellent SQL coding and experience with a broad array of development tools and platforms including exposure to a big data environment tools/languages, such as SQL, R, SAS, Python, etc.- Experience creating relational database design and data models- Experience with various analytical data platforms and technologies- Experience with using REST API, Cloud (GCP Ideally)- Experience in custom or structured data integration design, implementation, and maintenance- Experience with business intelligence tools such as Qlikview, Tableau and Microstrategy (Nice to have for Data Visualizations)- Previous experience working in retail or eCommerce is highly preferred- Ability to curate data to tell stories and provide business insightsPlease apply with an updated resume and ensure the required skills you are able to speak to for this position are included.For more roles like this please go to www.corgta.com/find-a-job/Job Type: Full-timeSalary: $45.00-$50.00 per hour", "job_collect_date": "2020-08-16T16:18:30.000Z"}, "74": {"job_id": "54e0fcc38385e3a2", "job_title": "Data Engineer/Integration Consultant", "job_employer": "Copperstone Connect", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Ceb7pqR8iGmLuXIZQklF6pqed3xddONNM6Jjumj-ndyGK0kULQUmQ2REqfJ5KAzFVeK_es1WVg0s1sLgKW09UsvVRp0-dSizFQr1oNyK_VIqdjla00VU9GNVvu2x4v9p3hsL7NeF-6hBksbAaSgKazsaYTODGMKQQeVuYcxoOuxvFKuD0HRLD9ybgbcXTn1LOPdftLSRgjwLGAgq4BZkETzA1vslYmb1bVzz-1hHtXxxiTYX-W_PFTBeCgJgA2h4lEzuxbyT-6fA7KeGJQThb0vZqTrZMazSuACM5K0x7SrRFYZeDFl1eVNEj6vY5YHVRa87eLiW9EfuhpceCpOapROAgpdsyDZTaQessPMxy3LyU5ADubHB0rbc_KNfaxDxwD4kkgatIldxtzM38STMQnrceg9WK1MvN6IdzqgPK_8EQk6RxOHirazOcLhSFl1KHqkl_PDvJfXF6rwP2Nag-DPqSiWHA-8aGpCI4IjHIRIKpqlTN7YabUlz9IueG6mBDOBm80fH4UiFENYNhE87B4LQabK78GKrXjFb5MJbvzALhfpwIjEyIvJ-dKUWlI_7-1ND4FKv3H77G4qlzIT4XDVFi_QYo9SMogWZUcKifJszffxyNyHkPUzwm-cpiQSfHT8yipHDgYS1gMccbsTqZRltoMeFdrsWbYwMH0B7ibfw==&p=13&fvj=0&vjs=3", "job_description": "Our client, a boutique-size consulting firm, prides itself on being at the forefront of innovation in the Big Data space. Founded in 2010, they provide thought leadership and implementation excellence within the ever-growing data and analytics world.\nThey take pride in having some of the most highly trained and experienced consultants in the industry which translates into optimal value for their clients. They were one of the first companies to provide analytics and data as a service, via the cloud, as early as 2010. They strive to make sure their customers are well-positioned with the best technologies/tools in the industry, constantly evaluating new and existing technology partnerships. Some of the more prominent companies they have partnered with include; Snowflake, DataRobot, MicroStrategy, Informatica, Amazon AWS & Microsoft. They continue to invest in their most valuable resource, their people. They do this through extensive training both on the job and through various educational programs.\nDue to growth, they are seeking to hire an Intermediate - Senior Integration Consultant.\n\nDesired Skills and Experience\n: University/College degree in Computer Science, Mathematics, Data Science and/or Relevant Degree\n: 5+ years hands-on development, configuration, scripting and administration experience with Data Integration platforms. (i.e. Informatica, Talend, DataStage, SSIS)\n: BI Experience (MicroStrategy, Looker, Tableau, PowerBI) considered a nice to have\n: Extensive theoretical and practical knowledge of data warehousing principles/concepts and practical development experience in all areas of the data warehousing life cycle\n: Experience with Data Management, ETL, Cloud Data (AWS), Data Integration\n: Knowledge of OLAP-related principles and concepts\n: Strong grasp of data modeling techniques and concepts (Normalized/Denormalized, Conceptual/Logical/Physical, Star, Snowflake, Data Vault)\n: Strong knowledge and experience with relational databases such as Snowflake, SQL Server, Oracle (Advanced knowledge of reading and writing SQL, Performance analysis and tuning)\n: Knowledge and experience with key Big Data technologies (Hive, Presto, Spark, Kafka, NoSQL databases, Semi-structured data access patterns (Json, Parquet, XML, etc.))\n: Strong Python scripting skills\n: Excellent communication skills\n: Great problem-solving skills\n: Leadership and good client management skills\n\nDay to Day Activities Would Include\n: Conduct relevant customer interviews to determine key business requirements and objectives\n: Build appropriate analytical data models based on outcomes of user interviews\n: Analyze and profile data systems to build source to target data mappings\n: Review ETL performance and conducts performance tuning as required on mappings/workflows or SQL\n: Administration and support of data integration infrastructure\n: 2nd level on-call support of ETL services as required\n\nYou will be responsible for attaining the following goals:\n: Attaining a minimum of 1 new accreditation/certification per year\n: Spending 80% or more of their time on billable work\n: Completing 90% or more of their agile delivery tasks on time\n: Demonstrating competency in 1 new relevant technology every year", "job_collect_date": "2020-08-16T16:18:31.000Z"}, "75": {"job_id": "a32b7c599156a218", "job_title": "BHJOB15656_15145 - Data Engineer w/ DevOps - Azure Cloud", "job_employer": "Myticas Consulting", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Bt274jo0r_e0cCCqhskoQvohXG3bfDAo8rIdE-Msu3yuVfyF2oiDFKHFOU-vxVulAfe8kPL7riFwoXa1bsfN-szjxO_8qUQWbPsJOnTg0EkutIoK2S4NNLL24JQh9e4DsE9hv907k2_zDbvj0a8WAj5oPSV5J0YhhOguMmzdyDAqna7fjKHwW5m8REzLWcEdvFIPg9tirNLM_vA2xwuLxh_8Y9rw21Nyqe6a5pN0BQxdlgjP1hYfdZMFAfZfzWCqBksUy7PPNaPQhpdV8_I5bNcJ-Y1joNJ4qKCK7Pk1c5P1Sy7TMV91SZzyNQRFYL_JMy40l1d31W9ypLjgYiJsfrIU8mo561kEbHgsTlPl3_gMIhXk8zEgE_SdheMrkdkbOxIqtN6CBiEyTqu5WjoYUBrcRPVMgSupMG9-fUyE5Wrt7TK-pP3udXp5wkB34yqKuyu9Ql4EegMQ==&p=14&fvj=0&vjs=3", "job_description": "The growing team here at Myticas is looking for an experienced Data Engineer who would be interested in a remote contract opportunity within the Ottawa, Toronto and Montreal regions.\n\nAccountabilities:\n\nDefining and reviewing security design requirements for cloud infrastructure and application components.\nEvaluating architecture patterns from security perspective.\nBuilding and implementing security controls to enable enforcement of compliance with Cloud Control Objectives, using custom Azure policies and integrated controls in DevOps processes\n\nRequirements:\n\nStrong Data Engineer w/ DevOps expertise + Azure Cloud Experience\nMust know how to code and stand up scripts.\nExperience with Data Digestions\nExperience writing scripts to automate (infrastructure)\nARM Templating Expertise\nAzure Synapse Expertise\nSupport developing automated DevOps processes and procedures for the following Azure components:\nAzure Synapse (Azure DW) & Studio (private preview)\nAzure Data Catalog Gen 2 (Babylon \u2013 private preview)\nAzure Data Lake Storage Gen 2\nAzure ML\nML Flow\nAzure SQL Analysis Service\nAzure Databricks\nADF data pipelines for data loading to AzSQL/Synapse\nADF data pipelines for connecting to on-prem data sources for data\n\nCandidates looking to apply for this role are to send us an updated version of their resume in confidence. Our team will be sure to review all applicants and follow up accordingly at the conclusion of the review process.\n\nJob is also known as: Data Engineer, DevOps Engineer, Azure Cloud Engineer, Azure Engineer, Cloud Engineer\n\nINDMY", "job_collect_date": "2020-08-16T16:18:32.000Z"}, "76": {"job_id": "1f6d9f1ab7c60335", "job_title": "Senior Big Data Engineer", "job_employer": "Mosaic North America", "job_location": "Mississauga, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=1f6d9f1ab7c60335&fccid=1c4aa3d5a92746d4&vjs=3", "job_description": "The Senior Big Data Engineer DevOps role reports to the Director, Data Sciences DevOps. The role is part of the DevOps team in charge of the daily operations of various integration technologies. The role is responsible for supporting workloads running on the Hadoop environment and associated technologies. This position will be responsible for monitoring the batch jobs, resolving incidents, optimizing workloads, tuning jobs, and making job enhancements.\n\nThe role will focus on production support and will also take part in the DevOps rotation for making enhancements. Also, the role will be involved in R&D of emerging technologies with the application administrators and technical architects.\n\nStrategic:\nEvaluate tools and technologies in the context of the future state architecture, and evolving business requirements\nResponsible for review of project artifacts during the transition phase and ensure operational needs are met\nResearch & development about new Hadoop & Analytical technologies\nReview solution and technical designs\nPropose best practices/standards\nBenchmark the performance in line with the non-functional requirements\nPrevious experience in developing and deploying operational procedures, tuning guides and best practices documentation\nAttention to detail to review project deliverables for completeness, quality, and compliance with established project standards\n\nCandidate Profile:\n3+ years developing and supporting applications leveraging the Hadoop stack\n3+ of experience with data integration/ETL tools such as DataStage or alternatives\nSDLC knowledge in both waterfall and agile methodologies\nHands-on experience with source code management system (SVN, Git) and continuous integration tools (Jenkins)\nExperience on following tools: Hive, SQL, Spark, Kafka, Flume, Sqoop, HBase ,Pig, HDFS, R, NoSQL\nExperience on handling data processing, delivering distributed and highly scalable application\nExperience with HortonWorks Hadoop Distribution\nExperience with large scale domain or enterprise solution analysis development, selection and implementation\nExperience with high-volume, transaction processing software applications\nGood understanding of workload management, schedulers, scalability and distributed platform architectures\nExperience in software development and architecture experience using Java EE technologies (Application Server, Enterprise Service Bus, SOA, Messaging, Data Access Layers)\nExperience in scripting languages & automation such as bash, PERL, and Python\nExperience in data warehousing, analytics, and business intelligence/visualization/presentation\nExperience using SQL against relational databases\nWorking knowledge of search technologies like Lucene, Solr\n5+ years of hands-on experience on Linux, AIX, and z/OS\nProfessional Skill Requirements\nExcellent communication skills (both written and oral) combined with strong interpersonal skills\nStrong analytical skills and thought processes combined with the ability to be flexible and work analytically in a problem-solving environment\nAttention to detail\nStrong organizational & multi-tasking skills\nDISCLAIMER:\nAcosta/Mosaic North America is an Equal Opportunity Employer\nThe above statements are intended to describe the general nature and level of work being performed by people assigned to this classification. They are not intended to be construed as an exhaustive list of all responsibilities, duties, and skills required of personnel so classified. Mosaic reserves the right to modify all or part of any job descriptions at its discretion in order to meet and or exceed the needs of the business.\nWe are committed to providing accommodations for persons with disabilities. If you require accommodation, we will work with you to meet your needs, to the extent required by law.\n\nQualifications\n\n\nPrimary Location: CA-ON-Mississauga\nWork Locations: Acosta-Mosaic Mississauga Corporate Office 2700 Matheson Blvd. E. W. Tower 2nd Floor Mississauga L4W 4V9\nJob: Information Technology\nOrganization: CoE - Client Information Services - Canada\nShift: Standard\nJob Type: Full-time\n Day Job\nJob Posting: Aug 4, 2020, 10:54:13 AM", "job_collect_date": "2020-08-16T16:18:34.000Z"}, "77": {"job_id": "71cd34c1d32326c5", "job_title": "Senior Data Engineer", "job_employer": "Manulife", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=71cd34c1d32326c5&fccid=1747adf6142beb48&vjs=3", "job_description": "Are you looking for unlimited opportunities to develop and succeed? With work that challenges and makes a difference, within a flexible and supportive environment, we can help our customers achieve their dreams and aspirations.\nJob Description\nManulife\u2019s Global Data Office (GDO) is seeking a Sr. Data Engineer reporting into the Director, Advanced Analytics and AI Engineering & Enablement Lead. Located in Toronto, Canada - the role will champion and support strategic and global data initiatives that strengthens Manulife\u2019s global data and advanced analytic capabilities, foster cross-segment collaboration and communication helping build an agile data insight driven culture, and lead and nurture open data design and architecture establishing conditions for successful technical and analytic innovation. The Sr. Data Engineer will develop, maintain, and test: data pipelines, application framework, infrastructure for data generation and work closely with Data Scientists to enable their work using modern data architecture and tools.\nJob Description\nManulife has a clear vision for a Global Data Strategy. By liberating and strengthening Manulife\u2019s data capabilities we will enable deeper insights, better product and service design, and more effective business processes. The result will be exceptional experiences for our customers.\n\nKey Responsibilities:\nLeveraging new & existing Big Data & Cloud technologies contributing to the innovative design, development and management of data analytics labs supporting to increase knowledge and insight from enterprise data\nPerform technical systems and data flow development in a variety of projects for complex front, middle and back office applications, with a focus on the reporting and analytics environment; this environment is built on a Microsoft Azure cloud and is underpinned by a Hortonworks Hadoop stack\nPerform technical systems and data flow design for small-to-medium sized projects\nWork with multiple project execution and deployment teams (e.g. Development, Business Analysis, Architecture, Release Management, Production Support)\nWork closely with the technical leads and architecture teams, and align solutions that meet the departmental architectural vision\nAssess the completeness and accuracy of data, identifying gaps in data, provide feedback to business and system owners with guidance and options to obtain missing information\nDesign, build and implement modern data architectures in development and production environments (data orchestration pipelines, data sourcing, cleansing, augmentation and quality control processes)\nTranslates business needs into data engineering and architecture solutions\nContributes to overall solution, integration and enterprise architectures\nBuild and support deploying machine learning models in development and production environments\nProvide proactive data ingestion and analysis of large structured and unstructured datasets involving a wide range of systems across Group Functions (i.e., Finance, Treasury, Risk, Human Resources, Brand & Communications)\nEvaluating existing and proposed data models and how to best access and query them as well as existing and proposed data interfaces and how to clearly document them, including specification of data flow models, data flow timing, data mapping, and data transformation rules including data validations and controls\n\nEducation, Experience & Skills:\nDemonstrated 2-5 years of professional experience in related industry experience in working in big data/data management & understanding big data analytic tooling and environments including a University degree and or Master\u2019s degree in Engineering, Computer Science or equivalent quantitative program\nExperience in Big Data, Analytics and Business Intelligence technologies to support design, build and implementation for advanced analytics and business intelligence reporting;\nExperience working with Cloudera and/or Hortonworks Hadoop stack\nExperience with big data processing frameworks and techniques such as HDFS, MapReduce, Syncsort, Sqoop, Oozie, Storage formats (Avro, Parquet), Stream processing (NiFi, Kafka), etc.\nUnderstanding of relational and warehousing database technology working with Hadoop and other major databases platforms (e.g., Hadoop, Oracle, SQLServer, Teradata, MySQL, or Postgres)\nExperience in data technologies and use of data to support software development, advanced analytics and reporting. Focus on Cloud (Azure), Hadoop-based technologies and programming or scripting languages like Java, Scala, Linux, C++, PHP, Ruby Python, R and SAS.\nKnowledge regarding different databases such as Hawq/HDB, MongoDB, Cassandra or Hbase.\nWorking knowledge of modern data streaming using Kafka, Apache Spark and data ingestion frameworks: NiFi, Hive and Pig\nExperience writing complex SQL and NoSQL jobs to analyze data in both traditional DBMS (MS-SQL, Oracle) and Big Data environments (i.e., HADOOP, SPARK, or similar open source and commercial technologies)\nKnowledge of non-relational (Cassandra, MongoDB) databases preferred\nPredictive analytics and machine learning experience (scikit-learn, Tensorflow, MLlib, recommendation systems) preferred\nExperience with integrating to back-end/legacy environments\nExperience integrating business and technology teams\nKnowledge and familiarity with machine learning models application and production pipelines\nCollaborative attitude, willingness to work with team members; able to coach, participate in code reviews, share skills and methods\nRemains current with emerging technologies, innovations and practices within the data and analytics industry\nGood organizational and problem-solving abilities that enable you to manage through creative abrasion\nGood verbal and written communication; effectively articulates technical vision, possibilities, and outcomes\nStrong work ethic, results oriented, and accuracy / attention to detail are critical; ability to work in agile or scrum delivery environments\nExceptional oral, written and interpersonal communication skills with the ability to simplify complex technical concepts into business & value-focused language. A key requirement is to communicate clearly and consistently keeping stakeholders well-informed of progress and challenges\nExcellent organizational and time management skills, strong business presence with ability to multi-task and effectively deal with competing priorities. Ability to work with minimal or no supervision while performing duties; has the ability and initiative to organize various functions and be a strong team player.\nWhat about Perks?\nManulife has lots of perks including, but not limited to:\nCompetitive compensation\nRetirement Savings Accounts including a RPP (Pension Plan), RRSP (Retirement Savings Plan), and TFSA (Tax Free Savings account)\nManulife Share Ownership Program with employer matching\nCustomizable Benefits Package including Health, Dental, Vision, and 100% of Mental Health expenses\nFinancial support for ongoing training, learning, and education\nMonthly Innovation Days (Hackathons)\nWearing jeans to work every day\nAn abundance of career paths and opportunities to advance.\nThis is a full-time, permanent role located in Toronto, Ontario.\n\nIf you are ready to unleash your potential it\u2019s time to start your career with Manulife/John Hancock.\nAbout Manulife\nManulife Financial Corporation is a leading international financial services group that helps people make their decisions easier and lives better. With our global headquarters in Toronto, Canada, we operate as Manulife across our offices in Canada, Asia, and Europe, and primarily as John Hancock in the United States. We provide financial advice, insurance, and wealth and asset management solutions for individuals, groups and institutions. At the end of 2019, we had more than 35,000 employees, over 98,000 agents, and thousands of distribution partners, serving almost 30 million customers. As of March 31, 2020, we had $1.2 trillion (US$0.8 trillion) in assets under management and administration, and in the previous 12 months we made $30.4 billion in payments to our customers. Our principal operations are in Asia, Canada and the United States where we have served customers for more than 155 years. We trade as 'MFC' on the Toronto, New York, and the Philippine stock exchanges and under '945' in Hong Kong.\nManulife is an equal opportunity employer. We strive to attract, develop and retain a workforce that is as diverse as the customers we serve and to foster an inclusive work environment that embraces the strength of cultures and individuals. We are committed to fair recruitment, retention and advancement and we administer all of our practices and programs based on qualification and performance and without discrimination on any protected ground.\nIt is our priority to remove barriers to provide equal access to employment. A Human Resources representative will consult with applicants contacted to participate at any stage of the recruitment process who request any accommodation. Information received regarding the accommodation needs of applicants will be addressed confidentially.", "job_collect_date": "2020-08-16T16:18:35.000Z"}, "78": {"job_id": "fd5da69e089b7f27", "job_title": "Data Engineer", "job_employer": "Bond Brand Loyalty", "job_location": "Mississauga, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=fd5da69e089b7f27&fccid=9343039b36601ad2&vjs=3", "job_description": "At Bond, we design creative and innovative solutions for our clients, all with the goal of helping them build ever-stronger loyalty to their brands. That can take us in some pretty amazing directions, and as a Data Engineer, you\u2019ll have your hands on the wheel as we drive the future of loyalty.\n\nWorking on the bleeding edge of exciting technology, you're afforded the opportunity to experiment with new tools and attempt radically different approaches than traditional software engineering affords. Every day with the Data Engineering team is different and each project presents its own set of new and exciting challenges. Things shift very quickly in our industry and we rely on the Data Engineering team to keep us ahead of the curve and moving in the right direction.\nHere's what we want:\nProblem Solver: You are curious and loves exploring multiple approaches to find the most efficient, scalable solution and solve a problem\nCollaborative: You work well with other people\nPassionate: A passion for Big Data and an interest in the latest trends and developments constantly researching new tools and data technologies\nSelf-starter: You are comfortable helping your team get things done\nHere's what you'll be doing:\nDesign, implement, and maintain data pipelines for extraction, transformation, and loading of data from a wide variety of data sources to various data services\nIdentify, design, and implement system performance improvements\nIdentify, design, and implement internal process improvements\nAutomate manual processes and optimize data delivery\nUseful skills/background: You may or may not tick off every box, and that's ok. Each person brings a different background and different skills. If you think you are a good match for what we are looking for tell us why, and tell us what you are doing to improve yourself and we'll see what we can do to help!\nA degree in Computer Science/Engineering or related field\n2-4 years of experience in a software engineering environment\nExperience with SQL and NoSQL systems\nKnowledge of Hadoop, Spark, Kafka or other equivalent technologies\nProficiency in some of the following languages: Scala, Java, Python, Bash\nExperience with automated testing systems\nMentorship, collaboration, and communication skills\nKnowledge of data modelling, data warehousing, ETL processes, and business intelligence reporting tools\nExperience working with CI/CD, containerization, and virtualization tools such as Gitlab, Jenkins, Kubernetes, Docker\nExperience with tools like Databricks, Snowflake or PowerBI\n\nWhy Join Us?\nBond Brand Loyalty is proud to be recognized as one of Canada\u2019s Best Managed Companies.\n\nWe\u2019re 400(ish) people working tirelessly together to make the world a more loyal place. You\u2019ll be joining a hyper-talented team with a galaxy of skillsets ranging from research to creative to digital and beyond. You\u2019ll have an excellent opportunity to grow, learn and make an impact as we tackle some of our client\u2019s biggest business challenges.\nIf you\u2019re looking to build your career, build your skills and build bonds apply today!\nBond Brand Loyalty welcomes and encourages applications from people with disabilities. Accommodations are available on request for candidates taking part in all aspects of the selection process.", "job_collect_date": "2020-08-16T16:18:37.000Z"}, "79": {"job_id": "5dae399d2651bf86", "job_title": "Senior Data Engineer", "job_employer": "Veeva Systems", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=5dae399d2651bf86&fccid=353eb997fc901045&vjs=3", "job_description": "At Veeva, we build enterprise cloud technology that powers the biggest names in the pharmaceutical, biotech, consumer goods, chemical & cosmetics industries. Our customers make vaccines, life-saving medicines, and life-enhancing products that make a difference in everyday lives. Our technology has transformed these industries; enabling them to get critical products and services to market faster. Our core values, Do the Right Thing, Customer Success, Employee Success, and Speed, guide us as we make our customers more efficient and effective in everything they do.\n\nThe Role\n\nVeeva Systems is looking for experienced data engineers to build a cloud-based data analytics solution for the life science industry. If you are passionate about data and are eager to design and build data platforms from the ground up this is the role for you. The data analytics platform will provide data ingestion, data storage and rich data analytics capabilities with elegant visualization dashboards.\nWhat You'll Do\nDesign and implement AWS based ETL processes to onboard data into our data lake from a variety of internal and external sources for our new data analytics platform.\nDesign data models and data services for optimal storage and retrieval.\nImplement scalable data lake interfaces, microservices, and rest based API for querying and storing structured data.\nIntegrate new technologies to support advanced analytic use cases.\nRequirements\n5+ years\u2019 experience in Python or Java, preferably at an enterprise cloud software company\nProven ability to write clean, testable, readable code in a team environment\nHands-on experience with building data pipelines in a programming language like Java or Python\n3+ years of experience in relational databases with a mastery of SQL\nExperience in data modelling, ETL development (pref. Apache Spark), and Data warehousing\nNice to Have\nAWS Services (S3, Redshift, Elastic Search)\nExperience with large scale big data pipeline \u2013 ETL / Kafka / Spark / MapReduce / Hadoop\nFamiliarity with Open API Specifications and Swagger\nExperience working in an agile environment\nExperience working in a startup\nPerks & Benefits\n Conveniently located in downtown Toronto Snacks, beverages, and weekly lunches from local restaurants Team events and rec league sports teams Allocations for continuous learning & development Health & wellness programs Weekly yoga classes Ping pong and other games\n\nVeeva\u2019s headquarters is located in the San Francisco Bay Area with offices in more than 15 countries around the world.\n\nVeeva Systems is an equal opportunity employer. Accordingly, we are committed to fair and accessible employment practices. Veeva Systems welcomes and encourages applications from people with disabilities. Accommodations are available upon request for candidates taking part in all aspects of the selection process.", "job_collect_date": "2020-08-16T16:18:37.000Z"}, "80": {"job_id": "ad8d075517b97d64", "job_title": "DATA ENGINEER (SOPHI)", "job_employer": "The Globe and Mail", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=ad8d075517b97d64&fccid=2c209e080b2e73f5&vjs=3", "job_description": "DATA ENGINEER (SOPHI)\n\nPOSITION CODE: 2020-063\nLOCATION: The Globe and Mail, Toronto\nSALARY: Commensurate with qualifications and experience\n\nPOSITION OVERVIEW:\n\nWe\u2019re looking for experienced individuals with deep knowledge of data streaming, serialization, databases and distributed systems, and proficient in writing custom libraries but also know when to use off-the-shelf solutions when necessary. Ideal candidates are self-motivated engineers with a passion for both business and technology innovation, more importantly they quickly adapt with changing technologies. We value people who are passionate about system design and have an eye for improving product quality. We currently work with Scala, Kotlin, Java, Python, NodeJS, Postgres, Go, Kafka, and Flink.\n\nAs a Data Engineer you will:\nDevelop and optimize system components for maximum performance and scalability across a vast array of environments\nHave a commitment to collaborative problem solving, sophisticated design, and product quality\nEnsure that system components and the overall application are robust and easy to maintain\nContribute to backlog reviews, technical solutions design and implementations\nBe disciplined in implementing software in a timely manner while ensuring product quality isn\u2019t compromised\nMINIMUM QUALIFICATIONS:\nStrong analysis and problem solving skills\nDeep understanding of good programming practices, design patterns, Functional Programming, and Object Oriented Analysis and Design\nSuccessfully implemented and released a large number of data pipelines and web services using modern engineering frameworks in the past 3 years\nFormal training in software engineering, computer science or computer engineering.\nWorked as part of a mature engineering team\nIDEAL CANDIDATE:\nHave strong working knowledge with Scala and/or Kotlin.\nUnderstands reactive programming, Threads and Futures.\nSuccessfully implemented realtime and batch analytics using Kafka, Flink, Apache Beams and/or Google DataFlow.\nStrong working knowledge of data warehouses include Redshift, Snowflake, and/or Apache Druid.\nHave a working knowledge with containerization and build pipelines\nSuccessfully implemented data systems for very large data volumes such as click streams and/or IoT sensors data.\n\n\nTHE GLOBE AND MAIL INC. IS DEDICATED TO EQUITY IN THE WORKPLACE\nAt The Globe and Mail, we are committed to fostering an inclusive, accessible work environment, where all employees feel valued, respected and supported. The Globe and Mail offers accommodation for applicants with disabilities as part of its recruitment process. If you are contacted to arrange for an interview, please advise us if you require an accommodation.", "job_collect_date": "2020-08-16T16:18:38.000Z"}, "81": {"job_id": "035a5f23df595f6f", "job_title": "Sr. Data Engineer", "job_employer": "Cox Automotive", "job_location": "Mississauga, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=035a5f23df595f6f&fccid=2ca7392810684c2b&vjs=3", "job_description": "This position will join our growing team of data and analytics experts for Cox Automotive Canada. You will be responsible for expanding and optimizing our data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. You will support our data initiatives and will ensure optimal data delivery consistent throughout ongoing projects.\n\nAs a Senior Data Engineer, you will be responsible for leading Data Engineering projects throughout their entire lifecycle, that being: initial investigation and stakeholder engagement, solution design, application development, testing and sign-off, release and maintenance\n\nThe ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. They must be self-directed and comfortable supporting the data needs of Cox Automotive Canada. The right candidate will be excited by the prospect of optimizing or even re-designing our company\u2019s data architecture to support our next generation of products and data initiatives, enabling the evolution of our Data Science and Business Intelligence\n\nResponsibilities:\nAssemble large, complex data sets that meet functional / non-functional business requirements.\nRecommend different ways to constantly improve data reliability and quality\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nImprove upon the data ingestion pipelines, ETL jobs, and alarms to maintain data integrity and data availability.\nBuild analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics\nPerforms other job-related duties as assigned or apparent and Stay up-to-date with advances in data persistence and big data technologies\nPrototype and design new data integration solutions that balance security, scalability, fault-tolerance, performance as well as cost effectiveness\nDefine, document and champion best practice architectural patterns and work with wider technology development teams to proactively seek out new opportunities to utilise Data Solutions technology and data assets\nBe responsible for leading the delivery of multiple Data Engineering development projects at a time. Project delivery responsibility extends from design, development, testing, release and maintenance\nBe responsible for leading the delivery of multiple Data Engineering development projects at a time. Project delivery responsibility extends from design, development, testing, release and maintenance\nQualifications\n\nQualifications\n\n3+ years\u2019 technical experience working in Big Data, designing solutions to complex data ingestion problems\nGraduate degree in Computer Science, Computer Engineering or a related field\nBig Data Certification and AWS certification would be an asset\nMust have strong Data Orchestration experience using tools such has AWS Step Functions, Lambda, AWS Data Pipeline, Apache Nifi.\nAdvance Python Skills to develop efficient, decouple Data pipeline\nIn-depth knowledge of AWS Big Data Services\nMust possess in-depth knowledge and hands on development experience in building Distributed Big Data Solutions including ingestion, caching, processing, consumption, logging & monitoring)\nMust have a strong understanding and experience with Cloud Storage infrastructure and operationalizing AWS based storage services & solutions prefer S3 or related specific business questions and identify opportunities for improvement\nExperience in building processes supporting data transformation, data structures, metadata, dependency and workload management\nA successful history of manipulating, processing and extracting value from large disconnected datasets\nWorking knowledge of message queuing, stream processing, and highly scalable data stores\nAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases\nExperience supporting and working with cross-functional teams in a dynamic environment\nExperience deploying data pipelines with CI/CD\nExperience with Snowflake Data Warehouse is plus\nOrganization: Cox Automotive\nPrimary Location: Canada-Ontario-Mississauga-2233 Argentia Rd\nEmployee Status: Regular\nJob Level: Individual Contributor\nShift: Day Job -\nTravel: Yes, 5 % of the Time\nSchedule: Full-time\nUnposting Date: Ongoing", "job_collect_date": "2020-08-16T16:18:39.000Z"}, "82": {"job_id": "c475abcdee2df0df", "job_title": "Senior Data Engineer", "job_employer": "Coursera", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=c475abcdee2df0df&fccid=ef63980be6005ec6&vjs=3", "job_description": "Coursera is a leading online learning platform for higher education, where 64 million learners from around the world come to learn skills of the future. More than 200 of the world\u2019s top universities and industry educators partner with Coursera to offer courses, Specializations, certificates, and degree programs. 2,500 companies trust the company\u2019s enterprise platform Coursera for Business to transform their talent. Coursera for Government equips government employees and citizens with in-demand skills to build a competitive workforce. Coursera for Campus empowers any university to offer high-quality, job-relevant online education to students, alumni, faculty, and staff. Coursera is backed by leading investors that include Kleiner Perkins, New Enterprise Associates, Learn Capital, and SEEK Group.\n\nData Engineering is unique at Coursera. Our team doesn\u2019t simply build reports on demand. Rather, we build the semantic infrastructure and products that empower our internal and external customers with the data to innovate and perform their jobs better.\nWe\u2019re looking for a senior data engineer in Toronto who can help us drive data engineering efforts for our platform. In this role, you will work with cross-functional teams to design, develop, and deploy data solutions. Our ideal candidate is an independent, analytically-minded individual with strong data modeling and engineering skills, who shares our passion for education.\nYour responsibilities\nArchitect scalable data models and build efficient and reliable ETL pipelines to bring the data into our core data lake\nDesign, build, and launch visualization and self-serve analytics products that empower our internal and external customers with flexible insights\nBe a technical leader for the team; guide technical and architectural designs for the major team initiatives; mentor junior members of the team\nBuild data expertise, and partner with data scientists, product managers and engineers to define and standardize business rules and maintain high-fidelity data\nDefine and partner with other engineers in the development of new tools to enable our customers to understand and access data more efficiently\nWork cross-functionally (eg: product managers, engineers, business teams) to support new product and feature launches\nYour skills\n5+ years experience in a data-related field, including data engineering, data warehousing, business intelligence, data visualization, and/or data science\nStrong data engineering skills and at least one scripting language (e.g., Python)\nProficient with relational databases and SQL\nFamiliarity and experience with big data technologies (eg: Hive, Spark, Presto) preferred\nAbility to communicate technical concepts clearly and concisely\nIndependence and passion for innovation and learning new technologies\n\n\nIf this opportunity interests you, you might like these courses on Coursera -\n\nBig Data Specialization\n\nBig Data Essentials: HDFS, MapReduce and Spark RDD\n\nData Warehousing for Business Intelligence\n\nCoursera is an Equal Employment Opportunity Employer and considers all qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, age, marital status, national origin, protected veteran status, disability, or any other legally protected class.\n\nIf you are an individual with a disability and require a reasonable accommodation to complete any part of the application process, please contact us at accommodations@coursera.org.\n\nPlease review our CCPA Applicant Notice here.", "job_collect_date": "2020-08-16T16:18:39.000Z"}, "83": {"job_id": "e1337dfa4d5c3f3b", "job_title": "Data Engineer", "job_employer": "Pet Valu", "job_location": "Markham, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=e1337dfa4d5c3f3b&fccid=95581e84c2b81b02&vjs=3", "job_description": "Location: Markham, Ontario\nJob Description:\nJob Summary\nThis role requires a dynamic individual with experience and passion for using data and analytics to drive business results and help us build the foundation to use our rapidly expanding data lake to improve business decisions. As a member of the Marketing team, the Data Engineer will be a key contributing resource to continually measure, evaluate, and make recommendations on our website marketing efforts as well as ecommerce user-experience.\nResponsibilities\n\u2022 Design, implement, track performance and refresh advanced analytic automated segmentation and predictive models\n\u2022 Monitor Data quality with IT to ensure robust and accurate data\n\u2022 Partner with IT to define data solutions from new data sources, and build requirements for extraction into the data lake\n\u2022 Develop a strong acumen in source and downstream data storage system, and understand how the data is associated with business actions and potential solutions\n\u2022 Will function as the primary liaison between marketing and IT\nExperience and Education\n\u2022 5+ years designing data processes to automate organizational decisions ideally in a retail organization\n\u2022 Bachelor's Degree in Statistics, Business, Quantitative Economics, Mathematics, Marketing, Economics, Engineering, Operations Research or similar programs\nCompetencies and Skills\n\u2022 Strong Personal Drive for Excellence\n\u2022 Excellent organizational and time management skills, with the ability to manage multiple priorities in a high demand environment\n\u2022 Great sense of urgency and accountability, results-oriented with strong execution skills\n\u2022 An independent problem solver, must be able to find creative solutions to unusual or unprecedented questions\n\u2022 Quick learner \u2013 eagerness to learn about new tools and business systems to help craft solid solutions\nTechnology\n\u2022 Experience in relational and cloud data storage using advanced SQL, using technologies such as Snowflake, Oracle, SQL Server\n\u2022 Must be proficient in R, Python, Alteryx, Azure ML, or similar Machine Learning and data processing technology,\n\u2022 Solid understanding of BI tools such as Tableau, MicroStrategy or QlikView\n\u2022 Experience in Spark, Kafka, JAVA a strong plus\n\u2022 Proficiency in processing Web Analytic raw data, ideally from Google Analytics 360, a large plus\n\u2022 Snowflake, Orcale, SQL ServerSnowflake, Oracle, SQL Server\n\u2022 Must be proficient in either R or Python\n\u2022 Solid understanding in data visualization tools such as Tableau, MicroStrategy or QlikView\n\u2022 Experience in Spark, Kafka, JAVA a strong plus\n\u2022 Very comfortable with MS office (Word/Excel/Access/PowerPoint)\n\u2022 Understanding of using SQL against relational databases\n\u2022 Experience in data visualization and reporting tools, such as Tableau\n#INDC", "job_collect_date": "2020-08-16T16:18:40.000Z"}, "84": {"job_id": "256688ca085e919f", "job_title": "Senior Data Engineer", "job_employer": "Myant", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=256688ca085e919f&fccid=5d784228b1eee537&vjs=3", "job_description": "About us:\nAt Myant, we are creating the world\u2019s first textile computing platform, integrating technology directly into the only thing we\u2019ve been wearing our entire life \u2013 clothing. SKIIN is our first consumer facing brand, and SKIIN\u2019s vision is to enhance human ability through connected clothing - think Ironman\u2019s suit, but comfortable. The sensors and actuators embedded within our apparel create your Digital Identity, which will be consumed by those who matter to you - your family members, doctors, coaches, other IoT devices - without you consciously having to think about it. Imagine a world where you walk into your house and the temperature automatically adjusts to your optimal body temperature, the lights adjust to your mood, you can monitor and adjust your everyday lifestyle based on your vital signs, or your doctor is aware of the onset of a disease before you even visit. The line between the digital and physical world is becoming increasingly blurry, and we believe textile is the next medium to bridge that gap.\nWe\u2019re looking for people who believe in our mission to make wearable technology truly ubiquitous and convenient, so that everyone can benefit from it. We are a cross-functional team solving big challenges at the intersection of fashion, electronics, software, and data science.\nResponsibilities:\nTest the performance of the algorithms developed by Data Science team\nLeverage native APIs for integration of AWS platforms\nTake ownership of all your deliverables and communicate your results to timely project delivery\nPrepare reports and some technical documentations\n\nQualifications Required:\nBachelor\u2019s Degree in Computer Science, Computer Engineering, or equivalent work experience\nProficiency with JavaScript and Python language\nBasic knowledge of machine learning algorithm and libraries like keras, tensorflow, sklearn\nExperience in building RESTful APIs following Micro-Services Architecture\nExperienced in NodeJS, PostgreSQL, and GraphQL.\nSignificant experience in building microservices leveraging various AWS features (AWS Lambda, IAM, SQS, DynamoDB, Kinesis, Redshift, Aurora, EC2, S3, API Gateway etc.)\nSolid experience in Biomedical signal processing, and data mining related to physiological patient data is a bonus\nPowered by JazzHR\n8SQJftU9nl", "job_collect_date": "2020-08-16T16:18:41.000Z"}, "85": {"job_id": "3cc1d76cb36c1303", "job_title": "Data Engineer", "job_employer": "Paytm", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=3cc1d76cb36c1303&fccid=4e917d9a3b14765d&vjs=3", "job_description": "About Paytm Labs:\n At Paytm Labs, we build technologies that powers Paytm India, the world's\u2019 fastest growing mobile payments and commerce ecosystem. In addition to, the Paytm Canada app. We use our skills and our biggest asset \u2013 data, to make our dent in this universe.\n\nWe are committed to offering the most transparent, secure, and personalized consumer experience to over 230 million users. We believe that this kind of scale, and the unique problems that it presents attracts curious candidates like yourself.\n\nJob Description:\nIf working with billions of events, petabytes of data and optimizing for last millisecond is something that excites you then read on! We are looking for Data Engineers who have seen their fair share of messy data sets and have been able to structure them for building useful AI products.\n\nYou will be working on writing frameworks building for real time and batch pipelines to ingest and transform events(108 scale) from 100\u2019s of applications every day. Our ML and Software engineers consume these for building data products like personalization and fraud detection. You will also help optimize the feature pipelines for fast execution and work with software engineers to build event driven microservices.\n\nYou will get to put cutting edge tech in production and freedom to experiment with new frameworks, try new ways to optimize and resources to build next big thing in fintech using data!\nRequirements:\nYou have previously worked on building serious data pipelines ingesting and transforming > 10 ^6 events per minute and terabytes of data per day.\nYou are passionate about producing clean, maintainable and testable code part of real-time data pipeline.\nYou understand how microservices work and are familiar with concepts of data modelling.\nYou can connect different services and processes together even if you have not worked with them before and follow the flow of data through various pipelines to debug data issues.\nYou have worked with Spark and Kafka before and have experimented or heard about Flink/Druid/Ignite/Presto/Athena and understand when to use one over the other.\nOn a bad day maintaining zookeeper and bringing up cluster doesn\u2019t bother you.\nYou may not be a networking expert but you understand issues with ingesting data from applications in multiple data centres across geographies, on-premise and cloud and will find a way to solve them.\nProficient in Java/Scala/Python/Spark\nWhat we Offer!\nWe are proud to announce that we have been certified as a Great Place to Work!\nA collaborative, open work environment that fosters ownership, creativity, and urgency\nEnrolment in the Group Health Benefits plan right from Day 1, no waiting period\nFuel for the day: Weekly delivery of groceries and all types of snacks to our office\nAll types of signature drinks from coffee to lattes to cappuccinos\nCatered lunch and desserts on a monthly basis!\nPing Pong and Pool: Become the next Paytm Labs Table Tennis/ Pool champ!\nAnd so much more!\nDon't have Paytm Canada App yet?\nCheck us out in the Google Play or App Store.\n\nWe thank all applicants, however, only those selected for an interview will be contacted.\n\nPaytm Labs is committed to meeting the accessibility needs of all individuals in accordance with the Accessibility for Ontarians with Disabilities Act (AODA) and the Ontario Human Rights Code (OHRC). Should you require accommodations during the recruitment and selection process, please let us know. Paytm Labs is an equal opportunity employer.", "job_collect_date": "2020-08-16T16:18:41.000Z"}, "86": {"job_id": "e8f01b72a171ac29", "job_title": "Data Engineer", "job_employer": "goeasy", "job_location": "Mississauga, ON", "job_link": "https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DbNJQGwjC-NEA80-qlVyEGmSv34Xx0eW4t4u9rXSv_7_tlvMU2e-Gw_wrZ47s1zPCnrySU2yuEL07wtSyx7XOwYh5VPcVL7CxRhOYwHzR-CdmyBrwbeqommOn6y9FD_uKIYpUCb9-PC_XgwGzBDrPy7SD7JQndrQfke5qYpMa2Hs4kVVPDwFqyUMpSpsGJ173UxCcnqfTyQp_SFu4jD7pxOMoUdtrAaQx3s0XjPnfc98XG7qQcpBa9DkpkvGq20zAwbc0dZlM8CeBFS4YB-rxFOv68h4-dpGJuijITM1gzspjyXJhbAwILtKLHwp2bXvfzlopAecbNBVrzRtto_mQKN9_lzAxls1676LZO806OYUEI6cJ27-FWOwbFSoG_vhYOZAOmmc3nLEcFxiODaIFk-zeBxPBuy9sOyfmLMOhg9kb-rkYsLonPfGSGQ7KAL7aiHbEyyeI2IAuJgbVT1zvTDX57oBfn0ns-aEew4gOPGQ==&p=10&fvj=0&vjs=3", "job_description": "If you are looking to join one of Canada\u2019s fastest growing companies, goeasy Ltd. is the place for you! Recognized as one of Canada\u2019s Most Admired Corporate Cultures, one of Canada\u2019s Top 50 Fintech\u2019s and one of North America\u2019s Most Engaged Workplaces, we want the best and brightest to join our team.\n\nWe are a publicly traded company on the TSX with over 4000% shareholder return since 2001, goeasy operates two main business units. easyfinancial is our consumer lending business that offers secured and unsecured installment loans of up to $35,000 and easyhome is Canada\u2019s largest merchandise lease-to-own company. It is our mission to provide everyday Canadians the chance for a better tomorrow, today by giving them access to the credit they need and by offering them a second chance when they have been turned down by banks and traditional lenders. With a retail network of nearly 400 locations across Canada and over 1900 employees, we are able to build lasting relationships with our customers as we help them rebuild their credit and graduate towards prime rates and a brighter financial future.\n\nThe Data Engineer on the Data Science and Business Insights team will build, integrate data from various resources, manage data in goeasy operational data store and enterprise data warehouse. This position will develop ETL (Extract, Transform and Load) with various tools on large datasets to ensure data is easily accessible, works smoothly, as well as maintain and expand the data warehouse for reporting and analysis. The Data Engineer will also work closely with the data architect on the design and architecture of our enterprise data warehouse.\n\nResponsibilities:\n\nDevelop data set processes for data modeling, mining and production\nDevelop and maintain ETL processes using SSIS, Scripting and data replication technologies\nParticipate in development of datamarts for reports and data visualization solutions\nResearch opportunities for data acquisition and new uses for existing data\nIntegrate new data management technologies and software engineering tools into existing structures\nSupport the translation of business requirements for data acquisition/manipulation and provide detailed specifications that can be passed downstream for use\nDevelop detailed technical specifications and operational support documentation in collaboration with Business Systems Analysts, BI Engineers and Architects.\nIdentify and communicate technical problems, process and solutions\nCreate Ad-Hoc queries and reports as needed along with providing on-going analytical support for these requests\nAssist in the collection and documentation of user\u2019s requirements\nEnsure that existing business processes dependent on the ODS/EDW are monitored and respond quickly to bug fixes, enhancement requests and production ETL related issues.\nDealing with the database users on a daily basis to ensure that problems are dealt with promptly and that appropriate fixes are made to resolve any problems.\nRecommend ways to improve data reliability, efficiency and quality\nEnsure systems meet business requirements and industry practices\nWork effectively with the Business Intelligence and Data Solutions Architects, Data and BI Engineers to ensure that all approved development and deployment procedures are followed.\n\nQualifications:\n\nBachelor\u2019s Degree in Computer Science, MIS, Computer Engineering or other Information Technology related degree\n4+ years working with SQL Server or comparable relational database system\n3+ years of extensive ETL development experience with SSIS and/or ADF\n4+ years of experience troubleshooting within a Data Warehouse environment\nExpert domain knowledge & experience in Data warehousing, encompassing data model design, dimensional modeling, naming conventions, cross-cutting concerns, common integration technologies, patterns & standards and emerging technologies.\nExpert Knowledge of SQL skills to build, debug, and optimize (developing procedures, functions, SQL queries, etc.) and working with large data sets and to perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.\n2+ years SQL Server Database administration experience\nCloud experience (Azure) is highly preferred\nExposure and experience with Python, R, Hadoop, Azure, other Big data and advanced analytics\nKnowledge of AI and ML developments/solutions/implementations\nExperience with multiple programming languages (PowerShell scripting, C#, others) with basic scripting skills.\nHigh level of technical aptitude\n\nInclusion and Equal Opportunity Employment\n\ngoeasy is an equal opportunity employer. In addition, goeasy is committed to providing accommodations for applicants upon request at any stage of the recruitment process in accordance with all legislative requirements throughout Canada. Please let us know if you require an accommodation due to a disability during any aspect of the recruitment process and we will work with you to address your needs.\n\nAdditional Information:\n\nAll candidates considered for hire must successfully pass a criminal background check, credit check, and validation of their work experience to qualify for hire. We thank all interested applicants, however we will only be contacting those for interview who possess the skills and qualifications outlined above.\n\nWhy should you work for goeasy?\n\nTo learn more about our great company please click the links below:\n\nPAID1234", "job_collect_date": "2020-08-16T16:18:42.000Z"}, "87": {"job_id": "ed86ae879d5f5db2", "job_title": "Senior Big Data Engineer (f/m/x)", "job_employer": "Avira Operations GmbH & Co. KG", "job_location": "Canada", "job_link": "https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DVAdzec4bw6rQ4FO3PDdwz3gCZ43d7Gsr17jnu3o1g5CsoUOgyHaCzZOjoCGyji8ntYJxUdGd2FmwbktcVakqfF59toD5WP-Ro_DzTHrpq6YpYqUfP4QH5sxN4c835IUO3mqkddooJyzOy5iObgoXm6nmIVAYiD64A4oqzCBZE9D3695dcxeZ6SABBHoZDnMxUs5K6gSsbRhwawxvlEowEc2SLd0TXurqLBb6zuwHQ6XFItMZCGiySYx-Eu5njoRj01oUhO1SnyPVH8GsjUgYgVPB6sk0b5BAMRjkPi040aeEbnxsm8bnc-p-3KhdHSvzC_Xc5ZaZL_YTz_CC6Ge0D7cfXL4xHDnwsJytcCtc6e2GEq4GgJl0R4Fv4l6fvMWLfYD6DgcVBAR_TIebybFSqm9hsM9_aZKLhzx1lWw2pwk3OHbz3p7dx_JATNkMEQgtDjGFynwrU5g==&p=11&fvj=0&vjs=3", "job_description": "Senior Big Data Engineer (f/m/x)\n\nLocation: Tettnang\n\nThe Challenge\n\nAs a Data Engineer in the Threat Intelligence team, you will design, implement and maintain the Threat Intelligence Platform that transforms massive amounts of (real-time) data from various sources into descriptive knowledge about emerging threats. You challenge our status quo and define standards for data-intensive analytic pipelines. Your projects will collect and process security-related intelligence and will then provide that data to the company as a whole so Avira can protect its users on a global scale.\n\nThe team\nWe are an international team of engineers and researchers. We are a self-organized and result-oriented team, always looking to perfect the art of automation. Our systems and services are integrated within the Avira Protection Cloud that protect consumers and businesses around the world.\n\nWhat you will do\nStarting from day one you will familiarize yourself with our big data ecosystem and tool stack, as well as the main data source systems.\n\nWithin the first 3 months, you are fully integrated into the Threat Intelligence Platform development and maintenance. You are taking responsibilities for certain data stream integrations.\n\nWithin the first 6 months you are an expert for the Threat Intelligence Platform having a complete understanding of its integration into Avira\u2019s protection technologies and architecture landscape.\n\nWithin the first year you will be truly integrated within a team that delivers the most up-to-date Threat Intelligence solutions to millions of customers. You take full ownership of cross-department projects and collaborate efficiently with other teams. You are keen to improve existing technologies and implement new and innovative features. You also play a big part in securing and supporting the digital lifestyle of our customers and making the Internet a more secure place.\n\nYour Qualifications\n3+ years' experience in software development, with excellent development skills in PythonIndustrial experience with data-intensive projects in the Hadoop ecosystem, Spark, Kafka, and AirflowExperience in building data ingestion pipelinesGood knowledge in designing, building, using and maintaining REST APIsExperience with SQL/NoSQL databases, especially creating scalable, multi-node deploymentsExperience with AWS components and principles are a plusStrong analytical, technical, organizational and communication skills\nThe position is based in Tettnang, Germany, near lake Constance.\n\nBenefits and perks:\nNew Work\nStylish building with roof terracesCanteen and ChocaViraModern office concept\n\nLearning & Development\nUnlimited access to UdemyTrainings & Conferences\nSpecialist Career\n\nHealth & Wellbeing\nGym and fitness courses\u201cJobRad\u201d bike leasingMedical checkups\n\nFamily & Living\nRelocation PackageVacation child careAvira Prime licences\n\nEvents\nOnboarding eventsMonthly Employee MeetingsSummer & Christmas parties\n\nAN OPPORTUNITY TO MAKE A DIFFERENCE\nUpdate: Although there\u2019s a lot of disruption nowadays due to Covid-19, we at Avira are continuing to run our daily business activities so that we stay true to the promise to our customers - now even more than ever: Protecting people in the connected world.\n\nAnd we are doing this from the safeness of our own homes. Among other things, this means we are still hiring, but we have moved all our interviews online and all our colleagues are being onboarded remotely\n\nSo join us and you will be able to work from home until the danger is over.\n\nWe\u2019re an international software company at the forefront of imagining the future of digital security. Avira\u2019s award-winning products and technology protect over 500 million users in the connected world.\n\nWhat makes us special? First and foremost \u2013 it\u2019s the authentic people at Avira. We have a great community feeling that fosters your uniqueness and offers the space to reflect, the feedback to grow, and the freedom to innovate. If you are looking for a culture that also encourages aspiration and professional excellence, get in touch with us and discover the Avira experience firsthand.\n\nLena Komarek\nHR Recruiter\nAvira Operations GmbH & Co. KG\nHuman Resources\nKaplaneiweg 1\nD-88069 Tettnang, Germany\nPhone: +49 (0) 7542-500 -2207\nE-Mail: lena.komarek(at)avira.com", "job_collect_date": "2020-08-16T16:18:43.000Z"}, "88": {"job_id": "997673b8effa647f", "job_title": "Data Engineer (Intermediate 4-6 years) - Contract - Remote during COVID", "job_employer": "CorGTA", "job_location": "North York, ON", "job_link": "https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Dkh866153otbRJ8nVMuEXnXPd315JS377__3VyD6HnNQkmcaOHMemmy5pLrNV6j0WaVWEWWabnp4PP6-2u77DLfT_FCIGvCW8CQmQW140Z7EVU32Jnwl42FALsxnbc4n5M6FOTfoW2OoTMOcGG21HZGOoXL7fWT8eaT67VlcClh4sHQcJsHCX__l2zhjBVHiMWaNf13moHq_65-fpSHWZgBS5Ucs0HiY5i5k6PirB1DadF7g50RGZ9dNwRIbHeNMMLd-0iwFO4mIRUreHPqgFyF_phO8P2QvqvGGxTVTKmFI_IgrQINZ6IAe4vjm8HAUGZ6GSbeciq1UdVl5KkwgXzu3Qzi57Jeyv_pefMNIaQZ7Y5uH02PnOd128vw8sbcSijrNMyffBq_adXuiivoZ5m5IINpEWe6uqviWSCyU_xIjCQkKoyE6Lly1S5M-Z5SW-MSwGn99i7KSJ5cgN5kgO8puGxNNt5A19wAsQRjJYo_qalra5pg0ZD&p=12&fvj=1&vjs=3", "job_description": "Role: Data Engineer (Intermediate 4-6 years)Structure: Contract (6 months initially)Location: North York, ONHours: Monday - Friday 40 hours (remote during COVID-19)Pay: $50.00 p/h inc.The role: We are looking for Intermediate Data Engineers who have roughly between 4-6 years experience who ideally has experience moving data from Google cloud over to Big query.However, if you are someone who has good experience with ETL and Data Pipeline Design & Data Validation experience and can script in Python and create strong SQL Queries then this will also be a potential match for this role.Ideal Qualifications to be successful in this role: - Degree in Computer Science, Engineering, Information Systems (or equivalent combination of skill and experience)- 4-6 years of experience working with data architecture projects- Excellent SQL coding and experience with a broad array of development tools and platforms including exposure to a big data environment tools/languages, such as SQL, R, SAS, Python, etc.- Experience creating relational database design and data models- Experience with various analytical data platforms and technologies- Experience with using REST API, Cloud (GCP Ideally)- Experience in custom or structured data integration design, implementation, and maintenance- Experience with business intelligence tools such as Qlikview, Tableau and Microstrategy (Nice to have for Data Visualizations)- Previous experience working in retail or eCommerce is highly preferred- Ability to curate data to tell stories and provide business insightsPlease apply with an updated resume and ensure the required skills you are able to speak to for this position are included.For more roles like this please go to www.corgta.com/find-a-job/Job Type: Full-timeSalary: $45.00-$50.00 per hour", "job_collect_date": "2020-08-16T16:18:44.000Z"}, "89": {"job_id": "54e0fcc38385e3a2", "job_title": "Data Engineer/Integration Consultant", "job_employer": "Copperstone Connect", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Ceb7pqR8iGmLuXIZQklF6pqed3xddONNM6Jjumj-ndyGK0kULQUmQ2REqfJ5KAzFVeK_es1WVg0s1sLgKW09UsvVRp0-dSizFQr1oNyK_VIqdjla00VU9GNVvu2x4v9p3hsL7NeF-6hBksbAaSgKazsaYTODGMKQQeVuYcxoOuxvFKuD0HRLD9ybgbcXTn1LPlSAr6hhedor2Fs91CAtujMfUpgNORbw26_r_GB9gHYDcKIBANzWNcVghLLEBEQtJbBlN33UGZcGQ01IXOD7VpUJXMay9GgJtjq6Y3ZxluDna551RGa_79TCX8eYH120AttmvadFk_AoLLO3xG3C7n2JNF6jsF8Cu4dPz4W6zjesakw7yZvloCIluAOgmoc0nT3JgO-6kZW62QSpYL2jM0ky0xV_y2oNEsjr-rCPW8qyiQXbycrPXSP8S-PwQFs6XkCHVeeBTYwy0gE0-s9zV-X1gXUKtaCog8E9GLGafPbLTnOgAFeZLSdEdm2hzHFofzp9NlTAq30LCPsC5kwlE9KefVdLpYBNU2hvus-tNFYq9S56SS6MXjMqqvQMq96yTR9qZ00cyFhS6l-m72JnvKDgDC0qiSPJAHikNJRRPhmhgLUzWpRDfhUfREQJuzYoIP4aOUjBJDNDL7fOvKt3d_XWcN2nN5NqWp6jQy8Lf4Cg==&p=13&fvj=0&vjs=3", "job_description": "Our client, a boutique-size consulting firm, prides itself on being at the forefront of innovation in the Big Data space. Founded in 2010, they provide thought leadership and implementation excellence within the ever-growing data and analytics world.\nThey take pride in having some of the most highly trained and experienced consultants in the industry which translates into optimal value for their clients. They were one of the first companies to provide analytics and data as a service, via the cloud, as early as 2010. They strive to make sure their customers are well-positioned with the best technologies/tools in the industry, constantly evaluating new and existing technology partnerships. Some of the more prominent companies they have partnered with include; Snowflake, DataRobot, MicroStrategy, Informatica, Amazon AWS & Microsoft. They continue to invest in their most valuable resource, their people. They do this through extensive training both on the job and through various educational programs.\nDue to growth, they are seeking to hire an Intermediate - Senior Integration Consultant.\n\nDesired Skills and Experience\n: University/College degree in Computer Science, Mathematics, Data Science and/or Relevant Degree\n: 5+ years hands-on development, configuration, scripting and administration experience with Data Integration platforms. (i.e. Informatica, Talend, DataStage, SSIS)\n: BI Experience (MicroStrategy, Looker, Tableau, PowerBI) considered a nice to have\n: Extensive theoretical and practical knowledge of data warehousing principles/concepts and practical development experience in all areas of the data warehousing life cycle\n: Experience with Data Management, ETL, Cloud Data (AWS), Data Integration\n: Knowledge of OLAP-related principles and concepts\n: Strong grasp of data modeling techniques and concepts (Normalized/Denormalized, Conceptual/Logical/Physical, Star, Snowflake, Data Vault)\n: Strong knowledge and experience with relational databases such as Snowflake, SQL Server, Oracle (Advanced knowledge of reading and writing SQL, Performance analysis and tuning)\n: Knowledge and experience with key Big Data technologies (Hive, Presto, Spark, Kafka, NoSQL databases, Semi-structured data access patterns (Json, Parquet, XML, etc.))\n: Strong Python scripting skills\n: Excellent communication skills\n: Great problem-solving skills\n: Leadership and good client management skills\n\nDay to Day Activities Would Include\n: Conduct relevant customer interviews to determine key business requirements and objectives\n: Build appropriate analytical data models based on outcomes of user interviews\n: Analyze and profile data systems to build source to target data mappings\n: Review ETL performance and conducts performance tuning as required on mappings/workflows or SQL\n: Administration and support of data integration infrastructure\n: 2nd level on-call support of ETL services as required\n\nYou will be responsible for attaining the following goals:\n: Attaining a minimum of 1 new accreditation/certification per year\n: Spending 80% or more of their time on billable work\n: Completing 90% or more of their agile delivery tasks on time\n: Demonstrating competency in 1 new relevant technology every year", "job_collect_date": "2020-08-16T16:18:45.000Z"}, "90": {"job_id": "a32b7c599156a218", "job_title": "BHJOB15656_15145 - Data Engineer w/ DevOps - Azure Cloud", "job_employer": "Myticas Consulting", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Bt274jo0r_e0cCCqhskoQvohXG3bfDAo8rIdE-Msu3yuVfyF2oiDFKHFOU-vxVulAfe8kPL7riFwoXa1bsfN-szjxO_8qUQWbPsJOnTg0EkutIoK2S4NNLL24JQh9e4DsE9hv907k2_zDbvj0a8WAj5oPSV5J0YhhOguMmzdyDApvVREuw5Z9ETLmxjrJLy8hOsuxjnKrXi1owXlFe_IlhyeTQ1NmdCPTxqAoKmbVYdz-d24ZL3_aaz9kcOTlQoc2jKUmWxX_g5lbOO5LNxCqnbx4T5ZQPArP9wpjF33C57WCQ8gsDTc097EaHTnq3n6igVGqGv9hpXn7p7B6jkOlDVbuFzIcYjDGw1IT2IwzCw5sapVcPqmWtmwZnehdWxTpSsZTwVuiU1hQ7U3fHBI7SAd1I-F0Ya3-L8-zxBHh9o7fhq21R8dy54fAoYfTgfEDwZugJnaW8wg==&p=14&fvj=0&vjs=3", "job_description": "The growing team here at Myticas is looking for an experienced Data Engineer who would be interested in a remote contract opportunity within the Ottawa, Toronto and Montreal regions.\n\nAccountabilities:\n\nDefining and reviewing security design requirements for cloud infrastructure and application components.\nEvaluating architecture patterns from security perspective.\nBuilding and implementing security controls to enable enforcement of compliance with Cloud Control Objectives, using custom Azure policies and integrated controls in DevOps processes\n\nRequirements:\n\nStrong Data Engineer w/ DevOps expertise + Azure Cloud Experience\nMust know how to code and stand up scripts.\nExperience with Data Digestions\nExperience writing scripts to automate (infrastructure)\nARM Templating Expertise\nAzure Synapse Expertise\nSupport developing automated DevOps processes and procedures for the following Azure components:\nAzure Synapse (Azure DW) & Studio (private preview)\nAzure Data Catalog Gen 2 (Babylon \u2013 private preview)\nAzure Data Lake Storage Gen 2\nAzure ML\nML Flow\nAzure SQL Analysis Service\nAzure Databricks\nADF data pipelines for data loading to AzSQL/Synapse\nADF data pipelines for connecting to on-prem data sources for data\n\nCandidates looking to apply for this role are to send us an updated version of their resume in confidence. Our team will be sure to review all applicants and follow up accordingly at the conclusion of the review process.\n\nJob is also known as: Data Engineer, DevOps Engineer, Azure Cloud Engineer, Azure Engineer, Cloud Engineer\n\nINDMY", "job_collect_date": "2020-08-16T16:18:46.000Z"}, "91": {"job_id": "e8f01b72a171ac29", "job_title": "Data Engineer", "job_employer": "goeasy", "job_location": "Mississauga, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=e8f01b72a171ac29&fccid=6df7987324612088&vjs=3", "job_description": "If you are looking to join one of Canada\u2019s fastest growing companies, goeasy Ltd. is the place for you! Recognized as one of Canada\u2019s Most Admired Corporate Cultures, one of Canada\u2019s Top 50 Fintech\u2019s and one of North America\u2019s Most Engaged Workplaces, we want the best and brightest to join our team.\n\nWe are a publicly traded company on the TSX with over 4000% shareholder return since 2001, goeasy operates two main business units. easyfinancial is our consumer lending business that offers secured and unsecured installment loans of up to $35,000 and easyhome is Canada\u2019s largest merchandise lease-to-own company. It is our mission to provide everyday Canadians the chance for a better tomorrow, today by giving them access to the credit they need and by offering them a second chance when they have been turned down by banks and traditional lenders. With a retail network of nearly 400 locations across Canada and over 1900 employees, we are able to build lasting relationships with our customers as we help them rebuild their credit and graduate towards prime rates and a brighter financial future.\n\nThe Data Engineer on the Data Science and Business Insights team will build, integrate data from various resources, manage data in goeasy operational data store and enterprise data warehouse. This position will develop ETL (Extract, Transform and Load) with various tools on large datasets to ensure data is easily accessible, works smoothly, as well as maintain and expand the data warehouse for reporting and analysis. The Data Engineer will also work closely with the data architect on the design and architecture of our enterprise data warehouse.\n\nResponsibilities:\n\nDevelop data set processes for data modeling, mining and production\nDevelop and maintain ETL processes using SSIS, Scripting and data replication technologies\nParticipate in development of datamarts for reports and data visualization solutions\nResearch opportunities for data acquisition and new uses for existing data\nIntegrate new data management technologies and software engineering tools into existing structures\nSupport the translation of business requirements for data acquisition/manipulation and provide detailed specifications that can be passed downstream for use\nDevelop detailed technical specifications and operational support documentation in collaboration with Business Systems Analysts, BI Engineers and Architects.\nIdentify and communicate technical problems, process and solutions\nCreate Ad-Hoc queries and reports as needed along with providing on-going analytical support for these requests\nAssist in the collection and documentation of user\u2019s requirements\nEnsure that existing business processes dependent on the ODS/EDW are monitored and respond quickly to bug fixes, enhancement requests and production ETL related issues.\nDealing with the database users on a daily basis to ensure that problems are dealt with promptly and that appropriate fixes are made to resolve any problems.\nRecommend ways to improve data reliability, efficiency and quality\nEnsure systems meet business requirements and industry practices\nWork effectively with the Business Intelligence and Data Solutions Architects, Data and BI Engineers to ensure that all approved development and deployment procedures are followed.\n\nQualifications:\n\nBachelor\u2019s Degree in Computer Science, MIS, Computer Engineering or other Information Technology related degree\n4+ years working with SQL Server or comparable relational database system\n3+ years of extensive ETL development experience with SSIS and/or ADF\n4+ years of experience troubleshooting within a Data Warehouse environment\nExpert domain knowledge & experience in Data warehousing, encompassing data model design, dimensional modeling, naming conventions, cross-cutting concerns, common integration technologies, patterns & standards and emerging technologies.\nExpert Knowledge of SQL skills to build, debug, and optimize (developing procedures, functions, SQL queries, etc.) and working with large data sets and to perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.\n2+ years SQL Server Database administration experience\nCloud experience (Azure) is highly preferred\nExposure and experience with Python, R, Hadoop, Azure, other Big data and advanced analytics\nKnowledge of AI and ML developments/solutions/implementations\nExperience with multiple programming languages (PowerShell scripting, C#, others) with basic scripting skills.\nHigh level of technical aptitude\n\nInclusion and Equal Opportunity Employment\n\ngoeasy is an equal opportunity employer. In addition, goeasy is committed to providing accommodations for applicants upon request at any stage of the recruitment process in accordance with all legislative requirements throughout Canada. Please let us know if you require an accommodation due to a disability during any aspect of the recruitment process and we will work with you to address your needs.\n\nAdditional Information:\n\nAll candidates considered for hire must successfully pass a criminal background check, credit check, and validation of their work experience to qualify for hire. We thank all interested applicants, however we will only be contacting those for interview who possess the skills and qualifications outlined above.\n\nWhy should you work for goeasy?\n\nTo learn more about our great company please click the links below:\n\nPAID1234", "job_collect_date": "2020-08-16T16:18:47.000Z"}, "92": {"job_id": "5c06518576e750fc", "job_title": "Senior Data Engineer", "job_employer": "SADA", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=5c06518576e750fc&fccid=b704562e07a2a03f&vjs=3", "job_description": "Join SADA as a Sr. Data Engineer!\n\n\nYour Mission\n\nAs a Sr. Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data problems facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring batch or streaming data pipelines, data lakes and data warehouses.\n\nYou will be expected to run point on whole projects, end-to-end, and to mentor less experienced Data Engineers. You will be recognized as an expert within the team and will build a reputation with Google and our customers. You will demonstrate repeated delivery of project architectures and critical components that other engineers demur to you for lack of expertise. You will also participate in early-stage opportunity qualification calls, as well as lead client-facing technical discussions for established projects.\n\nPathway to Success\n\n#BeOneStepAhead: At SADA we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our engineers know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs.\n\nYour success starts by positively impacting the direction of a fast-growing practice with vision and passion. You will be measured quarterly by the breadth, magnitude, and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, how well you collaborate with your peers, and the consultative polish you bring to customer interactions.\n\nAs you continue to execute successfully, we will build a customized development plan together that leads you through the engineering or management growth tracks.\n\nExpectations\n\nRequired Travel - 30% travel to customer sites, conferences, and other related events. Due to the COVID-19 pandemic, travel has been temporarily restricted.\nCustomer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.\nTraining - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.\n\nJob Requirements\n\nRequired Credentials:\nGoogle Professional Data Engineer Certified or able to complete within the first 45 days of employment\n\nRequired Qualifications:\nMastery in at least one of the following domain areas:\nData warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).\nData migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.\nBackup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.\nExperience writing software in one or more languages such as Python, Java, Scala, or Go\nExperience building production-grade data solutions (relational and NoSQL)\nExperience with systems monitoring/alerting, capacity planning and performance tuning\nExperience in technical consulting or customer-facing role\n\nUseful Qualifications:\nExperience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)\nExperience with IoT architectures and building real-time data streaming pipelines\nExperience operationalizing machine learning models on large datasets\nDemonstrated leadership and self-direction - a willingness to teach others and learn new techniques\nDemonstrated skills in selecting the right statistical tools given a data analysis problem\n\nAbout SADA\n\nValues: We built our core values on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA's values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.\n\nMake them rave\nBe data driven\nBe one step ahead\nBe a change agent\nDo the right thing\n\nWork with the best: SADA has been the largest partner in North America for GCP since 2016 and has been named the 2019 and 2018 Google Cloud Global Partner of the Year. SADA has also been awarded Best Place to Work by Inc. as well as LA Business Journal!\n\nBenefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match, professional development reimbursement program as well as Google Certified training programs.\n\nBusiness Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.", "job_collect_date": "2020-08-16T16:18:49.000Z"}, "93": {"job_id": "4ecf6d90140603ee", "job_title": "Lead Data Engineer", "job_employer": "Manulife", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=4ecf6d90140603ee&fccid=1747adf6142beb48&vjs=3", "job_description": "Are you looking for unlimited opportunities to develop and succeed? With work that challenges and makes a difference, within a flexible and supportive environment, we can help our customers achieve their dreams and aspirations.\nJob Description\nAre you a go-getter who has a passion in building next gen data pipelines and provide Big data solution for business problems? Are you a big fan of simplification and automation?\nManulife is seeking an awesome Lead Data Engineer , with Big Data experience as well as strong understanding of data-ingestion, data curation and both Batch & Stream Data processing, to join our rapidly expanding IT Organization and assist us as we work to be a digital leader in our industries!\nSkills and Experience\nYou will have the following skills and experience:\nLead development teams to define and build data pipelines\nExpert in building and operationalizing BigData platforms in cloud using one of the public clouds, preferably MS Azure.\nHands on experience with Big Data streaming frameworks and tools (Spark Streaming, Storm, Kafka, etc.)\nExpert in Hadoop ecosystem and toolset \u2013 Sqoop, Nifi, Pig, Spark, HDFS, Hive, HBase, etc.\nExpert in automating data pipelines in a Big Data ecosystem, DevOps and CICD.\nExperience in developing Hadoop integrations (batch or streaming) for data ingestion, data mapping and data processing capabilities\nExperience programming in both compiled languages (Java, Scala) and scripting languages (Python or R)\nExpert in developing Big Data set processes for data modeling, mining and production\nExperience in working with key partners including business and technology to establish definition of success, goals, key use cases and aligning dev team on strategic priorities.\nExcellent communication and interpersonal skills\nExcellent analytics, problem solving and solutioning skills\nA capacity for constant learning from both success and failure, remaining open to change and continuous improvement\nGood to Haves\nExperience in Exploratory data analysis; Query and process Big Data, provide reports, summarize and visualize the data\nExperience in Canary deployments, 0-downtime, 0-dataloss, hot-hot DR\nExperience in designing solutions for Big Data warehouses\nExperience with Hadoop security frameworks like Knox, Ranger.\nExperience with Hadoop metadata frameworks and security policies such as Ranger, Atlas\nExperience in data profiling and analysis\nExposure to and an understanding of Agile scrum methodologies and experience of working in an Agile team\nExperience in Big Data performance analysis, tuning and capacity planning\nExperience in designing business intelligence systems, dashboard reporting, and analytical reporting is a plus\nExperience with the Hortonworks Data Platform (version 2.5)\nExperience in using Git flow.\nBasic understanding of following will be useful but not required:\nExposure to and basic understanding of collaboration tools like Slack, Skype, Teams, and JIRA\nWhat about Perks?\nManulife has lots of perks including, but not limited to:\nCompetitive compensation\nRetirement Savings Accounts including a RPP (Pension Plan), RRSP (Retirement Savings Plan), and TFSA (Tax Free Savings account)\nManulife Share Ownership Program with employer matching\nCustomizable Benefits Package including Health, Dental, Vision, and 100% of Mental Health expenses\nFinancial support for ongoing training, learning, and education\nMonthly Innovation Days (Hackathons)\nWearing jeans to work every day\nAn abundance of career paths and opportunities to advance\nA flexible work environment with flex hours, work from home arrangements, distributed teams, and condensed work week arrangements.\nThis is a full time permanent role and the team is located in Kitchener/Waterloo, Ontario. There is opportunity for Toronto based people to work in this role, however there would be travel to Kitchener / Waterloo twice per week.\nWe are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.\nAbout Manulife\nManulife Financial Corporation is a leading international financial services group that helps people achieve their dreams and aspirations by putting customers' needs first and providing the right advice and solutions. We operate primarily as John Hancock in the United States and Manulife elsewhere. We provide financial advice, insurance, as well as wealth and asset management solutions for individuals, groups and institutions. At the end of 2017, we had approximately 34,000 employees, 73,000 agents, and thousands of distribution partners, serving more than 26 million customers. As of December 31, 2017, we had over $1.04 trillion (US$829.4 billion) in assets under management and administration, and in the previous 12 months we made $26.7 billion in payments to our customers. Our principal operations are in Asia, Canada and the United States where we have served customers for more than 100 years. With our global headquarters in Toronto, Canada, we trade as 'MFC' on the Toronto, New York, and the Philippine stock exchanges and under '945' in Hong Kong\n\nIf you are ready to unleash your potential it\u2019s time to start your career with Manulife/John Hancock.\nAbout Manulife\nManulife Financial Corporation is a leading international financial services group that helps people make their decisions easier and lives better. We operate primarily as John Hancock in the United States and Manulife elsewhere. We provide financial advice, insurance, as well as wealth and asset management solutions for individuals, groups and institutions. At the end of 2018, we had more than 34,000 employees, over 82,000 agents, and thousands of distribution partners, serving almost 28 million customers. As of March 31, 2019, we had over $1.1 trillion (US$849 billion) in assets under management and administration, and in the previous 12 months we made $29.4 billion in payments to our customers.\nOur principal operations in Asia, Canada and the United States are where we have served customers for more than 100 years. With our global headquarters in Toronto, Canada, we trade as 'MFC' on the Toronto, New York, and the Philippine stock exchanges and under '945' in Hong Kong.\n\nManulife is an equal opportunity employer. We strive to attract, develop and retain a workforce that is as diverse as the customers we serve and to foster an inclusive work environment that embraces the strength of cultures and individuals. We are committed to fair recruitment, retention and advancement and we administer all of our practices and programs based on qualification and performance and without discrimination on any protected ground.\nIt is our priority to remove barriers to provide equal access to employment. A Human Resources representative will consult with applicants contacted to participate at any stage of the recruitment process who request any accommodation. Information received regarding the accommodation needs of applicants will be addressed confidentially.", "job_collect_date": "2020-08-16T16:18:49.000Z"}, "94": {"job_id": "bcb75a8b3d33578b", "job_title": "Senior Data Engineer", "job_employer": "BGIS", "job_location": "Markham, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=bcb75a8b3d33578b&fccid=84c23fbcabc14d59&vjs=3", "job_description": "Job Field:\nInformation Technology\nJob Type:\nFull-time\nBuilding Location:\nLength of Assignment:\n\nSUMMARY\n\nThe Senior Data Engineer is a deep technical expert in building complex data warehousing and business intelligence applications. At this level, the incumbent demonstrates a passion and in-depth knowledge of large, complex application development methodologies. They motivate themselves and the team to refine their skills and adopt best practices for developing pragmatic software solutions for the organization. Leading the charge, they continue to raise the bar on mastery of business intelligence application development within the team and the organization.\n\nKEY DUTIES & RESPONSIBILITIES\n\nProgramming\n\nUses in-depth knowledge of advanced programming techniques, design patterns and hardware/software interfaces to develop business intelligence and data warehouse applications.\nDesigns, tests and integrates data warehouse and BI modules and resolves programming errors using various debugging tools and techniques.\nProvides guidance/mentors on programming practices and techniques to individuals and cross-functional teams.\nProvides support, guidance and production assurance for very complex or urgent problems.\nPerforms work with minimum supervision, and work is assigned in terms of technical objectives.\nPrepares technical documentation (e.g., user guides, technical specifications).\nAssists in the design of business solutions.\n\nAnalysis\n\nConducts impact analysis for proposed changes to or problems across the system.\nLeads team discussions in the analysis and collaboration to clarify and improve specifications or to identify alternative programming solutions.\n\nContinuous Improvement\n\nMakes recommendations or decisions on architecture, application design, standards and process improvements.\nEnforces team and organizational standards and practices (e.g. at walkthroughs and peer code reviews).\nEngages in continuous learning by developing and executing on a learning plan.\nTakes responsibility for individual and the team's results.\nAdvocates for quality in all aspects of development efforts based on the team's definition of quality.\n\nRisk Management\n\nEstimates and prioritizes work to maximize value while taking into account risk, effort and dependencies.\nRaises impediments, risks, and issues as early as possible and work with the team to mitigate as needed.\n\nKNOWLEDGE & SKILLS\n\nUniversity graduation and minimum 5-10 years of relevant experience\nDemonstrates in-depth knowledge of Microsoft BI architecture, established data warehouse development methodologies, multi-dimensional data modelling, OLAP, metadata management, data security, predictive analysis and big data processing. Extensive experience in one of these cloud data warehouses (Snowflake, bigtable, Redshift), Data Vault 2.0 methodology, steaming data processing, BI components in SQL Server 2016+, TSQL, and DAX, Power BI.\nA good working knowledge of application security, C#, python, PowerShell, metadata management, NoSQL, and data security.\nExperience in programming and debugging complex data warehouse and BI applications as part of a multi-disciplinary team environment (following an agile framework such as SCRUM preferred) based on Microsoft Team Foundation Server and git.\nExperience in writing unit tests to support production code using a unit test framework.\nExperience with database management (i.e. database design, schema creation, concurrency and performance considerations).\nTakes ownership and initiative and collaborates well with a team of peers.\nDemonstrates a commitment to continuous learning (e.g. user groups, blogs, conferences, community awareness, and next generation tooling).\nAble to clearly communicate in both a verbal and written form within a predominantly English working environment.\nHas a positive, passionate, idea generating attitude when faced with challenges.\n\nLicenses and/or Professional Accreditation\n\nCertification in Microsoft technologies preferred", "job_collect_date": "2020-08-16T16:18:50.000Z"}, "95": {"job_id": "2ef0d527757decfa", "job_title": "Data Developer", "job_employer": "Fleet Complete", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=2ef0d527757decfa&fccid=c0b5558e336243b3&vjs=3", "job_description": "COMPANY OVERVIEW:\nSuccess stories like this, don\u2019t happen every day. From humble beginnings as a courier industry solutions provider in Canada, Fleet Complete quickly grew to be one of the world\u2019s leaders in telematics and connected mobility solutions for a wide variety of industries with fleets, assets and mobile workers.\n\nToday, with 20 years in the industry, Fleet Complete is one of the fastest-growing IoT (Internet of Things) companies across the globe, operating in 17 countries with offices in Canada, Netherlands, Denmark, Belgium, Estonia, Latvia, Lithuania and Australia. Fleet Complete continues to win employer, innovation thanks to our relentless customer-centric approach and commitment to company values of Innovation, Quality, Customers, Productivity, People and Community.\n\nThanks to strong partnerships and sound investments, our trusted Fleet and Mobile workforce platform provides real-time insights, visibility, employee safety and overall operational efficiency. This helps organizations, municipalities and businesses of all sizes to modernize their operations with ease. Fleet Complete is known for hiring, growing and empowering talented people who develop innovative products, build powerful relationships and provide personalized support that is unparalleled in our industry. Join Fleet Complete on our next chapter and we can work together to \"help fleets thrive\".\n\nProud to be named one of Greater Toronto\u2019s Top Employers for 2020: http://content.eluta.ca/top-employer-fleet-complete\n\nESSENTIAL DUTIES & RESPONSIBILITIES:\n\nCreate and maintain optimal data pipeline architecture for legacy and the new architecture of IoT streaming data (Telematics and other automotive sensors)\nAssemble large, complex data sets that meet functional / non-functional business requirements for data and application products\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources\nBuild analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics\nWork with the Product team and other stakeholders to assist with data-related technical issues and support their data infrastructure needs\nCreate data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader\n\nQUALIFICATIONS:\nAll applicants must possess the following:\n\n5+ years of experience in a Data Engineer role\nA degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field (Graduate degree would be a plus)\nExperience with cloud technologies. Specifically, AWS technologies such as S3, Glacier, Lambda, Athena, Redshift\nExperience with object-oriented & functional scripting languages including Python and Java\nExperience building and optimizing \u2018big data\u2019 data pipelines, architectures and data sets\nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement\nStrong analytic skills related to working with unstructured datasets\nBuild processes supporting data transformation, data structures, metadata, dependency and workload management\nA successful history of manipulating, processing and extracting value from large disconnected datasets\nStrong project management and organizational skills\nExperience supporting and working with cross-functional teams in a dynamic environment\nUnderstands and helps drive business impact via data systems and their resulting output\n\nFleet Complete will provide support in its recruitment processes to applicants with disabilities, including accommodation that takes into account an applicant's accessibility needs. If you require accommodation during the interview process, please contact the Recruitment Team, 866-649-7949.\n\nFleet Complete is an equal opportunity employer committed to diversity and inclusion. We are pleased to consider all qualified applicants for employment without regard to race, color, religion, sex, national origin, age, disability, protected veterans\u2019 status or any other legally protected factors.", "job_collect_date": "2020-08-16T16:18:51.000Z"}, "96": {"job_id": "e6c2ddd43685b4b3", "job_title": "Salentica Sr. Project Consultant", "job_employer": "SS&C Advent", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=e6c2ddd43685b4b3&fccid=609381a18aebd914&vjs=3", "job_description": "Salentica is a division of SS&C Technologies, Inc., a leading provider of software solutions and services for the international investment community. Investment managers, broker/dealers, sponsors, and custodians around the world use SS&C\u2019s mission-critical and decision support systems. SS&C brings together experts in investments, providing superior client service from its headquarters in Windsor, CT, and subsidiaries in Canada, Australia, and Europe.\nWe have an immediate opening for a Salentica Project Consultant in our Jacksonville, Fl Office\nResponsibilities\nLead and guide client through project implementation phases, including:\nbusiness process due diligence\nData analysis and mapping\nsolution design\napplication design\nconfiguration\ndocumentation of system requirements\nExecute data analysis of client source data databases and client files\nExecute business analysis on project implementation phases, including capture of detailed requirements for CRM\nWork closely with data engineer to present data analysis in support of data migrations\nExecute analysis and profiling on client data files, and where appropriate facilitate sessions with client and Salentica.\nWork closely with project manager and client to execute typical business analysis tasks\nProvide CRM configuration services to SS&C Salentica clients\nOther project duties as assigned\nDemonstrate superior communication skills\nAbility to work independently as a self starter and in a team environment\nAbility to work with internal teams to deliver client solutions\nWorking knowledge of financial services/asset management and RIA markets\nRecognizes and minimizes Salentica project risk exposure.\nEnsures project documents are complete, current, and stored appropriately\nPosition Qualifications\n\nIndustry Experience ( Wealth Management)\nBachelor\u2019s degree, preferably in business IT systems\nAbility to handle multiple projects/tasks simultaneously\nProven strong collaboration skills\nDemonstrated aptitude and ability for planning and execution\nAbility to recognize a client requirement, document, and outline steps for execution\nSuperior time management skills\nSuperior written, verbal communications skills\nAbility to manage stakeholder expectations and report/communicate these back to the SS&C/Salentica Delivery team\nAbility to take ownership of a task from initiation to completion\nWhat will set you apart?\n\nPMP designation\nMS CRM and/or Salesforce certifications\nMajor IT implementation experience and understanding how systems are affected by business process change\nExperience in dealing with complex business problems, identifying business/functional user requirements and recommending how to best support them through processes and applications\nAbility to have influence without authority, and a strong ability to sell recommendations and solutions by stating advantages and value in business terms\nRelated financial service/wealth management industry experience\n\n SS&C Offers:\nAn extensive health benefits program which includes Health, Dental, and Vision\n401k matching program\nGenerous Tuition Reimbursement and Training Allowance program\nBusiness Casual work environment and Work-Life Balance.", "job_collect_date": "2020-08-16T16:18:52.000Z"}, "97": {"job_id": "71cd34c1d32326c5", "job_title": "Senior Data Engineer", "job_employer": "Manulife", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=71cd34c1d32326c5&fccid=1747adf6142beb48&vjs=3", "job_description": "Are you looking for unlimited opportunities to develop and succeed? With work that challenges and makes a difference, within a flexible and supportive environment, we can help our customers achieve their dreams and aspirations.\nJob Description\nManulife\u2019s Global Data Office (GDO) is seeking a Sr. Data Engineer reporting into the Director, Advanced Analytics and AI Engineering & Enablement Lead. Located in Toronto, Canada - the role will champion and support strategic and global data initiatives that strengthens Manulife\u2019s global data and advanced analytic capabilities, foster cross-segment collaboration and communication helping build an agile data insight driven culture, and lead and nurture open data design and architecture establishing conditions for successful technical and analytic innovation. The Sr. Data Engineer will develop, maintain, and test: data pipelines, application framework, infrastructure for data generation and work closely with Data Scientists to enable their work using modern data architecture and tools.\nJob Description\nManulife has a clear vision for a Global Data Strategy. By liberating and strengthening Manulife\u2019s data capabilities we will enable deeper insights, better product and service design, and more effective business processes. The result will be exceptional experiences for our customers.\n\nKey Responsibilities:\nLeveraging new & existing Big Data & Cloud technologies contributing to the innovative design, development and management of data analytics labs supporting to increase knowledge and insight from enterprise data\nPerform technical systems and data flow development in a variety of projects for complex front, middle and back office applications, with a focus on the reporting and analytics environment; this environment is built on a Microsoft Azure cloud and is underpinned by a Hortonworks Hadoop stack\nPerform technical systems and data flow design for small-to-medium sized projects\nWork with multiple project execution and deployment teams (e.g. Development, Business Analysis, Architecture, Release Management, Production Support)\nWork closely with the technical leads and architecture teams, and align solutions that meet the departmental architectural vision\nAssess the completeness and accuracy of data, identifying gaps in data, provide feedback to business and system owners with guidance and options to obtain missing information\nDesign, build and implement modern data architectures in development and production environments (data orchestration pipelines, data sourcing, cleansing, augmentation and quality control processes)\nTranslates business needs into data engineering and architecture solutions\nContributes to overall solution, integration and enterprise architectures\nBuild and support deploying machine learning models in development and production environments\nProvide proactive data ingestion and analysis of large structured and unstructured datasets involving a wide range of systems across Group Functions (i.e., Finance, Treasury, Risk, Human Resources, Brand & Communications)\nEvaluating existing and proposed data models and how to best access and query them as well as existing and proposed data interfaces and how to clearly document them, including specification of data flow models, data flow timing, data mapping, and data transformation rules including data validations and controls\n\nEducation, Experience & Skills:\nDemonstrated 2-5 years of professional experience in related industry experience in working in big data/data management & understanding big data analytic tooling and environments including a University degree and or Master\u2019s degree in Engineering, Computer Science or equivalent quantitative program\nExperience in Big Data, Analytics and Business Intelligence technologies to support design, build and implementation for advanced analytics and business intelligence reporting;\nExperience working with Cloudera and/or Hortonworks Hadoop stack\nExperience with big data processing frameworks and techniques such as HDFS, MapReduce, Syncsort, Sqoop, Oozie, Storage formats (Avro, Parquet), Stream processing (NiFi, Kafka), etc.\nUnderstanding of relational and warehousing database technology working with Hadoop and other major databases platforms (e.g., Hadoop, Oracle, SQLServer, Teradata, MySQL, or Postgres)\nExperience in data technologies and use of data to support software development, advanced analytics and reporting. Focus on Cloud (Azure), Hadoop-based technologies and programming or scripting languages like Java, Scala, Linux, C++, PHP, Ruby Python, R and SAS.\nKnowledge regarding different databases such as Hawq/HDB, MongoDB, Cassandra or Hbase.\nWorking knowledge of modern data streaming using Kafka, Apache Spark and data ingestion frameworks: NiFi, Hive and Pig\nExperience writing complex SQL and NoSQL jobs to analyze data in both traditional DBMS (MS-SQL, Oracle) and Big Data environments (i.e., HADOOP, SPARK, or similar open source and commercial technologies)\nKnowledge of non-relational (Cassandra, MongoDB) databases preferred\nPredictive analytics and machine learning experience (scikit-learn, Tensorflow, MLlib, recommendation systems) preferred\nExperience with integrating to back-end/legacy environments\nExperience integrating business and technology teams\nKnowledge and familiarity with machine learning models application and production pipelines\nCollaborative attitude, willingness to work with team members; able to coach, participate in code reviews, share skills and methods\nRemains current with emerging technologies, innovations and practices within the data and analytics industry\nGood organizational and problem-solving abilities that enable you to manage through creative abrasion\nGood verbal and written communication; effectively articulates technical vision, possibilities, and outcomes\nStrong work ethic, results oriented, and accuracy / attention to detail are critical; ability to work in agile or scrum delivery environments\nExceptional oral, written and interpersonal communication skills with the ability to simplify complex technical concepts into business & value-focused language. A key requirement is to communicate clearly and consistently keeping stakeholders well-informed of progress and challenges\nExcellent organizational and time management skills, strong business presence with ability to multi-task and effectively deal with competing priorities. Ability to work with minimal or no supervision while performing duties; has the ability and initiative to organize various functions and be a strong team player.\nWhat about Perks?\nManulife has lots of perks including, but not limited to:\nCompetitive compensation\nRetirement Savings Accounts including a RPP (Pension Plan), RRSP (Retirement Savings Plan), and TFSA (Tax Free Savings account)\nManulife Share Ownership Program with employer matching\nCustomizable Benefits Package including Health, Dental, Vision, and 100% of Mental Health expenses\nFinancial support for ongoing training, learning, and education\nMonthly Innovation Days (Hackathons)\nWearing jeans to work every day\nAn abundance of career paths and opportunities to advance.\nThis is a full-time, permanent role located in Toronto, Ontario.\n\nIf you are ready to unleash your potential it\u2019s time to start your career with Manulife/John Hancock.\nAbout Manulife\nManulife Financial Corporation is a leading international financial services group that helps people make their decisions easier and lives better. With our global headquarters in Toronto, Canada, we operate as Manulife across our offices in Canada, Asia, and Europe, and primarily as John Hancock in the United States. We provide financial advice, insurance, and wealth and asset management solutions for individuals, groups and institutions. At the end of 2019, we had more than 35,000 employees, over 98,000 agents, and thousands of distribution partners, serving almost 30 million customers. As of March 31, 2020, we had $1.2 trillion (US$0.8 trillion) in assets under management and administration, and in the previous 12 months we made $30.4 billion in payments to our customers. Our principal operations are in Asia, Canada and the United States where we have served customers for more than 155 years. We trade as 'MFC' on the Toronto, New York, and the Philippine stock exchanges and under '945' in Hong Kong.\nManulife is an equal opportunity employer. We strive to attract, develop and retain a workforce that is as diverse as the customers we serve and to foster an inclusive work environment that embraces the strength of cultures and individuals. We are committed to fair recruitment, retention and advancement and we administer all of our practices and programs based on qualification and performance and without discrimination on any protected ground.\nIt is our priority to remove barriers to provide equal access to employment. A Human Resources representative will consult with applicants contacted to participate at any stage of the recruitment process who request any accommodation. Information received regarding the accommodation needs of applicants will be addressed confidentially.", "job_collect_date": "2020-08-16T16:18:53.000Z"}, "98": {"job_id": "e1337dfa4d5c3f3b", "job_title": "Data Engineer", "job_employer": "Pet Valu", "job_location": "Markham, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=e1337dfa4d5c3f3b&fccid=95581e84c2b81b02&vjs=3", "job_description": "Location: Markham, Ontario\nJob Description:\nJob Summary\nThis role requires a dynamic individual with experience and passion for using data and analytics to drive business results and help us build the foundation to use our rapidly expanding data lake to improve business decisions. As a member of the Marketing team, the Data Engineer will be a key contributing resource to continually measure, evaluate, and make recommendations on our website marketing efforts as well as ecommerce user-experience.\nResponsibilities\n\u2022 Design, implement, track performance and refresh advanced analytic automated segmentation and predictive models\n\u2022 Monitor Data quality with IT to ensure robust and accurate data\n\u2022 Partner with IT to define data solutions from new data sources, and build requirements for extraction into the data lake\n\u2022 Develop a strong acumen in source and downstream data storage system, and understand how the data is associated with business actions and potential solutions\n\u2022 Will function as the primary liaison between marketing and IT\nExperience and Education\n\u2022 5+ years designing data processes to automate organizational decisions ideally in a retail organization\n\u2022 Bachelor's Degree in Statistics, Business, Quantitative Economics, Mathematics, Marketing, Economics, Engineering, Operations Research or similar programs\nCompetencies and Skills\n\u2022 Strong Personal Drive for Excellence\n\u2022 Excellent organizational and time management skills, with the ability to manage multiple priorities in a high demand environment\n\u2022 Great sense of urgency and accountability, results-oriented with strong execution skills\n\u2022 An independent problem solver, must be able to find creative solutions to unusual or unprecedented questions\n\u2022 Quick learner \u2013 eagerness to learn about new tools and business systems to help craft solid solutions\nTechnology\n\u2022 Experience in relational and cloud data storage using advanced SQL, using technologies such as Snowflake, Oracle, SQL Server\n\u2022 Must be proficient in R, Python, Alteryx, Azure ML, or similar Machine Learning and data processing technology,\n\u2022 Solid understanding of BI tools such as Tableau, MicroStrategy or QlikView\n\u2022 Experience in Spark, Kafka, JAVA a strong plus\n\u2022 Proficiency in processing Web Analytic raw data, ideally from Google Analytics 360, a large plus\n\u2022 Snowflake, Orcale, SQL ServerSnowflake, Oracle, SQL Server\n\u2022 Must be proficient in either R or Python\n\u2022 Solid understanding in data visualization tools such as Tableau, MicroStrategy or QlikView\n\u2022 Experience in Spark, Kafka, JAVA a strong plus\n\u2022 Very comfortable with MS office (Word/Excel/Access/PowerPoint)\n\u2022 Understanding of using SQL against relational databases\n\u2022 Experience in data visualization and reporting tools, such as Tableau\n#INDC", "job_collect_date": "2020-08-16T16:18:54.000Z"}, "99": {"job_id": "1aca9ca80ec23fec", "job_title": "Expert BI/ETL Engineer (Tech Lead Cloud Data Engineer)", "job_employer": "Finastra", "job_location": "Mississauga, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=1aca9ca80ec23fec&fccid=1f9d0530a51ff611&vjs=3", "job_description": "What will you contribute?\nReporting to the Senior Manager, Development, the role of the Expert BI Developer is to ensure the effective design and delivery of the Student Lending reporting solution. This hands-on role serves as a Technical Lead for the Business Analytics and Reporting team providing development, technical guidance, review and support.\nResponsibilities & Deliverables:\n\nYour deliverables as an Expert BI Developer will include, but are not limited to, the following:\nDevelop and deliver a robust Reporting and Business Analytics framework, well aligned with the company\u2019s long-term strategic goals for data architecture vision.\nEnsure the solution supports Student Lending client data needs and can be easily extended to newly acquired clients and their standards.\nLiaise with vendors and service providers to select the products or services that best meet company cost and performance goals related to data architecture and analytics\nWorking closely with both enterprise level and project level team - data owners, stewards, users, business analysts, developers, quality analysts, department managers, architects and other stakeholders to understand reporting requirements and ensure strategic goals and tactical implementation are in alignment.\nTranslate project requirements into functional and non-functional specifications for BI reports and applications.\nLead conceptual and physical design, development and implementation of enterprise level BI and ETL framework, conforming to well defined business, technical rules and SLAs, preserving reusability of artefacts, single version of truth, centralization of logic, testability and well-designed error handling.\nPrepare all necessary documentation that clearly describes solution and Meta data.\nMonitor, tune up and administer BI Environments for quality and optimal performance purpose. Debug, monitor and troubleshoot BI solutions.\nBe aware of and comply with all corporate and department policies, procedures and standards that apply to your work area.\nEnsure that Reporting and Business Analytics strategies and architectures are in regulatory compliance .\nRequired Skills & Experience:\n8+ years' of hands-on experience developing BI and Reporting Solutions.\nExperience with business requirements analysis, entity relationship planning, data modeling, database design, reporting structures.\nDirect experience in implementing enterprise data management processes, procedures, and support on data monitoring.\nUnderstanding of large scale DB and reporting solution design, Source to Target Mappings, distributed DB design, multi environment structures, logical DB partitioning strategy, data archiving and retention, design and development of reporting semantic layer and view objects and logic\nExpert knowledge of MS SQL\nExpert hands-on experience with Azure Cloud Data Engineering suite: ADF, Databricks, Azure Data Lake, Spark, Azure SQL and Azure SQL Data Warehouse\nExperience with DAX, Tabular and Power BI.\nExperience with data processing flowcharting techniques.\nExperience developing and maintaining ETL tools and platforms such as SSIS, Azure ADF\nExperience with data architecting, large-scale data modeling, and business requirements gathering/analysis.\nStrong understanding of BI Reporting & ETL technologies, relational and dimensional data structures, Big Data hands-on experience, principles, and best practices.\nStrong familiarity with metadata management and associated processes.\nDemonstrated expertise with repository creation, and data and information system life cycle methodologies.\nUnderstanding of Web services (SOAP, XML, REST, JSON, UDDI).\nGood knowledge of applicable data privacy practices and laws.\n#LI-MG1\n*************************************************************************************************************\nThe above statements describe the general nature and level of work being performed by people assigned to this job. They are not intended to be an exhaustive list of all responsibilities, duties, and skills required. Reasonable accommodations may be made to enable qualified individuals with disabilities to perform the essential job functions. If you need assistance or an accommodation due to disability please contact your recruitment partner.\n*************************************************************************************************************", "job_collect_date": "2020-08-16T16:18:55.000Z"}, "100": {"job_id": "2fef127f6d4b3e92", "job_title": "Senior Pre-Sales Cloud Engineer", "job_employer": "SADA", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/rc/clk?jk=2fef127f6d4b3e92&fccid=b704562e07a2a03f&vjs=3", "job_description": "Join SADA as a Senior Pre-Sales Cloud Engineer!\n\nYour Mission\n\nAs a Senior Pre-Sales Cloud Engineer at SADA, you will work collaboratively with other architects and engineers to design, prototype and lead the deployment of scalable Google Cloud Platform (GCP) architectures. You will work with engineering teams, customers and sales teams to qualify potential engagements, craft robust architectural proposals, and deliver Statements of Work (SOWs) that engineering teams can successfully execute. You're also hands-on, able to conduct experiments and build functioning prototypes that prove out ideas and build confidence in the solutions you advocate.\n\nYou will be a recognized expert within SADA and will develop a reputation with customers as well as the Google Cloud sales and professional services organizations for the quality of your work. You will demonstrate repeated delivery of project architectures that other engineers and architects demur to you for lack of expertise. You will also lead early-stage opportunity technical qualification calls, as well as lead client-facing technical discussions.\n\nPathway to Success\n\n#BeAChangeAgent: You are a rainmaker! You are way out in front of our delivery organization, meeting with the spectrum of corporate and enterprise customers that need our consultative services. You have your finger on the pulse of their technical needs and take pride in helping them solve their real-world problems on GCP.\n\nYou will be measured quarterly by a combination of (a) the volume of signed SOWs that you shepherd through the sales funnel, and (b) the level of customer satisfaction measured at the end of each engagement.\n\nAs you continue to execute successfully, we will build a customized development plan together that leads you through the solution architecture or management growth tracks.\n\nExpectations\n\nRequired Travel - 30% travel to customer sites, conferences, and other related events. Due to the COVID-19 pandemic, travel has been temporarily restricted.\nCustomer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives\nTraining - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.\n\nJob Requirements\n\nRequired Credentials:\nGoogle Professional Cloud Architect Certified and/or Google Professional Data Engineer Certified, or able to complete one of the above within the first 45 days of employment.\nRequired Qualifications:\nMastery in at least one of the following domain areas:\nInfrastructure Modernization: migrating n-tiered workloads from on-prem and other clouds to GCP with near-zero-downtime (includes full spectrum of lift and shift to complete re-platforming scenarios), or building hybrid-cloud solutions based on Anthos and containerization technologies, such as Docker, Kubernetes, and Istio\nApplication Development: building custom web and mobile applications on top of the GCP stack\nData Engineering: data warehouse modernization (including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines and reporting/analytic tools), OLTP/OLAP data migrations, or backup, restore and disaster recovery solutions.\nExperience providing oversight and direction of cloud projects\nExperience leading technical design sessions, architecting and documenting technical solutions that are aligned with client business objectives, and identifying gaps between the client's current and desired end states\nExperience strategizing, designing, architecting and leading the deployment of scalable solutions on GCP\nExperience across multiple cloud platforms: GCP, AWS, Azure\nExperience with container engines: Kubernetes, Docker, AWS Elastic Container Service\nExperience with automation technologies including Terraform, Google Cloud Deployment Manager, AWS Cloud Formation or Microsoft Azure Automation\nExperience working with engineering and sales teams to elicit customer requirements\nAbility to communicate across business units and the ability to interface with and communicate complex technical concepts to a broad range of internal and external stakeholders\nTime management skills with the ability to manage multiple streams and lead less experienced architects\nExperience as a technical consultant or another customer-facing technical role\nUseful Qualifications:\nHands-on experience designing and recommending elegant solutions that drive business outcomes\nExperience building, designing and migrating complex cloud architectures\nStrong aptitude for learning new technologies and techniques with a willingness and capability to skill up the team\nAbility to lead an in-depth client meeting/workshop across a broad range of topics including discovery, cloud compliance, and security\nDeep understanding of best practices, design patterns, reference and compliance architectures with an uncanny ability to build and recommend these as needed\nKnowledge and understanding of industry trends, new technologies and the ability to apply these to customer architectures to drive outcomes\nHighly self-motivated and able to work independently as well as in a team environment\n\nAbout SADA\n\nValues: We built our core values on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA's values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.\n\nMake them rave\nBe data driven\nBe one step ahead\nBe a change agent\nDo the right thing\n\nWork with the best: SADA has been the largest partner in North America for GCP since 2016 and has been named the 2019 and 2018 Google Cloud Global Partner of the Year. SADA has also been awarded Best Place to Work by Inc. as well as LA Business Journal!\n\nBenefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match, professional development reimbursement program as well as Google Certified training programs.\n\nBusiness Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.", "job_collect_date": "2020-08-16T16:18:56.000Z"}, "101": {"job_id": "e8f01b72a171ac29", "job_title": "Data Engineer", "job_employer": "goeasy", "job_location": "Mississauga, ON", "job_link": "https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DbNJQGwjC-NEA80-qlVyEGmSv34Xx0eW4t4u9rXSv_7_tlvMU2e-Gw_wrZ47s1zPCnrySU2yuEL07wtSyx7XOwYh5VPcVL7CxRhOYwHzR-CdmyBrwbeqommOn6y9FD_uKIYpUCb9-PC_XgwGzBDrPy7SD7JQndrQfke5qYpMa2HqB-U6CfsiKU_ejlbt4Jw_9ewRhy5t2bPi49fYuJ7N_XnqBD-2Z3MPNXz6t8i9FQg1VeRRmtY4QzBSMYXIbAwv6fM8acKRGjFB3Sfgu3udGGvWiWca84e1NSsjCHCB0ECvxl2I_KiBbUFWeOa7W-mQt0KLwwt9_bzxZ2UemS1AHqWgbafNqR9ff6-2WwaIRTT2iVSP2JIMuWB2QXP2AOs7o84d8gN5yvOf-a-J6u0HS-NX5SZJ7QOB0lNaroAPg4HOZNMzCCHRA9wEhjHNXh3OqhxY6Dw5tzlHODAEh3Qr6MoSzBXrH51tky3KiM2KG3KA==&p=10&fvj=0&vjs=3", "job_description": "If you are looking to join one of Canada\u2019s fastest growing companies, goeasy Ltd. is the place for you! Recognized as one of Canada\u2019s Most Admired Corporate Cultures, one of Canada\u2019s Top 50 Fintech\u2019s and one of North America\u2019s Most Engaged Workplaces, we want the best and brightest to join our team.\n\nWe are a publicly traded company on the TSX with over 4000% shareholder return since 2001, goeasy operates two main business units. easyfinancial is our consumer lending business that offers secured and unsecured installment loans of up to $35,000 and easyhome is Canada\u2019s largest merchandise lease-to-own company. It is our mission to provide everyday Canadians the chance for a better tomorrow, today by giving them access to the credit they need and by offering them a second chance when they have been turned down by banks and traditional lenders. With a retail network of nearly 400 locations across Canada and over 1900 employees, we are able to build lasting relationships with our customers as we help them rebuild their credit and graduate towards prime rates and a brighter financial future.\n\nThe Data Engineer on the Data Science and Business Insights team will build, integrate data from various resources, manage data in goeasy operational data store and enterprise data warehouse. This position will develop ETL (Extract, Transform and Load) with various tools on large datasets to ensure data is easily accessible, works smoothly, as well as maintain and expand the data warehouse for reporting and analysis. The Data Engineer will also work closely with the data architect on the design and architecture of our enterprise data warehouse.\n\nResponsibilities:\n\nDevelop data set processes for data modeling, mining and production\nDevelop and maintain ETL processes using SSIS, Scripting and data replication technologies\nParticipate in development of datamarts for reports and data visualization solutions\nResearch opportunities for data acquisition and new uses for existing data\nIntegrate new data management technologies and software engineering tools into existing structures\nSupport the translation of business requirements for data acquisition/manipulation and provide detailed specifications that can be passed downstream for use\nDevelop detailed technical specifications and operational support documentation in collaboration with Business Systems Analysts, BI Engineers and Architects.\nIdentify and communicate technical problems, process and solutions\nCreate Ad-Hoc queries and reports as needed along with providing on-going analytical support for these requests\nAssist in the collection and documentation of user\u2019s requirements\nEnsure that existing business processes dependent on the ODS/EDW are monitored and respond quickly to bug fixes, enhancement requests and production ETL related issues.\nDealing with the database users on a daily basis to ensure that problems are dealt with promptly and that appropriate fixes are made to resolve any problems.\nRecommend ways to improve data reliability, efficiency and quality\nEnsure systems meet business requirements and industry practices\nWork effectively with the Business Intelligence and Data Solutions Architects, Data and BI Engineers to ensure that all approved development and deployment procedures are followed.\n\nQualifications:\n\nBachelor\u2019s Degree in Computer Science, MIS, Computer Engineering or other Information Technology related degree\n4+ years working with SQL Server or comparable relational database system\n3+ years of extensive ETL development experience with SSIS and/or ADF\n4+ years of experience troubleshooting within a Data Warehouse environment\nExpert domain knowledge & experience in Data warehousing, encompassing data model design, dimensional modeling, naming conventions, cross-cutting concerns, common integration technologies, patterns & standards and emerging technologies.\nExpert Knowledge of SQL skills to build, debug, and optimize (developing procedures, functions, SQL queries, etc.) and working with large data sets and to perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.\n2+ years SQL Server Database administration experience\nCloud experience (Azure) is highly preferred\nExposure and experience with Python, R, Hadoop, Azure, other Big data and advanced analytics\nKnowledge of AI and ML developments/solutions/implementations\nExperience with multiple programming languages (PowerShell scripting, C#, others) with basic scripting skills.\nHigh level of technical aptitude\n\nInclusion and Equal Opportunity Employment\n\ngoeasy is an equal opportunity employer. In addition, goeasy is committed to providing accommodations for applicants upon request at any stage of the recruitment process in accordance with all legislative requirements throughout Canada. Please let us know if you require an accommodation due to a disability during any aspect of the recruitment process and we will work with you to address your needs.\n\nAdditional Information:\n\nAll candidates considered for hire must successfully pass a criminal background check, credit check, and validation of their work experience to qualify for hire. We thank all interested applicants, however we will only be contacting those for interview who possess the skills and qualifications outlined above.\n\nWhy should you work for goeasy?\n\nTo learn more about our great company please click the links below:\n\nPAID1234", "job_collect_date": "2020-08-16T16:18:57.000Z"}, "102": {"job_id": "ed86ae879d5f5db2", "job_title": "Senior Big Data Engineer (f/m/x)", "job_employer": "Avira Operations GmbH & Co. KG", "job_location": "Canada", "job_link": "https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DVAdzec4bw6rQ4FO3PDdwz3gCZ43d7Gsr17jnu3o1g5CsoUOgyHaCzZOjoCGyji8ntYJxUdGd2FmwbktcVakqfF59toD5WP-Ro_DzTHrpq6YpYqUfP4QH5sxN4c835IUO3mqkddooJyzOy5iObgoXm6nmIVAYiD64A4oqzCBZE9OQd2SIMvc0wNYFZs_gEDJku4UjmLIAEZTB2qjvPJTwd_rElIrtA92j4qqvlYJNsFhAsVhFWnIJVDOGXN7pqF6gTI1tbpPq_r68wyTw8kD-2dx2iQJULGhRQ1Pcwb25o_h5Hz5cVpw0iar-nK6khegf5atszBqx5qYfC_aYeJYpgT4J1fh75QZrwkMm4JRnwLD4pS2HXGGaVceJ6wRxR5H2FwMhEMqWmgb0H8F0-rfEJx8oJfj2cPkaVdcb2fk8eZxW3bW59z4wUupNX7pHs29rxRXw5-D4oIA==&p=11&fvj=0&vjs=3", "job_description": "Senior Big Data Engineer (f/m/x)\n\nLocation: Tettnang\n\nThe Challenge\n\nAs a Data Engineer in the Threat Intelligence team, you will design, implement and maintain the Threat Intelligence Platform that transforms massive amounts of (real-time) data from various sources into descriptive knowledge about emerging threats. You challenge our status quo and define standards for data-intensive analytic pipelines. Your projects will collect and process security-related intelligence and will then provide that data to the company as a whole so Avira can protect its users on a global scale.\n\nThe team\nWe are an international team of engineers and researchers. We are a self-organized and result-oriented team, always looking to perfect the art of automation. Our systems and services are integrated within the Avira Protection Cloud that protect consumers and businesses around the world.\n\nWhat you will do\nStarting from day one you will familiarize yourself with our big data ecosystem and tool stack, as well as the main data source systems.\n\nWithin the first 3 months, you are fully integrated into the Threat Intelligence Platform development and maintenance. You are taking responsibilities for certain data stream integrations.\n\nWithin the first 6 months you are an expert for the Threat Intelligence Platform having a complete understanding of its integration into Avira\u2019s protection technologies and architecture landscape.\n\nWithin the first year you will be truly integrated within a team that delivers the most up-to-date Threat Intelligence solutions to millions of customers. You take full ownership of cross-department projects and collaborate efficiently with other teams. You are keen to improve existing technologies and implement new and innovative features. You also play a big part in securing and supporting the digital lifestyle of our customers and making the Internet a more secure place.\n\nYour Qualifications\n3+ years' experience in software development, with excellent development skills in PythonIndustrial experience with data-intensive projects in the Hadoop ecosystem, Spark, Kafka, and AirflowExperience in building data ingestion pipelinesGood knowledge in designing, building, using and maintaining REST APIsExperience with SQL/NoSQL databases, especially creating scalable, multi-node deploymentsExperience with AWS components and principles are a plusStrong analytical, technical, organizational and communication skills\nThe position is based in Tettnang, Germany, near lake Constance.\n\nBenefits and perks:\nNew Work\nStylish building with roof terracesCanteen and ChocaViraModern office concept\n\nLearning & Development\nUnlimited access to UdemyTrainings & Conferences\nSpecialist Career\n\nHealth & Wellbeing\nGym and fitness courses\u201cJobRad\u201d bike leasingMedical checkups\n\nFamily & Living\nRelocation PackageVacation child careAvira Prime licences\n\nEvents\nOnboarding eventsMonthly Employee MeetingsSummer & Christmas parties\n\nAN OPPORTUNITY TO MAKE A DIFFERENCE\nUpdate: Although there\u2019s a lot of disruption nowadays due to Covid-19, we at Avira are continuing to run our daily business activities so that we stay true to the promise to our customers - now even more than ever: Protecting people in the connected world.\n\nAnd we are doing this from the safeness of our own homes. Among other things, this means we are still hiring, but we have moved all our interviews online and all our colleagues are being onboarded remotely\n\nSo join us and you will be able to work from home until the danger is over.\n\nWe\u2019re an international software company at the forefront of imagining the future of digital security. Avira\u2019s award-winning products and technology protect over 500 million users in the connected world.\n\nWhat makes us special? First and foremost \u2013 it\u2019s the authentic people at Avira. We have a great community feeling that fosters your uniqueness and offers the space to reflect, the feedback to grow, and the freedom to innovate. If you are looking for a culture that also encourages aspiration and professional excellence, get in touch with us and discover the Avira experience firsthand.\n\nLena Komarek\nHR Recruiter\nAvira Operations GmbH & Co. KG\nHuman Resources\nKaplaneiweg 1\nD-88069 Tettnang, Germany\nPhone: +49 (0) 7542-500 -2207\nE-Mail: lena.komarek(at)avira.com", "job_collect_date": "2020-08-16T16:18:58.000Z"}, "103": {"job_id": "997673b8effa647f", "job_title": "Data Engineer (Intermediate 4-6 years) - Contract - Remote during COVID", "job_employer": "CorGTA", "job_location": "North York, ON", "job_link": "https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Dkh866153otbRJ8nVMuEXnXPd315JS377__3VyD6HnNQkmcaOHMemmy5pLrNV6j0WaVWEWWabnp4PP6-2u77DLfT_FCIGvCW8CQmQW140Z7EVU32Jnwl42FALsxnbc4n5M6FOTfoW2OoTMOcGG21HZGOoXL7fWT8eaT67VlcClh4sHQcJsHCX_MlEd1zbmR-3Dr4yFbEIVPfDFZE1XXYbwESJ7EsAKoTkDjOcMQ9pAMV0mg18dKhQnLjDviyjNw7FI2Z-3BDhHIkE3b3IQmBzRtu_fT3yheoIQ76r_gJxJ1WaJ9VImebqSU2UBkjLj8G4TYzRHTCHqZ4k0OGE9Is6yC4aUtMxKTZ3R7UgVDDabl1UE6LgIVmrEynKHKodIyC82B60lKONpGwvi7ZkD8MwRkrJqUNsd-d_GK12uL0auw9jDZ-XQ73mGKnH9o1hqJKWTaomsvn0wk3X9VM8lpajCCHWvJvZ_S-HsXrqPBevG_Rw4td7AuDu1&p=12&fvj=1&vjs=3", "job_description": "Role: Data Engineer (Intermediate 4-6 years)Structure: Contract (6 months initially)Location: North York, ONHours: Monday - Friday 40 hours (remote during COVID-19)Pay: $50.00 p/h inc.The role: We are looking for Intermediate Data Engineers who have roughly between 4-6 years experience who ideally has experience moving data from Google cloud over to Big query.However, if you are someone who has good experience with ETL and Data Pipeline Design & Data Validation experience and can script in Python and create strong SQL Queries then this will also be a potential match for this role.Ideal Qualifications to be successful in this role: - Degree in Computer Science, Engineering, Information Systems (or equivalent combination of skill and experience)- 4-6 years of experience working with data architecture projects- Excellent SQL coding and experience with a broad array of development tools and platforms including exposure to a big data environment tools/languages, such as SQL, R, SAS, Python, etc.- Experience creating relational database design and data models- Experience with various analytical data platforms and technologies- Experience with using REST API, Cloud (GCP Ideally)- Experience in custom or structured data integration design, implementation, and maintenance- Experience with business intelligence tools such as Qlikview, Tableau and Microstrategy (Nice to have for Data Visualizations)- Previous experience working in retail or eCommerce is highly preferred- Ability to curate data to tell stories and provide business insightsPlease apply with an updated resume and ensure the required skills you are able to speak to for this position are included.For more roles like this please go to www.corgta.com/find-a-job/Job Type: Full-timeSalary: $45.00-$50.00 per hour", "job_collect_date": "2020-08-16T16:18:59.000Z"}, "104": {"job_id": "54e0fcc38385e3a2", "job_title": "Data Engineer/Integration Consultant", "job_employer": "Copperstone Connect", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Ceb7pqR8iGmLuXIZQklF6pqed3xddONNM6Jjumj-ndyGK0kULQUmQ2REqfJ5KAzFVeK_es1WVg0s1sLgKW09UsvVRp0-dSizFQr1oNyK_VIqdjla00VU9GNVvu2x4v9p3hsL7NeF-6hBksbAaSgKazsaYTODGMKQQeVuYcxoOuxvFKuD0HRLD9ybgbcXTn1LNLZ_IgBpIskQ7xBJXX5JXVwyGIJlMTRLTToktJJ-Ss365L4Br4i7vlTKk43gKp4jWrnWv6Wd8OjlEjYE7kN1bJSr-gdHcPZXw1Rl5wtASB5OXU4rr0fndshCjnYdwX8wK65SevFJuEdG6q9cAYCs5J0hJUtEMJycmRgqT6bkVARH4N0cUzGVhzwGGJ3-8g7eqCX_732XFDu73BYiSMvZg8sk_04N7Yylyah9YpZ6qCFEx5yu__6Y8PYL5yJ8y0tBrBei2sV_QUyn6eZeyCbU8StZyB8lbCJCc5UUd5ZIGkyWXFZbjRVdeSy4j9X1ILowutiPfOS7yirTaSzbOH-2rQRA427gaevEp1ruvXyKJ3hfaJiK-k3rpGnA7gnsBzKxPfPKrq55Rcu5cxoKrtaED8_YpRUHZQo4qTULqBfzrw_rSllgHcdTCVCZjiAOXCiGT1m5Fc53Pdtq36D6cw3mXfJhBgHcXvN1Xy95G7COS3Mw==&p=13&fvj=0&vjs=3", "job_description": "Our client, a boutique-size consulting firm, prides itself on being at the forefront of innovation in the Big Data space. Founded in 2010, they provide thought leadership and implementation excellence within the ever-growing data and analytics world.\nThey take pride in having some of the most highly trained and experienced consultants in the industry which translates into optimal value for their clients. They were one of the first companies to provide analytics and data as a service, via the cloud, as early as 2010. They strive to make sure their customers are well-positioned with the best technologies/tools in the industry, constantly evaluating new and existing technology partnerships. Some of the more prominent companies they have partnered with include; Snowflake, DataRobot, MicroStrategy, Informatica, Amazon AWS & Microsoft. They continue to invest in their most valuable resource, their people. They do this through extensive training both on the job and through various educational programs.\nDue to growth, they are seeking to hire an Intermediate - Senior Integration Consultant.\n\nDesired Skills and Experience\n: University/College degree in Computer Science, Mathematics, Data Science and/or Relevant Degree\n: 5+ years hands-on development, configuration, scripting and administration experience with Data Integration platforms. (i.e. Informatica, Talend, DataStage, SSIS)\n: BI Experience (MicroStrategy, Looker, Tableau, PowerBI) considered a nice to have\n: Extensive theoretical and practical knowledge of data warehousing principles/concepts and practical development experience in all areas of the data warehousing life cycle\n: Experience with Data Management, ETL, Cloud Data (AWS), Data Integration\n: Knowledge of OLAP-related principles and concepts\n: Strong grasp of data modeling techniques and concepts (Normalized/Denormalized, Conceptual/Logical/Physical, Star, Snowflake, Data Vault)\n: Strong knowledge and experience with relational databases such as Snowflake, SQL Server, Oracle (Advanced knowledge of reading and writing SQL, Performance analysis and tuning)\n: Knowledge and experience with key Big Data technologies (Hive, Presto, Spark, Kafka, NoSQL databases, Semi-structured data access patterns (Json, Parquet, XML, etc.))\n: Strong Python scripting skills\n: Excellent communication skills\n: Great problem-solving skills\n: Leadership and good client management skills\n\nDay to Day Activities Would Include\n: Conduct relevant customer interviews to determine key business requirements and objectives\n: Build appropriate analytical data models based on outcomes of user interviews\n: Analyze and profile data systems to build source to target data mappings\n: Review ETL performance and conducts performance tuning as required on mappings/workflows or SQL\n: Administration and support of data integration infrastructure\n: 2nd level on-call support of ETL services as required\n\nYou will be responsible for attaining the following goals:\n: Attaining a minimum of 1 new accreditation/certification per year\n: Spending 80% or more of their time on billable work\n: Completing 90% or more of their agile delivery tasks on time\n: Demonstrating competency in 1 new relevant technology every year", "job_collect_date": "2020-08-16T16:19:00.000Z"}, "105": {"job_id": "a32b7c599156a218", "job_title": "BHJOB15656_15145 - Data Engineer w/ DevOps - Azure Cloud", "job_employer": "Myticas Consulting", "job_location": "Toronto, ON", "job_link": "https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Bt274jo0r_e0cCCqhskoQvohXG3bfDAo8rIdE-Msu3yuVfyF2oiDFKHFOU-vxVulAfe8kPL7riFwoXa1bsfN-szjxO_8qUQWbPsJOnTg0EkutIoK2S4NNLL24JQh9e4DsE9hv907k2_zDbvj0a8WAj5oPSV5J0YhhOguMmzdyDAmWbzZpk8LpfjYU62fNVxT0bNh9EapNUwKiRH_9-JJ9AEt10W8vUPflKb8dnausE4O-Kq6TMn-Ch9eTtGT0Z5QdqrE5qo5ah2cZKqsdQ_7sY9K2NrdPPRyda9ovkEUn6Vi2J6bbhRrUMRLM_rJz-3BJyCt0AmpY9GYOfs-82QvWVJA0iZjBWzog3e4mQM-09Dz3nXYHKP79itxweK-CvUl1Z2aF6oz65dfoW3SageL0GPQq6Wuy-8ugmloieCxtu0TiJOhsnV0Lfmqlxnl3d2FXy_GqIFKgXOQ==&p=14&fvj=0&vjs=3", "job_description": "The growing team here at Myticas is looking for an experienced Data Engineer who would be interested in a remote contract opportunity within the Ottawa, Toronto and Montreal regions.\n\nAccountabilities:\n\nDefining and reviewing security design requirements for cloud infrastructure and application components.\nEvaluating architecture patterns from security perspective.\nBuilding and implementing security controls to enable enforcement of compliance with Cloud Control Objectives, using custom Azure policies and integrated controls in DevOps processes\n\nRequirements:\n\nStrong Data Engineer w/ DevOps expertise + Azure Cloud Experience\nMust know how to code and stand up scripts.\nExperience with Data Digestions\nExperience writing scripts to automate (infrastructure)\nARM Templating Expertise\nAzure Synapse Expertise\nSupport developing automated DevOps processes and procedures for the following Azure components:\nAzure Synapse (Azure DW) & Studio (private preview)\nAzure Data Catalog Gen 2 (Babylon \u2013 private preview)\nAzure Data Lake Storage Gen 2\nAzure ML\nML Flow\nAzure SQL Analysis Service\nAzure Databricks\nADF data pipelines for data loading to AzSQL/Synapse\nADF data pipelines for connecting to on-prem data sources for data\n\nCandidates looking to apply for this role are to send us an updated version of their resume in confidence. Our team will be sure to review all applicants and follow up accordingly at the conclusion of the review process.\n\nJob is also known as: Data Engineer, DevOps Engineer, Azure Cloud Engineer, Azure Engineer, Cloud Engineer\n\nINDMY", "job_collect_date": "2020-08-16T16:19:01.000Z"}}