202008181	2020-08-18T11:02:51.000Z	Our client, a boutique-size consulting firm, prides itself on being at the forefront of innovation in the Big Data space. Founded in 2010, they provide thought leadership and implementation excellence within the ever-growing data and analytics world.
They take pride in having some of the most highly trained and experienced consultants in the industry which translates into optimal value for their clients. They were one of the first companies to provide analytics and data as a service, via the cloud, as early as 2010. They strive to make sure their customers are well-positioned with the best technologies/tools in the industry, constantly evaluating new and existing technology partnerships. Some of the more prominent companies they have partnered with include; Snowflake, DataRobot, MicroStrategy, Informatica, Amazon AWS & Microsoft. They continue to invest in their most valuable resource, their people. They do this through extensive training both on the job and through various educational programs.
Due to growth, they are seeking to hire an Intermediate - Senior Integration Consultant.

Desired Skills and Experience
: University/College degree in Computer Science, Mathematics, Data Science and/or Relevant Degree
: 5+ years hands-on development, configuration, scripting and administration experience with Data Integration platforms. (i.e. Informatica, Talend, DataStage, SSIS)
: BI Experience (MicroStrategy, Looker, Tableau, PowerBI) considered a nice to have
: Extensive theoretical and practical knowledge of data warehousing principles/concepts and practical development experience in all areas of the data warehousing life cycle
: Experience with Data Management, ETL, Cloud Data (AWS), Data Integration
: Knowledge of OLAP-related principles and concepts
: Strong grasp of data modeling techniques and concepts (Normalized/Denormalized, Conceptual/Logical/Physical, Star, Snowflake, Data Vault)
: Strong knowledge and experience with relational databases such as Snowflake, SQL Server, Oracle (Advanced knowledge of reading and writing SQL, Performance analysis and tuning)
: Knowledge and experience with key Big Data technologies (Hive, Presto, Spark, Kafka, NoSQL databases, Semi-structured data access patterns (Json, Parquet, XML, etc.))
: Strong Python scripting skills
: Excellent communication skills
: Great problem-solving skills
: Leadership and good client management skills

Day to Day Activities Would Include
: Conduct relevant customer interviews to determine key business requirements and objectives
: Build appropriate analytical data models based on outcomes of user interviews
: Analyze and profile data systems to build source to target data mappings
: Review ETL performance and conducts performance tuning as required on mappings/workflows or SQL
: Administration and support of data integration infrastructure
: 2nd level on-call support of ETL services as required

You will be responsible for attaining the following goals:
: Attaining a minimum of 1 new accreditation/certification per year
: Spending 80% or more of their time on billable work
: Completing 90% or more of their agile delivery tasks on time
: Demonstrating competency in 1 new relevant technology every year	Copperstone Connect	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Ceb7pqR8iGmLuXIZQklF6pqed3xddONNM6Jjumj-ndyGK0kULQUmQ2REqfJ5KAzFVeK_es1WVg0s1sLgKW09UsvVRp0-dSizFQr1oNyK_VIqdjla00VU9GNVvu2x4v9p3hsL7NeF-6hBksbAaSgKazsaYTODGMKQQeVuYcxoOuxvFKuD0HRLD9ZozEtlooKFMf54qKdhtkdAGlCoyV-QdHwQiSOwSfFBgkxUSJlR2YeG02SVn3b5VztyfwHCRhLWAgmOYANVLVMGm1AXgD6Lhy-H8LwloN3FMdI8ApsMq1QGebM0AR74rbdU8HjYuHrRKdxqpo0SRbMJ9W33V_6lEh-I8rHCZwXkjBRSlImwrtfg6WnbWhe-mFZd_goCGsrVROO_NCbe15D5skqRIEhmFSQ-rWPZvUWOX9vT_7qyv66Np8PYHETW8jAy6W9A9NX2fJGv5ZZVUev4oU1_M8PR5eVFDjpPGdSyOabATTubJu329iJO2A_S1ttLAsyvGcZui3bZbz1_oEeH0F-mxuuuA_zB3m2f-HER2yFjAs9rpPjBZ2mRUBkx4v4lmB2bmjjYFdA9FKjF2JLsJEd91xqufhdFZcSfhivH_OmlNUtn7EUWy5OAfPyOsORVW1uuTGBbrLQMaY0dGZsLYwLZCnLSNkDBd9LbwX_Q9ntCgbWyyINw==&p=0&fvj=0&vjs=3	Toronto, ON	Data Engineer/Integration Consultant	54e0fcc38385e3a2
202008182	Myticas Consulting	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Bt274jo0r_e0cCCqhskoQvohXG3bfDAo8rIdE-Msu3yuVfyF2oiDFKHFOU-vxVulAfe8kPL7riFwoXa1bsfN-szjxO_8qUQWbPsJOnTg0EkutIoK2S4NNLL24JQh9e4DsE9hv907k2_zDbvj0a8WAj5oPSV5J0Yhj5JoKRGGFYykSLxAP_-eO2-MgwBDmcLIF8JHZlfYaweBU8Blzj00IE82IekA1IsT4BroH3V73k21lyudvi1i-BewUeXyLLH7LXaPSkpdUmjfm0t92Y9AwGtIEp7xA4bWFnJXY0FlIUlJ2OXMeJK51OQdUoZ61ekpiGjFpTWTZ9ai-nC0WoJdrfsJZniCpwg1utHrqbwsC5_QX3vR0zxPMu8MWEYl3OZWy6M91mXzkKL9nb8T1h4AByg3ZlAVOKVwaE-3T2eHJSI3-9hSQ7ii9YQ5DFBUcj27WFjmG7lJfDig==&p=1&fvj=0&vjs=3	2020-08-18T11:02:51.000Z	BHJOB15656_15145 - Data Engineer w/ DevOps - Azure Cloud	Toronto, ON	Our recruiting team at Myticas Consulting is looking for an experienced Data Engineer who would be interested in a remote contract opportunity offered within the Ottawa, Toronto and Montreal regions.

Responsibilities:

Defining and reviewing security design requirements for cloud infrastructure and application components.
Evaluating architecture patterns from security perspective.
Building and implementing security controls to enable enforcement of compliance with Cloud Control Objectives, using custom Azure policies and integrated controls in DevOps processes

Qualifications:

Strong Data Engineer w/ DevOps expertise + Azure Cloud Experience
Must know how to code and stand up scripts.
Experience with Data Digestions
Experience writing scripts to automate (infrastructure)
ARM Templating Expertise
Azure Synapse Expertise
Support developing automated DevOps processes and procedures for the following Azure components:
Azure Synapse (Azure DW) & Studio (private preview)
Azure Data Catalog Gen 2 (Babylon – private preview)
Azure Data Lake Storage Gen 2
Azure ML
ML Flow
Azure SQL Analysis Service
Azure Databricks
ADF data pipelines for data loading to AzSQL/Synapse
ADF data pipelines for connecting to on-prem data sources for data

Candidates looking to apply for this role are to send us an updated version of their resume in confidence. Our team will be sure to review all applicants and follow up accordingly at the conclusion of the review process.

Job is also known as: Data Engineer, DevOps Engineer, Azure Cloud Engineer, Azure Engineer, Cloud Engineer

INDMY	a32b7c599156a218
202008183	If you are looking to join one of Canada’s fastest growing companies, goeasy Ltd. is the place for you! Recognized as one of Canada’s Most Admired Corporate Cultures, one of Canada’s Top 50 Fintech’s and one of North America’s Most Engaged Workplaces, we want the best and brightest to join our team.

We are a publicly traded company on the TSX with over 4000% shareholder return since 2001, goeasy operates two main business units. easyfinancial is our consumer lending business that offers secured and unsecured installment loans of up to $35,000 and easyhome is Canada’s largest merchandise lease-to-own company. It is our mission to provide everyday Canadians the chance for a better tomorrow, today by giving them access to the credit they need and by offering them a second chance when they have been turned down by banks and traditional lenders. With a retail network of nearly 400 locations across Canada and over 1900 employees, we are able to build lasting relationships with our customers as we help them rebuild their credit and graduate towards prime rates and a brighter financial future.

The Data Engineer on the Data Science and Business Insights team will build, integrate data from various resources, manage data in goeasy operational data store and enterprise data warehouse. This position will develop ETL (Extract, Transform and Load) with various tools on large datasets to ensure data is easily accessible, works smoothly, as well as maintain and expand the data warehouse for reporting and analysis. The Data Engineer will also work closely with the data architect on the design and architecture of our enterprise data warehouse.

Responsibilities:

Develop data set processes for data modeling, mining and production
Develop and maintain ETL processes using SSIS, Scripting and data replication technologies
Participate in development of datamarts for reports and data visualization solutions
Research opportunities for data acquisition and new uses for existing data
Integrate new data management technologies and software engineering tools into existing structures
Support the translation of business requirements for data acquisition/manipulation and provide detailed specifications that can be passed downstream for use
Develop detailed technical specifications and operational support documentation in collaboration with Business Systems Analysts, BI Engineers and Architects.
Identify and communicate technical problems, process and solutions
Create Ad-Hoc queries and reports as needed along with providing on-going analytical support for these requests
Assist in the collection and documentation of user’s requirements
Ensure that existing business processes dependent on the ODS/EDW are monitored and respond quickly to bug fixes, enhancement requests and production ETL related issues.
Dealing with the database users on a daily basis to ensure that problems are dealt with promptly and that appropriate fixes are made to resolve any problems.
Recommend ways to improve data reliability, efficiency and quality
Ensure systems meet business requirements and industry practices
Work effectively with the Business Intelligence and Data Solutions Architects, Data and BI Engineers to ensure that all approved development and deployment procedures are followed.

Qualifications:

Bachelor’s Degree in Computer Science, MIS, Computer Engineering or other Information Technology related degree
4+ years working with SQL Server or comparable relational database system
3+ years of extensive ETL development experience with SSIS and/or ADF
4+ years of experience troubleshooting within a Data Warehouse environment
Expert domain knowledge & experience in Data warehousing, encompassing data model design, dimensional modeling, naming conventions, cross-cutting concerns, common integration technologies, patterns & standards and emerging technologies.
Expert Knowledge of SQL skills to build, debug, and optimize (developing procedures, functions, SQL queries, etc.) and working with large data sets and to perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
2+ years SQL Server Database administration experience
Cloud experience (Azure) is highly preferred
Exposure and experience with Python, R, Hadoop, Azure, other Big data and advanced analytics
Knowledge of AI and ML developments/solutions/implementations
Experience with multiple programming languages (PowerShell scripting, C#, others) with basic scripting skills.
High level of technical aptitude

Inclusion and Equal Opportunity Employment

goeasy is an equal opportunity employer. In addition, goeasy is committed to providing accommodations for applicants upon request at any stage of the recruitment process in accordance with all legislative requirements throughout Canada. Please let us know if you require an accommodation due to a disability during any aspect of the recruitment process and we will work with you to address your needs.

Additional Information:

All candidates considered for hire must successfully pass a criminal background check, credit check, and validation of their work experience to qualify for hire. We thank all interested applicants, however we will only be contacting those for interview who possess the skills and qualifications outlined above.

Why should you work for goeasy?

To learn more about our great company please click the links below:

PAID1234	goeasy	2020-08-18T11:02:52.000Z	Mississauga, ON	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DbNJQGwjC-NEA80-qlVyEGmSv34Xx0eW4t4u9rXSv_7_tlvMU2e-Gw_wrZ47s1zPCnrySU2yuEL07wtSyx7XOwYh5VPcVL7CxRhOYwHzR-CdmyBrwbeqommOn6y9FD_uKIYpUCb9-PC_XgwGzBDrPy7SD7JQndrQc_s4FG972Nho8RPzAIO6HA6Yw55GxKG_W_mNDiQYuzy5N2e9gY_FaysS1AtPDzhIZf2ey_HAXRDgezdbWkDLp1XlY_w_Sl1yQ2JGHIJrRi0p3G_46--RpDVmCUSts3ax6LRo0UDCOKXB8WVj54QgE5qZjWkkHvZduQEeeF6Nuorft5CDXDo09gE2DzOvNLpCYnO5xo-z5U4qWAllYvFxznwdVgXzA7xciW-ywG7NpmgySkJkKfLzLmsPWPZvTkFRUYwAkwzBwI_68Kaa_AM0KFe6_HwSdLE7V_XGKb6vrW2h4AFHCPE_-0nTO7llMMX7EMTLyY95CA0Q==&p=2&fvj=0&vjs=3	e8f01b72a171ac29	Data Engineer
202008184	VMware	Data Engineer Toronto, Ontario	8e8d2a7026da25b3	https://ca.indeed.com/rc/clk?jk=8e8d2a7026da25b3&fccid=c762a27145bd166e&vjs=3	Toronto, ON	2020-08-18T11:02:53.000Z	Are you looking for a high energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform?
Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley?
Your next adventure at VMware is only a click away!
VMware's Data Engineering Team is looking for a Data Engineer to help build on Next generation Near Realtime Data Platform based on Hadoop, Spark & SAP HANA. You will be responsible for building and enhancing the solutions on the existing platform based on the business needs in partnering with fellow Developers and Business groups.
Responsibilities:Understand the business capability/requirements and transform them into robust design solutionsPerform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as neededPerform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms.Perform hands on work using SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform.Integrate data sets from difference sources using Informatica, Python, SAP SDI/SLTProtect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data.Help data consumers to correctly understand and use the data.
Qualifications:8+ years of experience in as a Data Engineer handling large volumes of data.Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools.Expertise in writing advanced SQL queries.Experience working with Informatica, SAP SDI/SLTExpertise in SAP HANA, Hive/Hadoop/Hawq/SparkWorking knowledge of BI Reporting tools like BOBJ and TableauExperience in Python ScriptingStrong analytical and troubleshooting skillsExcellent verbal and written communication skillsBachelor’s degree in Computer science, Statistics, Mathematics, Engineering or relevant field.
VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. VMware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.

Category : Engineering and Technology
Subcategory: Software Engineering
Experience: Manager and Professional
Full Time/ Part Time: Full Time
Posted Date: 2020-08-17
Cloud Management: VMware’s Cloud Management team delivers vRealize Suite, a solution that’s essential to accelerating our customers’ journey to digital transformation. It’s our market-leading, state-of-the-art cloud management platform designed to deliver and manage IT services across private, public, and hybrid clouds. VRealize Suite is an integrated, comprehensive solution that meets the challenge of managing a cloud infrastructure from a single pane of glass. We’re changing the way users design, build, view, and manage public and private clouds, and enabling them to run with optimal performance, insightful analytics and automated delivery. Join our user-focused team of software engineers, data scientists, web designers, product managers, and marketers. You’ll gain valuable experience in the fast-growing field of cloud infrastructure management from a pioneer and industry leader.

VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com.

Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.
202008185	Omnia AI: Data Engineer - New Grad 2021 - National	https://ca.indeed.com/rc/clk?jk=b43311c42dac4dc1&fccid=9e215d88a6b33622&vjs=3	Deloitte	Toronto, ON	2020-08-18T11:02:54.000Z	Job Type: New Graduate
Primary Location: Multiple Locations, Canada
All Available Locations: Calgary; Montreal; Ottawa; Toronto; VancouverBe encouraged to deepen your technical skills in data engineering on Cloud and Big Data platformsBe part of a firm that leads the way and pushes themselves to look like contemporary CanadaBe expected to share your ideas and make them a reality

If you love to wrestle down data puzzles; If you embrace the potential of always more and more data; If you commit to solve crunchy data problems no one else can; and if you envision to make impacts that matter with data, then Omnia AI is where you want to be.
What will your typical day look like?
A data engineer is passionate about data and technology solutions, is curious to learn about them and keep up with market evolution. You will play an active role throughout the entire engagement cycle – from architecture and design to build, test and deploy – specializing in technical data solutions, including data ingestion, data transformation and mapping, data curation, quality and governance, data preparation and consumption. You will work closely in cooperation with data scientists, report / dashboard builders and business analysts to serve up the right data, at the right time, in the right format to create business insights.

In this role, you will:

Design, engineer and test data pipelines and platforms that enable business analytics and insights
Translate business rules and requirements into data objects, produce associated data models and source to target mappings and write abstracted, reusable code components accordingly
Plan/schedule tasks, be part of small development teams, and learn from the trenches
Participate in technical meetings with client staff, and help advise clients with technical option analyses based on leading practices
About the team
As a member of our Data & Analytics Modernization team, you will help our clients design and implement the data platform architectures – be it in the cloud or on premise – required to enable cutting-edge AI solutions. You will be part of a practice to deliver a breadth of solutions to solve our clients most challenging business problems, with a focus on Cloud Platforms, Big Data, BI/DW, Data Integration, Data Governance, Master Data and Analytics applications. Each of these applications leverages a different mix of traditional and innovative technologies to achieve business outcomes and unprecedented insights.
Enough about us, let’s talk about you
You are someone who is:

Graduating from a BA/BSc or MA/MSc degree in Applied Mathematics, Statistics, Computer Science, Engineering, Business, or related field
Fascinated with data structures, data models and data patterns, interested in wrangling, joining and melting data sets together to create the foundation for analysis
Quick to learn new technologies, easy to adapt to new environments, curious to try out new things and not intimidated by large and complicated data sets
Exposed to and/or experienced with building data sets in the cloud (on AWS, Azure or Google platforms), on Big Data / Hadoop (using Sqoop, Kafka, Parquet, Scala, Python, etc.), or on database platforms (using SQL-like programming on SQL Server, Oracle, Hive, HBase, NoSQL databases, etc.)
Able to work in environments with significant ambiguity, can develop creative approaches to data problems, and is curious and enthusiastic about data in the context of business and industry implications
Why Deloitte?

Launch your career with The One Firm where you can make an impact that matters in a way that you never thought possible. With endless opportunities at every turn, and a culture built to support and develop our people to be the very best they can be, Deloitte is The One Firm for you to learn, grow, create, connect, and lead. We do this by making three commitments to you:
You will lead at every level: We grow the world’s best leaders so you can achieve the impact you seek, faster.You can work your way: We give you the means to be flexible in how you need and want to work, and we have innovative spaces, arrangements and the mindset to help you be wildly successful.You will feel included and inspired: We create a deep sense of belonging where you can bring your whole self to work.

The next step is yours

Sound like The One Firm. For You? Apply by September 13, 2020 at 11:59 pm EST.

To be considered, you must submit your cover letter, resume and unofficial transcript in ONE PDF document.

We are passionate about helping you launch your career at Deloitte. Prepped is a free, digital and personalized, voluntary program that may provide the guidance and training you need to properly tackle your job search, and could prepare you for the next steps in the interview process. Prepped is currently only available in English, and is not a Deloitte created tool. Choosing to use or not use Prepped will not be weighted towards or against you. Get Prepped now at https://www.fullyprepped.ca/invite.

At Deloitte we are all about doing business inclusively – that starts with having diverse colleagues of all abilities! We encourage you to connect with us at accessiblecareers@deloitte.ca if you require an accommodation in the recruitment process, or need this job posting in an alternative format. We’d love to hear from you!

By applying to this job you will be assessed against the Deloitte Global Talent Standards. We’ve designed these standards to provide our clients with a consistent and exceptional Deloitte experience globally.	b43311c42dac4dc1
202008186	NLB Services Inc	2020-08-18T11:02:55.000Z	904ae8a46b296ad2	Title: Data EngineerLocation: Toronto, CADuration: 6+ monthsExpertise in the design, creation, management, and business use of large datasets, across AWS Data and Analytics product.Excellent business and interpersonal skills to be able to work with business owners to understand data requirements, and to build the required data pipelines.Crafting, implementing, and operating stable, scalable, low cost solutions data pipelines to ingest real-time and event-based data using AWS technologies.5+ years of work experience with Data Pipelines, Data Modelling, and Data Architecture. ·Expert-level skills in writing and optimizing SQL.Proficiency in python scripting languages and integrating it with lambda-based processing in AWSExperience operating very large data warehouses or data lakes.Experience with building data pipelines and applications to stream and process datasets at low latencies. ·Demonstrate efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data. ·Sound knowledge of distributed systems and data architecture (lambda)- design and implement batch and stream data processing pipelines using KinesisGood understanding of different DB technologies like Dynamo DB (NoSQL),Aurora DB ( RDBMS), Redshift (In-Memory DBs), across AWS stack.Job Types: Full-time, ContractSalary: $70.00-$75.00 per hourExperience:AWS: 1 year (Preferred)Work remotely:Temporarily due to COVID-19	Toronto, ON	Data Engineer	https://ca.indeed.com/company/NLB-Services-Inc/jobs/Data-Engineer-904ae8a46b296ad2?fccid=0b209b5526418580&vjs=3
202008187	Requisition ID: 88577

Join the Global Community of Scotiabankers to help customers become better off.

JOB SUMMARY
The Bank’s Internal Audit Department plays a key role in the risk management process of the Bank. Our mandate is to provide independent and objective assurance over the design and operation of the Bank’s internal controls and to advise senior management on improvements to the Bank’s operations.

The role of the Data Engineer is to assist in the development of the data infrastructure and leverage existing analytical tools and methods to help Audit professionals to execute audits through enhanced data sampling, advanced analytics, continuous monitoring and visualization. As an active member of the team, they will grow in capability, coverage and knowledge to move the team forward.

The successful candidate will have a solid knowledge of R or Python, working knowledge of data analytics and statistical methods, as well as solid working knowledge of data acquisition and transformation (SQL, SAS). Experience with Mainframe commands and procedural programming, including Mainframe SAS, CLIST and ACF2, is strongly preferred. The ideal candidate will be able to communicate with Auditors to execute on projects independently and increase engagement.
KEY ACCOUNTABILITIES
Identify, acquire and use large volumes of data to provide incremental business value through data analytics methods and algorithms that help the Internal Audit group execute Audits with greater efficiency, accuracy and coverage
Assist in the development of the data infrastructure / architectureDesign and build a data pipeline that cleans, transforms and aggregates unstructured data in organized databases / data sourcesDevelop and write complex queries (that are suitable and scalable) for ‘big data’, build models, and ensure data is accessible to Data Scientists and Analysts, and the data sets are working smoothly.Develop data infrastructure necessary to support audits and continuous monitoring
Work with large datasets and distributed computing tools for analysis, data mining and modelingCollaborate with business lines and other stakeholders to enhance Internal Audit's use of data and analyticsPrepare detailed documentation to outline data sources, models and algorithms used and developed, as well as results obtainedEnhance and acquire skills as needed to move the team forward
FUNCTIONAL COMPETENCIES
Degree in Statistics, Computer Science or related fieldExperience with data ingestion, cleansing, transformation, and integrationExtensive experience with statistical, data processing and analytical tools like R, Python and SASExperience on mainframe with CLIST, skeletons, and panels.Working knowledge of ACF2Experience with SAS on the mainframe including FSEDIT.Strong analytical skillsSQL skills for querying relational databases (e.g., SQL Server, DB2, MySQL)Nice to have: implementing visualization tools like Microsoft Power BI or TableauExcellent written and oral communication skills to communicate with Auditors and present clear resultsAt least 5 years experience with similar tools / functions
Location(s): Canada : Ontario : Toronto
As Canada's International Bank, we are a diverse and global team. We speak more than 100 languages with backgrounds from more than 120 countries. Our employees are committed to a superior customer experience and use the Bank’s six guiding sales practice principles to ensure they act with honesty and integrity.

At Scotiabank, we value the unique skills and experiences each individual brings to the Bank, and are committed to creating and maintaining an inclusive and accessible environment for everyone. If you require accommodation (including, but not limited to, an accessible interview site, alternate format documents, ASL Interpreter, or Assistive Technology) during the recruitment and selection process, please let our Recruitment team know. If you require technical assistance, please click here. Candidates must apply directly online to be considered for this role. We thank all applicants for their interest in a career at Scotiabank; however, only those candidates who are selected for an interview will be contacted.	4cd09805ecea1dd2	Scotiabank	https://ca.indeed.com/rc/clk?jk=4cd09805ecea1dd2&fccid=3002307a9e5b4706&vjs=3	Toronto, ON	2020-08-18T11:02:56.000Z	Data Engineer
202008188	2020-08-18T11:02:57.000Z	Toronto, ON	AMZN CAN Fulfillment Svcs, ULC	https://ca.indeed.com/rc/clk?jk=59c48782b60cacd0&fccid=fe2d21eef233e94a&vjs=3	59c48782b60cacd0	Data Engineer	3+ years of experience as a Data Engineer or in a similar roleExperience with data modeling, data warehousing, and building ETL pipelinesExperience in SQL

Amazon.com was recently voted #2 most admired company in the US, #1 most innovative, and # 1 in Customer Service. We are investing heavily in building an excellent advertising business, and are responsible for defining, and delivering a collection of self-service performance advertising products – “always-on analytics” that is fully scalable and reliable. Our products are strategically important to our leadership, finance, economists, analysts, and BI partners to drive long-term growth. We mine billions of ad impressions and millions of clicks daily and are breaking fresh ground to create world-class products. We are highly motivated, collaborative, and fun loving with an entrepreneurial spirit and bias for action. With a broad mandate to experiment and innovate, we are growing at an unprecedented rate with a seemingly endless range of new opportunities.
The Advertising Analytics and Data Management team is looking for an exceptional Data Engineer who is passionate about data and the insights that large amounts of data can provide, who thinks/acts globally, and who has the ability to contribute to major novel innovations in the industry. The role will focus on working with a team of data engineers, business and tech savvy professionals to lay down scalable data architecture to ingest large amounts of structured and unstructured datasets and work with stakeholders to drive business decisions based on these datasets.

The ideal candidate will possess both a data engineering background and a strong business acumen that enables him/her to think strategically and add value to the customer experience. He/She will experience a wide range of problem solving situations, requiring extensive use of data collection and analysis techniques such as data mining and machine learning.

The successful candidate will work with multiple global site leaders, Business Analysts, Software Developers, Database Engineers, Product Management in addition to stakeholders in sales, finance, marketing and service teams to create a coherent customer view. They will:

Develop and improve the current data architecture using AWS Redshift, AWS S3, AWS Aurora (Postgres) and Hadoop/EMR.Improve upon the data ingestion models, ETL jobs, and alarming to maintain data integrity and data availability.Stay up-to-date with advances in data persistence and big data technologies and run pilots to design the data architecture to scale with the increased data sets of advertiser experience.Partner with analysts across teams such as product management, operations, sales, finance, marketing and engineering to build and verify hypothesis to improve the business performance.Manage weekly business reports via dashboards and paper the analyses of daily, weekly, and monthly reporting of performance via Key Performance Indicators.

AWS ExperienceAdvertising domain knowledge is a plus
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.

#sspajobs
202008189	It's fun to work in a company where people truly BELIEVE in what they're doing !
Job Description
About the Opportunity:
As part of the Data Hub team at AIR MILES, you will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flows and collection for cross functional teams. The pipeline needs to be scalable, repeatable, and secure. You will work with some of the largest and most varied data sets (both batch and real-time) in Canada. You will expand and develop the AIR MILES Cloud analytical platform that enables business users, data analysts and data scientists to make data driven decisions, build innovative data products and roll out advanced analytics.
What will you bring?
Ability and desire to work in our collaborative environment: open team room, pair programming and fluid interactions with all products and operations teams.
Focusing on building solutions utilizing an agile approach: close relationships with Product Managers, communicating and digesting real time feedback, and working smartly to build story cards on daily basis.
Passionate about Big Data and the latest trends and developments. We strongly believe in and encourage continuous learning.
You are self-driven, need minimal supervision and comfortable pushing your own projects and getting things done.
Experience with Python, Spark, and SQL
Experience building ‘big data’ pipelines, architectures, and datasets
Experience with Amazon AWS and other cloud platforms
Experience with Databricks
Experience with Agile methodologies as well as familiar with CI/CD tools (Jenkins, Travis, github)
Experience in ETL and Data Modeling preferred
Experience in designing and implementing streaming applications is preferred
Fully understand standard architecture methodologies, processes and best practices
About AIR MILES
Today, there are more ways than ever to engage shoppers. At AIR MILES, we believe that understanding the people behind the purchase is key to winning their hearts – and their wallets. For over two decades and from more than fifty locations around the globe, we have paired expertise in shopper behavior with advanced analytics to uncover the data-driven insights that drive successful loyalty, marketing and merchandising solutions. At AIR MILES, we know that in coming together we are at our strongest – and that together we can help shape the future for our clients, their shoppers and our communities. AIR MILES is an Alliance Data company. For more information, visit www.loyalty.com
About ADS

Alliance Data® (NYSE: ADS) is a leading global provider of data-driven marketing and loyalty solutions serving large, consumer-based industries. The Company creates and deploys customized solutions, enhancing the critical customer marketing experience; the result is measurably changing consumer behavior while driving business growth and profitability for some of today's most recognizable brands. Alliance Data helps its clients create and increase customer loyalty through solutions that engage millions of customers each day across multiple touch points using traditional, digital, mobile and emerging technologies. An S&P 500 and Fortune 500 company headquartered in Plano, Texas, Alliance Data consists of three businesses that together employ more than 16,000 associates at approximately 100 locations worldwide. For more information, visit www.alliancedata.com
Alliance Data is an Equal Employment Opportunity employer. Accordingly, we will make reasonable accommodations to respond to the needs of people with disabilities in accordance with legislation.
Alliance Data participates in E-Verify.
Information Systems
Job Type:
Regular	AIR MILES	3cc54e101ad26b17	https://ca.indeed.com/rc/clk?jk=3cc54e101ad26b17&fccid=cfd800e466ff9171&vjs=3	Data Engineer III	Toronto, ON	2020-08-18T11:02:58.000Z
2020081810	Fluid Hose & Coupling	https://ca.indeed.com/company/FLUID-HOSE-&-COUPLING-INC/jobs/Junior-Data-Engineer-7cc1b932ac3ee1bf?fccid=5156f9b5283a3233&vjs=3	7cc1b932ac3ee1bf	Mississauga, ON	Job summaryResponsible for the completion of long-term IT engineering projects. Performs engineering design evaluations and works to complete projects within budget and scheduling restraints. Develops, implements, and monitors information systems policies and controls to ensure data accuracy, security, and regulatory compliance.Responsibilities· Responsible for the analysis, improvement, cleaning, and manipulation of legacy relational databases at the company.· Responsible for the architectural design and development of a modernized database, integrated with legacy systems.· Responsible for the full-stack development of web applications which enhance customer and employee relationships with company data.· Responsible for general development and maintenance of the company's IT systems, including web server maintenance and website updates.· Works with the business’s leadership team, in order to aid in the implementation of database requirements and troubleshoot any existent issues.· Defines and builds the data pipelines that will enable faster, better, data-informed decision-making within the business.· Manages Salesforce database base as required by sales leadershipEducation and Skill Requirement· Computer Programming degree or equivalent. Equivalence may be demonstrated from past projects, results, etc· Computer Programming/Cloud Computing/Data Science/Data Engineer/Database Management/IT Security and Infrastructure· Ability to learn new technologies unguided/with minimal intervention (Adaptability)· Complete knowledge of Web programming, design, e-commerce, SQL and relational DB Design. Understands data structures and hierarchies (1-2+ years of experience with SQL, or any other database management system)· Clear understanding of System Architecture and ERP systems· Detail oriented, organized with strong analytical and critical thinking skills· Excellent communication skills combined with patience in the process· Excellent Excel and MS Office skills with a deep understanding of Excel macros and programming. May be required to demonstrate this in a remote interview by example· Quick learner and familiar with CRUD Operations· Resourcefulness to keep moving forward (does not stall at a challenge)· Preference will be given to those who understand App design and machine learningJob Types: Full-time, PermanentSalary: $40,000.00-$40,001.00 per yearBenefits:Dental CareExtended Health CareOn-site ParkingSchedule:8 Hour ShiftMonday to FridayEducation:Bachelor's Degree (Required)Work remotely:No	2020-08-18T11:02:58.000Z	Junior Data Engineer
2020081811	Data Engineer - Datamart Azure	Data EngineerRequirements: · 8+ years of hands-on development experience in multiple projects, with progressively increasing responsibility and ETL background, and an understanding of type 2 dimension, and prior experience with data marts.· 5+ years of hands-on experience working with data warehousing like applications and big data.· Experience leveraging big data technologies (One or more of Hadoop, Python, Spark) is required.· Exposure to Microsoft Azure (or other cloud) platforms is preferred· Experience working with various data exchange formats (JSON, CSV, XML etc.)· Solid understanding of relational and dimensional database design and knowledge of logical and physical data models is preferred· Experience with SDLC and/or Agile methodologies for project development, and participation in all phases of project development, is required· Excellent knowledge of SQL and Linux shell scripting· Experience in deploying and managing SQL and NoSQL databases is preferred· Experience with job scheduling (TIDAL, CAWLA, Oozie) and file transfer (e.g. SFTP)· Excellent diagnostic, analytical and problem-solving skills are preferred· Experience with continuous delivery tools (Jenkins, Bamboo, Circle CI), and an understanding of the principles and pragmatics for build pipelines, artefact repositories, zero-downtime deployment, etc. is preferred· Experience building real-time data pipelines using Kafka or spark streaming is preferredKey Accountabilities: · Perform technical systems and data flow development in a variety of projects for complex front, middle and back office applications, with a focus on the reporting and analytics environment; this environment is built on a Microsoft Azure cloud and is underpinned by a Hortonworks Hadoop stack· Perform technical systems and data flow design for small-to-medium sized projects· Work with multiple project execution and deployment teams (e.g. Development Architecture, Release Management, Production Support)· Work closely with source system SMEs to produce source to target mappings· Translate business requirements to technical specifications· Work closely with the technical leads and architecture teams, and align solutions that meet the departmental architectural vision· Able to handle multiple priorities seamlesslyJob Types: Full-time, ContractSalary: $89,471.00-$170,126.00 per yearExperience:total IT: 10 years (Required)dartmart: 2 years (Required)Data engineer: 10 years (Required)Azure: 3 years (Required)Big data: 3 years (Required)ETL: 5 years (Required)Work remotely:Temporarily due to COVID-19	https://ca.indeed.com/company/ARISOFT-INC./jobs/Data-Engineer-76d86fb24f1bdc31?fccid=1dbce7609f8e0b52&vjs=3	76d86fb24f1bdc31	2020-08-18T11:02:59.000Z	Toronto, ON	ARISOFT INC.
2020081812	https://ca.indeed.com/rc/clk?jk=6e7b18ed4b257445&fccid=e981b35e4c0452a3&vjs=3	6e7b18ed4b257445	ABOUT US
CI Investments Inc. is one of the country’s largest investment fund companies. CI is known for its innovation and ability to adapt quickly to the changing needs of Canadian investors. It provides employees with a fast-paced and challenging work environment with opportunities for advancement. CI is part of CI Financial, a diverse group of financial services firms.
POSITION: Data Engineer
LOCATION: Toronto (M5J 0A3)
STATUS: Contract (6 months with extension possibility)
JOB OVERVIEW
We are currently seeking a Data Engineer to join our Client Reporting and Data Management team. The successful candidate will work closely with our data science team on the development of our centralized predictive analytics function. In this role, you will assist with solving high-value business problems by extracting and manipulating large, complex datasets for use by data scientists. The role will be a six-month contract position, with an option to extend based on performance.
WHAT YOU WILL DO
Collaborate with business analysts, data scientists, software engineers, and solution architects to develop data pipelines to feed our data marketplace
Extract, analyze & interpret large, complex datasets for use in predictive modelling
Utilize AWS tools to develop automated, productionized data pipelines
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Develop and support ETL code for data warehouse and data marts to support the reporting and data analytic systems.
WHAT YOU WILL BRING
At least two years of work experience in quantitative analysis
Post-secondary degree in a quantitative discipline
Experience with large-scale, AWS big data storage such as S3 and EBS
Experience creating ETL jobs using AWS Glue or Talend
Experience with AWS Data pipeline tools like Cloudwatch and Stepfunctions
Experience working with data preparation tools like Talend
Experience in the Financial Services Industry is an asset
Strong knowledge with programming methodologies (version control, testing, QA) and agile development methodologies.
In-depth knowledge of AWS tools required to develop automated, productionized data pipelines
In depth knowledge of and experience with relational, SQL and NoSQL databases
Fluency with SQL and Python
Experience working with large, complex datasets
Excellent communication, writing and interpersonal skills
WHAT YOU CAN EXPECT FROM US
Our dedication to the Employee Experience at CI is aimed at supporting, empowering and inspiring our talented team through:
Recognition & Compensation
Training & Development
Health & Well-being
Communication & Feedback
If you are a passionate, committed and dynamic individual, please submit your resume in confidence by clicking “Apply”.
Only qualified candidates selected for an interview will be contacted.
CI Financial Corp. and all of our affiliates (“CI”) are committed to fair and accessible employment practices and we are committed to providing accommodations for persons with disabilities. If you require accommodations in order to apply for any job opportunities, or require this posting in an additional format, please contact us at accessible.recruitment@ci.com, or call 416-364-1145 ext. 4747. If you are contacted by CI regarding a job opportunity or testing and require accommodation in any stage of the recruitment process, please use the above contact information. We will work with all applicants to determine appropriate accommodation for individual accessibility needs.	2020-08-18T11:02:59.000Z	Toronto, ON	CI Investments Inc	Data Engineer
2020081813	There is never a typical day at Accenture, but that’s why we love it here! This is an extraordinary chance to begin a rewarding career at Accenture Technology. Immersed in a digitally compassionate and innovation-led environment, here is where you can help top clients shift to the New using leading-edge technologies on the most ground-breaking projects imaginable.

Interested in building end-to-end marketing solutions for clients? Bring your talent and join Data which operates in the Interactive, Mobility and Analytics space. You will have opportunities to get involved in digital marketing, eCommerce and end-to-end mobility capabilities to help clients to improve productivity and more!

WORK YOU’LL DO
Work across the Service Delivery Lifecycle to analyze, design, build, test, implement and/or maintain multiple system components or applications for Accenture or our clients
Responsible for the maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases
Support our database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects
Determines methods and procedures on new assignments with guidance
Manages small teams and/or work efforts (if in an individual contributor role) at a client or within Accenture

WHO WE´RE LOOKING FOR?
Minimum 5 years of experience as a Data Engineer
Must have experience with one of the Cloud Technologies (Azure or AWS)
Azure cloud includes Spark, Python, Databricks, Synapse, Snowflake, Data Factory and ADLS
AWS cloud includes Glue, EC2, EMR, Athena, redshift, Snowflake, S3, Spark, Python and Databricks
Experience with Big Data technologies like MapReduce, Pig, Hive, HBase, Sqoop, Flume, YARN, Kafka, Storm and etc.
2+ years of experience with at least one SQL language such as T-SQL or PL/SQL
2+ years of work experience with ETL and data modeling
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets
Experience in both batch and stream processing technologies
Experience with object-oriented/object function scripting languages: Java, C++, Scala
Machine learning experience with Spark or similar
Must be eligible for security clearance
Microsoft /AWS Certified: Azure Data Engineer Associate/ AWS Certified Data Analytics (Specialty)
Professional Skills Qualifications:
Proven success in contributing to a team-oriented environment.
Proven ability to work creatively in a problem-solving environment.
Desire to work in an information systems environment.
Demonstrated teamwork and collaboration in professional setting; either military or civilian.
WHAT´S IN IT FOR YOU?
Competitive benefits, including a fair and balanced parental leave policy.
Fantastic opportunities to develop your career across industries with local and global clients.
Performance achievement and career mentorship: our performance management process focuses on your strengths, progress and career possibilities.
Opportunities to get involved in corporate citizenship initiatives, from volunteering to charity work.
To learn more about Accenture, and how you will be challenged and inspired from Day 1, please visit our website at accenture.ca/careers.
It is currently our objective to assign our people to work near where they live. However, given the nature of our business and our need to serve our clients, our employees must be available to travel when needed.

Accenture does not discriminate on the basis of race, religion, color, sex, age, non-disqualifying physical or mental disability, national origin, sexual orientation, gender identity or expression, or any other basis covered by local law. Accenture is committed to providing employment opportunities to current or former members of the armed forces.

We are committed to employment equity. We encourage all people, including women, visible minorities, persons with disabilities and persons of aboriginal descent to apply.

Accenture is a leading global professional services company, providing a broad range of services and solutions in strategy, consulting, digital, technology and operations. Combining unmatched experience and specialized skills across more than 40 industries and all business functions — underpinned by the world’s largest delivery network — Accenture works at the intersection of business and technology to help clients improve their performance and create sustainable value for their stakeholders. With 469,000 people serving clients in more than 120 countries, Accenture drives innovation to improve the way the world works and lives. Visit us at www.accenture.com.	2020-08-18T11:03:01.000Z	Toronto, ON	https://ca.indeed.com/rc/clk?jk=fbcc383eeed7b6dc&fccid=a4e4e2eaf26690c9&vjs=3	fbcc383eeed7b6dc	Data Engineer	Accenture
2020081814	North York, ON	Role: Data Engineer (Intermediate 4-6 years)Structure: Contract (6 months initially)Location: North York, ONHours: Monday - Friday 40 hours (remote during COVID-19)Pay: $50.00 p/h inc.The role: We are looking for Intermediate Data Engineers who have roughly between 4-6 years experience who ideally has experience moving data from Google cloud over to Big query.However, if you are someone who has good experience with ETL and Data Pipeline Design & Data Validation experience and can script in Python and create strong SQL Queries then this will also be a potential match for this role.Ideal Qualifications to be successful in this role: - Degree in Computer Science, Engineering, Information Systems (or equivalent combination of skill and experience)- 4-6 years of experience working with data architecture projects- Excellent SQL coding and experience with a broad array of development tools and platforms including exposure to a big data environment tools/languages, such as SQL, R, SAS, Python, etc.- Experience creating relational database design and data models- Experience with various analytical data platforms and technologies- Experience with using REST API, Cloud (GCP Ideally)- Experience in custom or structured data integration design, implementation, and maintenance- Experience with business intelligence tools such as Qlikview, Tableau and Microstrategy (Nice to have for Data Visualizations)- Previous experience working in retail or eCommerce is highly preferred- Ability to curate data to tell stories and provide business insightsPlease apply with an updated resume and ensure the required skills you are able to speak to for this position are included.For more roles like this please go to www.corgta.com/find-a-job/Job Type: Full-timeSalary: $45.00-$50.00 per hour	Data Engineer (Intermediate 4-6 years) - Contract - Remote during COVID	997673b8effa647f	2020-08-18T11:03:02.000Z	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Dkh866153otbRJ8nVMuEXnXPd315JS377__3VyD6HnNQkmcaOHMemmy5pLrNV6j0WaVWEWWabnp4PP6-2u77DLfT_FCIGvCW8CQmQW140Z7EVU32Jnwl42FALsxnbc4n5M6FOTfoW2OoTMOcGG21HZGOoXL7fWT8eaT67VlcClh6Pj9F1SyGch-giihttMt1bQ2e0T2O5oIbTzvTBaFCXWW4M0xyTSXkSxztT9C4zb4rK7CFFPanJhlykmbGCKAsL7VC8jMnvKhwoRukd7h1xSCuxVrhkIVPFjXXmsVcD_2JnveEzvXjIvp-3PZVxc1HqWDIsrlQdpVLw5UqKYPoqFT962MriP-vubPdxqdeCqMYA_vHmoNjMUvqQbrFMHcr2Vv44hz-kaE4b-YSQq7l0GlpMDHFS0L2gomSM87fzzfFazut-Mf7ppEoaQkTHQx381Gfiqy0JttycyphtDBQDe3m-L9T_W5q2wX1RlQaeHdA==&p=13&fvj=1&vjs=3	CorGTA
2020081815	North York, ON	Role: Data Engineer (Intermediate 4-6 years)Structure: Contract (6 months initially)Location: North York, ONHours: Monday - Friday 40 hours (remote during COVID-19)Pay: $50.00 p/h inc.The role: We are looking for Intermediate Data Engineers who have roughly between 4-6 years experience who ideally has experience moving data from Google cloud over to Big query.However, if you are someone who has good experience with ETL and Data Pipeline Design & Data Validation experience and can script in Python and create strong SQL Queries then this will also be a potential match for this role.Ideal Qualifications to be successful in this role: - Degree in Computer Science, Engineering, Information Systems (or equivalent combination of skill and experience)- 4-6 years of experience working with data architecture projects- Excellent SQL coding and experience with a broad array of development tools and platforms including exposure to a big data environment tools/languages, such as SQL, R, SAS, Python, etc.- Experience creating relational database design and data models- Experience with various analytical data platforms and technologies- Experience with using REST API, Cloud (GCP Ideally)- Experience in custom or structured data integration design, implementation, and maintenance- Experience with business intelligence tools such as Qlikview, Tableau and Microstrategy (Nice to have for Data Visualizations)- Previous experience working in retail or eCommerce is highly preferred- Ability to curate data to tell stories and provide business insightsPlease apply with an updated resume and ensure the required skills you are able to speak to for this position are included.For more roles like this please go to www.corgta.com/find-a-job/Job Type: Full-timeSalary: $45.00-$50.00 per hour	Data Engineer (Intermediate 4-6 years) - Contract - Remote during COVID	2020-08-18T11:03:03.000Z	997673b8effa647f	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Dkh866153otbRJ8nVMuEXnXPd315JS377__3VyD6HnNQkmcaOHMemmy5pLrNV6j0WaVWEWWabnp4PP6-2u77DLfT_FCIGvCW8CQmQW140Z7EVU32Jnwl42FALsxnbc4n5M6FOTfoW2OoTMOcGG21HZGOoXL7fWT8eaT67VlcClh6Pj9F1SyGchmH7sEbxRwbXgD16yNFjfM1ZNG8gEr9WsnCj_LJgBWAxhltI11wHFNF9JXv2rLkWatbyPeYAv_GROjbxMDLOgDnLtFYLniU47aP3QZj_tAvMyEV8a6CRHmRiZxVl-oOi3pHMtf0WzXItsUZIuslIKKXRHNA_W4af5JcFF_dn3qz5pCT8vkZm754uCmlFvheF072Zz74IRQfO10d_IZKPKOsTtDAerJyOOvWP8zxntDLl04ThjNACTI5CvNRox__Mf444lNzfGuO_IMUXyCnpcfLDkkzwfDaCYKhfj7g9O6mxz0BlyfLdIgQ==&p=0&fvj=1&vjs=3	CorGTA
2020081816	Myticas Consulting	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Bt274jo0r_e0cCCqhskoQvohXG3bfDAo8rIdE-Msu3yuVfyF2oiDFKHFOU-vxVulAfe8kPL7riFwoXa1bsfN-szjxO_8qUQWbPsJOnTg0EkutIoK2S4NNLL24JQh9e4DsE9hv907k2_zDbvj0a8WAj5oPSV5J0Yhj5JoKRGGFYyqBeP0lnBxOGvMXVIawaY3xhqgEOU0JEqmtEF0aDRhzN8ss4oL6uqZml5zPcMRq-IgoVd25nAYhuQyE3ec3CLrl9UmZ_sCPXldc5AZL6cVcq5lNFXJdJPtNDzMEAIRIxgBYdEKCDVnLUp5VFcc6P9B8KKDlw2otwETKW8L7QAaGqw8elENexmFMZR0kXG2KS74skr1j5pDqzVnJGyee5NCv-3qgmAosvh1Qp7W1P5OFoaZSRqazcfDnUDLcWalXj4HKE6v04dCVQSQxCEiP7Hl1UKP10eLUL6Q==&p=1&fvj=0&vjs=3	BHJOB15656_15145 - Data Engineer w/ DevOps - Azure Cloud	2020-08-18T11:03:04.000Z	Toronto, ON	Our recruiting team at Myticas Consulting is looking for an experienced Data Engineer who would be interested in a remote contract opportunity offered within the Ottawa, Toronto and Montreal regions.

Responsibilities:

Defining and reviewing security design requirements for cloud infrastructure and application components.
Evaluating architecture patterns from security perspective.
Building and implementing security controls to enable enforcement of compliance with Cloud Control Objectives, using custom Azure policies and integrated controls in DevOps processes

Qualifications:

Strong Data Engineer w/ DevOps expertise + Azure Cloud Experience
Must know how to code and stand up scripts.
Experience with Data Digestions
Experience writing scripts to automate (infrastructure)
ARM Templating Expertise
Azure Synapse Expertise
Support developing automated DevOps processes and procedures for the following Azure components:
Azure Synapse (Azure DW) & Studio (private preview)
Azure Data Catalog Gen 2 (Babylon – private preview)
Azure Data Lake Storage Gen 2
Azure ML
ML Flow
Azure SQL Analysis Service
Azure Databricks
ADF data pipelines for data loading to AzSQL/Synapse
ADF data pipelines for connecting to on-prem data sources for data

Candidates looking to apply for this role are to send us an updated version of their resume in confidence. Our team will be sure to review all applicants and follow up accordingly at the conclusion of the review process.

Job is also known as: Data Engineer, DevOps Engineer, Azure Cloud Engineer, Azure Engineer, Cloud Engineer

INDMY	a32b7c599156a218
2020081817	There is never a typical day at Accenture, but that’s why we love it here! This is an extraordinary chance to begin a rewarding career at Accenture Technology. Immersed in a digitally compassionate and innovation-led environment, here is where you can help top clients shift to the New using leading-edge technologies on the most ground-breaking projects imaginable.

Interested in building end-to-end marketing solutions for clients? Bring your talent and join Data which operates in the Interactive, Mobility and Analytics space. You will have opportunities to get involved in digital marketing, eCommerce and end-to-end mobility capabilities to help clients to improve productivity and more!

WORK YOU’LL DO
Work across the Service Delivery Lifecycle to analyze, design, build, test, implement and/or maintain multiple system components or applications for Accenture or our clients
Responsible for the maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases
Support our database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects
Determines methods and procedures on new assignments with guidance
Manages small teams and/or work efforts (if in an individual contributor role) at a client or within Accenture

WHO WE´RE LOOKING FOR?
Minimum 5 years of experience as a Data Engineer
Must have experience with one of the Cloud Technologies (Azure or AWS)
Azure cloud includes Spark, Python, Databricks, Synapse, Snowflake, Data Factory and ADLS
AWS cloud includes Glue, EC2, EMR, Athena, redshift, Snowflake, S3, Spark, Python and Databricks
Experience with Big Data technologies like MapReduce, Pig, Hive, HBase, Sqoop, Flume, YARN, Kafka, Storm and etc.
2+ years of experience with at least one SQL language such as T-SQL or PL/SQL
2+ years of work experience with ETL and data modeling
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets
Experience in both batch and stream processing technologies
Experience with object-oriented/object function scripting languages: Java, C++, Scala
Machine learning experience with Spark or similar
Must be eligible for security clearance
Microsoft /AWS Certified: Azure Data Engineer Associate/ AWS Certified Data Analytics (Specialty)
Professional Skills Qualifications:
Proven success in contributing to a team-oriented environment.
Proven ability to work creatively in a problem-solving environment.
Desire to work in an information systems environment.
Demonstrated teamwork and collaboration in professional setting; either military or civilian.
WHAT´S IN IT FOR YOU?
Competitive benefits, including a fair and balanced parental leave policy.
Fantastic opportunities to develop your career across industries with local and global clients.
Performance achievement and career mentorship: our performance management process focuses on your strengths, progress and career possibilities.
Opportunities to get involved in corporate citizenship initiatives, from volunteering to charity work.
To learn more about Accenture, and how you will be challenged and inspired from Day 1, please visit our website at accenture.ca/careers.
It is currently our objective to assign our people to work near where they live. However, given the nature of our business and our need to serve our clients, our employees must be available to travel when needed.

Accenture does not discriminate on the basis of race, religion, color, sex, age, non-disqualifying physical or mental disability, national origin, sexual orientation, gender identity or expression, or any other basis covered by local law. Accenture is committed to providing employment opportunities to current or former members of the armed forces.

We are committed to employment equity. We encourage all people, including women, visible minorities, persons with disabilities and persons of aboriginal descent to apply.

Accenture is a leading global professional services company, providing a broad range of services and solutions in strategy, consulting, digital, technology and operations. Combining unmatched experience and specialized skills across more than 40 industries and all business functions — underpinned by the world’s largest delivery network — Accenture works at the intersection of business and technology to help clients improve their performance and create sustainable value for their stakeholders. With 469,000 people serving clients in more than 120 countries, Accenture drives innovation to improve the way the world works and lives. Visit us at www.accenture.com.	2020-08-18T11:03:05.000Z	Toronto, ON	https://ca.indeed.com/rc/clk?jk=fbcc383eeed7b6dc&fccid=a4e4e2eaf26690c9&vjs=3	fbcc383eeed7b6dc	Data Engineer	Accenture
2020081818	2020-08-18T11:03:06.000Z	Data Engineer, GEMINI (Contract)	https://ca.indeed.com/rc/clk?jk=25f6a9d8e47d869e&fccid=1a9a6d236cf89e9e&vjs=3	25f6a9d8e47d869e	POSITION OVERVIEW


The Vector Institute is seeking a Data Engineer, GEMINI to join our team in Toronto. This role will predominantly work with the General Medicine Inpatient Initiative (GEMINI) team at St. Michael’s Hospital, in addition to providing support to the Vector community.


The Data Engineer, GEMINI will primarily work in R to lead a team to automate and optimize GEMINI’s data pipeline workflow including extracting, transforming and loading data, conducting quality and validation checks, and standardizing data from multiple data sources. The ideal candidate will have excellent programming skills, a strong understanding of data pipelines and analytic methods, an aptitude for data visualization, and strong leadership and communication skills. You will be joining a dynamic and mission-driven team of clinicians, scientists, and quality improvement experts.
EMPLOYMENT TYPE


Contract (18 months)


ABOUT THE VECTOR INSTITUTE


A thriving, independent not-for-profit, the Vector Institute strives to advance the Artificial Intelligence ecosystem in Ontario, developing and attracting the world’s best machine learning and deep learning experts, and creating an unrivalled convergence of research, investment, entrepreneurialism, and economic growth. Located in the MaRS Discovery District in downtown Toronto, we are part of a dynamic and vibrant community of research, academia, health science and commerce.


ABOUT GEMINI


Co-led by Drs. Fahad Razak and Amol Verma, GEMINI has developed methods to extract and standardize data from electronic health records to harness the tremendous potential of data generated through routine patient care of General Internal Medicine hospital inpatients for research and quality improvement purposes. GEMINI is a unique data platform in the Canadian healthcare landscape and currently exists at 7 hospitals, with data collected on 345,000+ patient visits, including billions of data points. The GEMINI data platform has recently been funded to expand to the 30 largest hospitals in Ontario to support data analytics that inform the COVID-19 pandemic response. GEMINI supports a network of nearly 100 collaborating scientists and more than 40 students, including clinicians, computer scientists, biostatisticians, epidemiologists, social scientists, and engineers.
RESPONSIBILITIES


Develop scripts to extract, transform and load data from multiple data sources into GEMINI’s platform;

Gather requirements and conduct analyses to design, develop and maintain secure data pipelines;
Automate and optimize GEMINI’s data pipeline workflows;
Provide leadership and guidance to data pipeline team;
Maintain and update on-going quality assurance of data workflows;
Engages with team and collaborators to understand needs and requirements;
Documents data pipeline architecture;
Routinely tests and monitors system for failures, errors, breaches;
Troubleshoot problems;
Provides teaching and training as needed;
Create and update Standard Operating Procedures and other documentation files, as needed;
Establish methods to improve and automate data workflow of rapidly changing COVID-19-related data;
Support GEMINI’s data platform as required; and,
Perform other functions as required (e.g., providing scientific advice and support to other members Vector’s teams and corporate projects led by Vector).


SUCCESS MEASURES


GEMINI’s data pipeline workflow is automated and optimized through the development of algorithms and procedures.
High quality coding practices are maintained.
Significant contributions are made towards GEMINI’s data platform.


PROFILE OF IDEAL CANDIDATE


A degree in Engineering, Computer Science and/or related discipline; graduate degree preferred
At least 5 years’ relevant professional experience
Fully knowledgeable in designing and testing of data pipelines required
Extensive experience in SQL as well as at least one of either R or Python required
Familiarity of key databases such as: PostgreSQL, MySQL, Oracle, etc. is preferred
Knowledge of Linux commands and Shell scripts required
Strong analytical, technical design and problem-solving skills required
Strong working knowledge of Microsoft Office products (e.g. Outlook, Word, Excel, PowerPoint) required
Experience with GitHub preferred
Good judgement and understanding of what issues to escalate, resolve on your own, making suggestions for possible resolution required
Ability to learn new technology expediently required
Demonstrated success working within interdisciplinary teams
Experience with the Ontario healthcare system is an asset
Excellent attention to detail and proven ability to learn new skills
Experience working independently and as part of a team
Excellent organizational skills to manage multiple tasks in a timely manner
Demonstrated flexibility and have the ability to adapt and manage changing priorities


Please address applications (cover letter and resume) to Kailyn Burke, HR Generalist, using the link provided. Review of applications will begin August 19, 2020. We thank all applicants for their interest in this exciting opportunity and will be in touch with those whose qualifications most closely match with our needs. Please note that candidates may be required to demonstrate proficiency in R.

The Vector Institute is committed to employment equity and diversity in the workplace and welcomes applications from women, racialized persons/visible minorities, Indigenous peoples, persons with disabilities, and LGBTQ+ persons. All qualified candidates are encouraged to apply.


Further, we are committed to fostering an environment of inclusivity and accessibility. If you require an accommodation at any point throughout the recruitment and selection process, please

contact hr@vectorinstitute.ai and we will happily work with you to meet your needs.	Toronto, ON	Vector Institute
2020081819	https://ca.indeed.com/rc/clk?jk=9b3e8aaf625b3563&fccid=09abad886b83c501&vjs=3	2020-08-18T11:03:06.000Z	9b3e8aaf625b3563	Square	Company Description

Square builds common business tools in unconventional ways so more people can start, run, and grow their businesses. When Square started, it was difficult and expensive (or just plain impossible) for some businesses to take credit cards. Square made credit card payments possible for all by turning a mobile phone into a credit card reader. Since then Square has been building an entire business toolkit of both hardware and software products including Square Capital, Square Terminal, Square Payroll, and more. We’re working to find new and better ways to help businesses succeed on their own terms—and we’re looking for people like you to help shape tomorrow at Square.

Job Description

Square began with a simple yet revolutionary piece of hardware—the Square Reader. We have expanded that vision to give our merchants access to the latest secure payment technologies (contactless and chip cards) and easy-to-use Point of Sale products. Today our hardware carries over $30 billion in transactions annually. Measuring the performance of our hardware, and the journey in getting the hardware to our merchants, is essential in ensuring that our merchants’ experience with Square is as seamless as possible.
We need your help to collect data about our hardware, organize that data, and make it available to analysts across Square. As a member of the Hardware data engineering team, you will define, develop, and manage a variety of data infrastructure components and pipelines so that our analytics teams and collaborators have trusted data to inform decisions and insights.
You will:
Join the small but mighty Hardware data engineering team that partners with internal Hardware data consumers to understand their needs and to source the right data sets to work on
Work in a remote environment that allows you to build scalable Hardware data pipelines and tools to ingest data from internal/external sources to our cloud data stack (Snowflake / AWS)
Develop data structures to support flexible analysis of this data, including creating data models, structuring optimized ETLs, designing validation scripts
Troubleshoot technical issues with platforms, data discrepancies, alerts, etc.
Be a voice between the Hardware team and Square’s data community. Help the hardware team’s data producers embrace best-practices, represent the hardware team’s voice in data infrastructure planning discussions.
Report into a data engineering manager on the Platform Infrastructure Engineering team

Qualifications

You have:
3+ years building and supporting reporting data systems built on columnar oriented RDBMS systems (e.g. Snowflake, BigQuery, Redshift, Vertica, etc.)
Strong experience building data pipelines from heterogeneous data sources (e.g. Event streams, Flat Files, RDBMS, REST APIs, SFTP, etc.) to support real-time operational and analytical workloads
Technical accomplishments working with SQL, ETL, and Apache Airflow; and knowledge of at least one mature programming language (Ruby (and Rails), Python, Java, Go, or similar)
Experience with Linux/OSX command line, version control software (git), and general software development
Experience with cloud based data tools and services (e.g. AWS Lambda/S3/Transfer, GCP Cloud Functions/GCS, Fivetran, etc.)
Additional Information

At Square, our purpose is to empower – within and outside of our walls. In order to build the best tools for the businesses and customers we support all over the world, we have to start at home with a workforce as diverse and empowered as our sellers. To this end, we take great care to evaluate all employees and job applicants equally, based on merit, competence, and qualifications. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, disability status, or any other characteristic protected by law. We encourage candidates from all backgrounds to apply. Applicants in need of special assistance or accommodation during the interview process or in accessing our website may contact us by sending an email to assistance(at)squareup.com. We will treat your request as confidentially as possible. In your email, please include your name and preferred method of contact, and we will respond as soon as possible.

Perks

At Square, we want you to be well and thrive. Our global benefits package includes:

Healthcare coverage
Retirement Plans
Employee Stock Purchase Program
Wellness perks
Paid parental leave
Flexible time off
Learning and Development resources	Toronto, ON	Data Engineer - Hardware
2020081820	91168fc331947e74	2020-08-18T11:03:07.000Z	https://ca.indeed.com/company/Smarttechlink-technologies/jobs/Aws-Data-Engineer-91168fc331947e74?fccid=7be406286ca6842e&vjs=3	Job Description: AWS Data engineer with 3-5 years work experience using Python, PySpark, AWS EMR and Airflow. The desired candidate should have strong development skills and experience on datalake implementation including data extraction and building data pipeline.Job Type: Full-timePay: $94,832.00-$120,000.00 per yearSchedule:Monday to FridayExperience:AWS Data: 3 years (Preferred)	Toronto, ON	SmartTechlink Sollutions Inc.	AWS Data Engineer
2020081821	Must have:
Strong Spark, Kafka and are familiar with Flink/Druid/Ignite/Presto/Athena

You are proficient in Java/Scala/Python/Spark	https://ca.indeed.com/rc/clk?jk=5929006533b711f6&fccid=580f6b9a5ada66c2&vjs=3	2020-08-18T11:03:08.000Z	Toronto, ON	5929006533b711f6	Data Engineer- Permanent, downtown Toronto	IT Connex
2020081822	AstraNorth	2020-08-18T11:03:09.000Z	Senior Data Engineer	6752644600b05d88	Toronto, ON	https://ca.indeed.com/company/AstraNorth/jobs/Senior-Data-Engineer-6752644600b05d88?fccid=8116154355837827&vjs=3	Key Accountabilities: Perform technical systems and data flow development in a variety of projects for complex front, middle and back office applications, with a focus on the reporting and analytics environment; this environment is built on a Microsoft Azure cloud and is underpinned by a Hortonworks Hadoop stackPerform technical systems and data flow design for small-to-medium sized projectsWork with multiple project execution and deployment teams (e.g. Development Architecture, Release Management, Production Support)Work closely with source system SMEs to produce source to target mappingsTranslate business requirements to technical specificationsWork closely with the technical leads and architecture teams, and align solutions that meet the departmental architectural visionAble to handle multiple priorities seamlesslyRequirements: 8+ years of hands-on development experience in multiple projects, with progressively increasing responsibility5+ years of hands-on experience working with data warehousing like applications and big data.Experience leveraging big data technologies (One or more of Hadoop, Python, Spark) is required.Exposure to Microsoft Azure (or other cloud) platforms is preferredExperience working with various data exchange formats (JSON, CSV, XML etc.)Solid understanding of relational and dimensional database design and knowledge of logical and physical data models is preferredExperience with SDLC and/or Agile methodologies for project development, and participation in all phases of project development, is requiredExcellent knowledge of SQL and Linux shell scriptingExperience in deploying and managing SQL and NoSQL databases is preferredExperience with job scheduling (TIDAL, CAWLA, Oozie) and file transfer (e.g. SFTP)Excellent diagnostic, analytical and problem-solving skills are preferredExperience with continuous delivery tools (Jenkins, Bamboo, Circle CI), and an understanding of the principles and pragmatics for build pipelines, artefact repositories, zero-downtime deployment, etc. is preferredExperience building real-time data pipelines using Kafka or spark streaming is preferredJob Types: Contract, PermanentSchedule:Monday to FridayWork remotely:Temporarily due to COVID-19
2020081823	2020-08-18T11:03:09.000Z	67bbfacf7e5a2ca0	The Company

Smart Nora is a wellness tech company based in Toronto that has improved sleep and relationships for tens of thousands of couples around the world. Our debut product is the world's most comfortable snoring solution and has been listed on Oprah’s Favorite Things, as well as Good Morning America, TIME, TODAY and BBC to name a few.

The Team

We are an ambitious, tight-knit team with an open work environment and a self-directed approach. We typically work out of our office at King+Spadina or remotely in a weekly cadence of Monday kick-offs, Wednesday tea times, and Friday wrap-ups — taking a lot of pride in our individual work and holding one another accountable for producing great work.

Note: Currently we are all working from home throughout the COVID-19 pandemic.

The Product

Smart Nora is an over-the-counter, contact-free snoring solution relevant to the 40% of adults who snore. Smart Nora is loved by customers for its comfortable contact-free design that enables users to sleep without any attachments to their face or body.

The Role

We are looking for a Software / Data Engineer. This is a co-op placement, full-time from September - December ideally for a 4th-5th year student. We are flexible with an earlier starting date or longer term.

Location

Remote

What you would do

Support the current FW project conducted with our third party project partners by implementing structured testing and documentation
Build supporting tools in Python, Node, or other scripting languages to validate firmware and hardware
Actively participate in Mobile App development project with our third party project partners
Support acoustic performance optimization including microphone and codec gain settings, assisted by automated tests
Be part of the Product team developing the next generation Smart Nora device
Wrangle data and decipher meaning from quantitative tests and analytics
Write clear documentation

Our requirements

Comprehensive understanding of Software Development processes (SDLC)
Comprehensive understanding of Object Oriented Programming (OOP) principles
Experience with Pandas, R, Tableau, or other data processing/visualization tools
Experience with GitHub (send us your profile!)
Exposure to cloud (AWS) and APIs is a plus
Experience in audio processing is a plus
Experience in machine learning is a plus
Majoring in Software Engineering, Computer Science, Industrial & Systems Engineering, or related field.

How to Apply

Please apply using our online application form to include your resume and links to your LinkedIn, GitHub (if have), and Kaggle (if have) profiles.

Accommodations are available on request for candidates throughout the application process. Please let us know your needs so that we may accommodate. Email careers@smartnora.com if you would like to discuss this role before applying.	Smart Nora	https://ca.indeed.com/rc/clk?jk=67bbfacf7e5a2ca0&fccid=e4a22b3d28be02c0&vjs=3	Toronto, ON	Data/Software Engineer Co-op
2020081824	Data Engineer (Cloud)	StackPros Inc is seeking a candidate for a full-time role within our Data Systems Team in Toronto, Ontario.The *Cloud Data Engineer* will play a key role at StackPros, required to help create and maintain industry-leading quality and efficiency of service and software delivery.StackPros will rely on the Data Engineer to support the Data Systems team, in both data engineering and data science-related workflows. The Data Engineer will be expected to meet and exceed StackPros' quality standards, while helping the organization rapidly expand complex Machine Learning and related applications.*Key Responsibilities*:*Data Engineering-Specific Responsibilities*Participate in continuous delivery pipeline to fully automate deployment of the highly available cloud platform that supports multiple projectsDesign and develop ETL workflows and datasets to be used in data visualization toolsWrite complex SQL queries with multiple joins to automate and manipulate data extractsPerform end to end Data Validation to maintain accuracy of data setsBuild tools for deployment, monitoring and operationsTroubleshoot and resolve issues in the development, test and production environmentsDevelop re-useable processes that can be leveraged and standardized for multiple instancesPrepare technical specifications and documentation for projectsStay up-to-date on relevant technologies, plug into user groups, understand trends and opportunities to ensure we are using the best possible techniques and toolsUnderstand, implement, and automate security controls, governance processes, and compliance validationDesign, manage, and maintain tools to automate operational processes*Data Science-Specific Responsibilities*Perform exploratory data analysis to identify patterns from historical data, generate and test hypotheses, and provide product owners with actionable insightsDesign experiments for product initiatives and perform statistical analysis of the results with recommendations for next steps and future experimentsCreate and design dashboards by using different data visualization tools to present reports and insights, and support business decision makingHelp the StackPros Data Systems team adopt and evolve Predictive Modeling, Machine Learning and Deep Learning processes to deliver to clients in the future*Company-Wide Responsibilities: *Maintain and exceed client satisfaction with StackPros Inc.’s deliverables, day-to-day work and overall value as a partnerCultivate opportunities for company growth, always seek areas where StackPros Inc.’s role could be expandedAdapt to ever-changing client needs and expectationsMaintain dedication toward achieving excellence in StackPros Inc.’s delivery against client needs, and overall success as an organizationBe an enthusiastic, positive and generally awesome team mate, mentor & constantly curious learner*Qualifications: *4+ years experience in Data EngineeringUnderstanding of digital ecosystems including online data collection, cloud systems and analytics tools (Google Stack, Facebook, AWS, Salesforce, Adobe Suite etc.) a strong assetStrong technical understanding of a range of marketing concepts such as cookie-based data collection, setting and leveraging audience segments, attribution modelling, AB/N & multivariateExcellent written & verbal communication skills are essential; candidate should be comfortable presenting and participating in group discussions of concepts with internal and external stakeholdersCandidate must exhibit an analytical, detail-oriented approach to problem solvingExperience with Jira / Atlassian project management tools is an asset*WHAT’S IN IT FOR YOU?*100% employer-paid benefits packageMonthly yoga and meditation classes onsiteRegular Lunch and Learns from your Team MatesStanding desksEntertainment and Games area, including pool tableFully-loaded kitchen: snacks/fruit/drinks/beerFun Employee Events and ActivitiesParticipation in Community EngagementJob Type: Full-time	fa77394a330e96c4	https://ca.indeed.com/company/StackPros-Inc./jobs/Data-Engineer-fa77394a330e96c4?fccid=efee8d7cab418992&vjs=3	2020-08-18T11:03:10.000Z	Toronto, ON	StackPros Inc.
2020081825	Our client located in downtown Toronto is looking for a Big Data Engineer who will be responsible for developing new system stacks and tools for big data ingestion, processing, and analytics. This position is perfect for a developer whose passion is to apply cutting-edge technologies to solve complex business and engineering problems. This individual will work on a team of talented engineers responsible for the full life-cycle of production systems, software, tools, and flows.

Responsibilities

Design, develop, and maintain the software and systems that make up the data platform that runs our entire business
Participate in multi-disciplinary projects
Partner with the Data Science and Engineering teams who use our platform to by diagnosing, predicting and correcting scaling problems
Contribute to our teams growing set of development platforms, tools, and processes

Required Skills

Hands-on experience with Big data technologies (HBASE, HDFS, SPARK, and/or HADOOP)
Demonstrated proficiency with Spark, Scala, Python
Experience in building stream processing systems using spark streaming or Storm.
Experience in integration of data from multiple sources
Experience with NoSQL cluster databases, such as HBase, Cassandra, Druid
Experience with various messaging systems such as KAFKA, or RabbitMQ
Proficient understanding of distributed computing principles.

Preferred Skills

Experience with other highly scalable, low latency big data systems is a plus
Experience with Hortonworks / Cloudera distributions is a plus
Experience with Cloud technology and containerization (docker / kubernetes) is a plus

Who you are

Able to learn and apply new technologies quickly
Proven excellent problem-solving abilities
Able to work both independently and as part of a team
Able to multi-task in a dynamic environment
Have Excellent verbal and communication skills

Qualifications

Software development experience using mainstream languages such as Java, Scala and/or C++
Experience architecting, deploying and operating mission-critical big data clusters.	https://ca.indeed.com/rc/clk?jk=d5e196c24ff9d6f9&fccid=d5eebd59dbfe9145&vjs=3	2020-08-18T11:03:10.000Z	Toronto, ON	Data Engineer	Technology Development Corp	d5e196c24ff9d6f9
2020081826	FreshBooks	2020-08-18T11:03:11.000Z	FreshBooks has an ambitious vision. We launched in 2003 but we're just getting started and there's a lot left to do. We're a high-performing team working towards a common goal: building an elite online accounting application to help small businesses better handle their finances. Known for extraordinary product and customer service experiences and based in Toronto, Canada, FreshBooks serves paying customers in over 120 countries.

The Opportunity - Staff Data Engineer (Remote)

We're looking for a strong technical leader who has gravitas to influence and has deep knowledge of building and designing scalable data platforms. An effective communicator, a mentor who can think on their feet and be able to come up with practical, simple solutions to complex problems. As a Staff Data Engineer, you are someone who can redefine data engineering capability and constantly push boundaries. You will co-own FreshBooks' data platform to ensure scalability and elasticity.

If this appeals to you, please come and chat with us to learn more about how you can become a part of the intrinsically motivated teams of data engineers!

What you'll do in your first twelve months at FreshBooks:

Collaborate with analytics, engineering and business teams working on the product for our customers.
Be the technical lead and advocate best practices, contribution to roadmaps, developing the domain breadth and/or depth, stewardship.
Contribute towards defining technical vision and challenge across different levels of business including Senior Executives and Senior Leadership group.
Help in growing the technical expertise of the team as they continue to work and touch services outside their realm.
Demonstrate experience coaching and mentoring data engineering teams, and growing the overall technical maturity of our Data organization.
Evolve our data platform architecture to scale with our rapidly growing customer community.
Develop a deep understanding of multiple parts of our stack as well as the processes and technologies relevant to our tech space.
Be capable of supporting the data platform from end to end.
Raise the bar for our entire data engineering team through best practices, automation, documentation, and hiring.
Level up our operational excellence, and drive our team to maintain it, so that common regressions are root caused.
Support operational excellence and make measurable improvements to our support processes.

What you bring:

At least 10 years of combined experience in software and data engineering (Agile or Lean environment)
Strong programming skills in Python or similar language with deep refactoring skills.
Five or more years with cloud based data platforms and technologies such as AWS or Google Cloud Technologies, Elastic MapReduce, S3, EC2, and Kinesis.
Experience in architecting and building large-scale batch or stream processing data pipelines.
Experience architecting scalable ETLs with inputs from multiple data sources.
Experience with data warehousing technologies such as Amazon RedShift or Google BigQuery.
Experience in Airflow or other data infrastructure job scheduling software.
Experience writing complex SQL queries.
Ability to troubleshoot and determine root causes of issues.
Experience working with large codebases, writing robust and testable code
Experience ensuring security and governance of data.
A passion for keeping up to date in current technologies and future trends
A deep understanding of test-driven (and behavioural test driven) development, and of building substantially complete test code, and not just for the happy path
Familiarity with continuous integration (or better, continuous delivery) and automated build pipelines.
The ability to balance desire to ship code quickly to our customers with the responsibility of making good technical decisions.
A long-standing habit of continuous learning, and of applying new technologies, architectures, and methodologies to improve the code and Engineering organization.

What you might bring:

We're looking for a variety of talented technical leadership and know that a mix of skills and experience is useful. Even having a couple of the skills from the list below would be a strong asset.

Expertise in the core areas of business of FreshBooks (accounting, payments, small business solutions)
A background in DevOps and service ownership, and a clear understanding of bounded contexts and how they map onto microservices.
Strong pair programming both as a mechanism for producing better code, and for teaching skills.
Experience with Docker, Kubernetes, Ansible, Terraform, or other similar tools.
Experience with Redis / Elasticsearch & RabbitMQ.

Why Join Us

We're a motivated bunch, with our eye's laser-focused on shipping extraordinary experiences to businesses. You will be surrounded by hardworking team members who share a common vision for what an amazing software company could be and have the opportunity to help build an elite one, right here in downtown Toronto.

Apply Now

Have we got your attention? Submit your application today and a member of our recruitment team will be in touch with you shortly!

FreshBooks is an equal opportunity employer. We do not discriminate based on gender, religion, race, mental disability, sexual orientation, age, or any other status. All applicants are considered based on their qualifications and merits. At FreshBooks, we inspire an environment of mutual respect and we believe diversity and inclusion are crucial to our success.

Here at FreshBooks, we welcome and encourage applications from people with disabilities. Should you require any accommodations during the recruitment process, please advise your recruiter on how we can meet your needs to ensure a fair and equitable selection process in a confidential manner.	https://ca.indeed.com/rc/clk?jk=980a5785f8688836&fccid=785af18d53962443&vjs=3	Toronto, ON	Staff Data Engineer (Remote)	980a5785f8688836
2020081827	05f7bf2a7458c7fc	Job Title- Data Engineer
Location- Downtown, Toronto
Type - Full Time Permanent
Salary - Negotiable + Benefits



Focus on data architecture, best practices, reliability, security, and complianceImprove and extend ETL, data processing, and analytics processesFacility with PowerBI, including creating dashboards and data sourcesDeveloping high complexity, fast performing SELECT queries.Developing T-SQL procedures, functions, triggers, jobs, scripts, etc.Development of Advanced T-SQL such as temporal tables, PIVOTs, recursive table expressions and more.Modeling and implementing Data Mart solution for Power BI analytics
Managing indexes, statistics, query plans alerts, database activity, and overall performance activity.In-depth experience working with relational databases, such as Microsoft SQL Server or PostgreSQLEnthusiasm for applying good data design, testing, documentation, and support practicesExperience building and optimizing data pipelines, architectures, and data setsKnowledge of message queueing, stream processing, and data stores/warehousesWorking knowledge of AWS products related to data engineeringBachelor's degree in Computer Science, Software Engineering or an equivalent
Excellent communication skills - both written and verbal; ability to speak in Spanish is a bonus

To apply please send an email to sheetalk@tes.net	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DQQnUBuQBSuyaIQhpC59TW7hrTbBg8v-nGtzzV8dunbQHh9VTp-k89gJ8q3B9UEXLHcYbAG67d8jpOK4E3ok8olcOhfyyiChWMI3BPvUFdra3ljwgLS_vFsJXlW6jHt2NpQMzd2p9F6BXGm9ND3feUN42yGy4tVXCBKlhgis4DmETmLbR23von02-fLHfbz0SQ4od-72JxV4waJf4WUtLUWhH5YKRZwSvyj8mgI3Sil1zYhwvAaRYHxYMLq5xRFjVld6WdTeEqsW8grXtGi0hKh-LMhRxdg7AbMXPjKzajo2PZdXic5ZK-BbHmGoNlXfUj4PQym1UBrrMxL7Nao7oktWMGSbO80T1DScSqjP3AZRFkmKn_Pjm4ezPdObIg8OdowvPahMrI7kw4cegrVL9ZwQ10TkWKfSCfYS2XRI-6cqP_XbUm8DwS3eknGNuX0X2JYcsz6ettizpVtv28Q4xQBX4lhHeczd1hWL03DhhMcA63a0UZPDOSAPBPTXZ1FA2CfAbN98kSgwO5Y5x4z-gsze5luc00G0fMCgXzrl8ldbQlulGA-Y2a7j3sye11mvTE16tNMHQe5DKFocsLT0EqprkqKlghVV_pvTJBA5lbIZHxlagafALrbHko13H5kcOxIRU0N0_yD6BVS4P0LbxxUn9kLspZJi4KZGZIdBdIlAPcTDVpK4QQD0mdlNUKaJUrpnpeVuV84fT5PWQTYGTrMBlrNCkHosTnDdxzq60p6h_gTSedl0g93425hIItSA3YPKfrgqep_RfjqNxeAAa2A2XTJekn7RU0xQU3QaNGa6qn-m8eKpWlytp0WZhHbvSZNNKlgp154wHXplQGfPYoI7uKYVz0jcdlZWTqwONrMw==&p=12&fvj=0&vjs=3	Toronto, ON	2020-08-18T11:03:12.000Z	Data Engineer	TES - The Employment Solution
2020081828	2020-08-18T11:03:13.000Z	If you are looking to join one of Canada’s fastest growing companies, goeasy Ltd. is the place for you! Recognized as one of Canada’s Most Admired Corporate Cultures, one of Canada’s Top 50 Fintech’s and one of North America’s Most Engaged Workplaces, we want the best and brightest to join our team.

We are a publicly traded company on the TSX with over 4000% shareholder return since 2001, goeasy operates two main business units. easyfinancial is our consumer lending business that offers secured and unsecured installment loans of up to $35,000 and easyhome is Canada’s largest merchandise lease-to-own company. It is our mission to provide everyday Canadians the chance for a better tomorrow, today by giving them access to the credit they need and by offering them a second chance when they have been turned down by banks and traditional lenders. With a retail network of nearly 400 locations across Canada and over 1900 employees, we are able to build lasting relationships with our customers as we help them rebuild their credit and graduate towards prime rates and a brighter financial future.

The Data Engineer on the Data Science and Business Insights team will build, integrate data from various resources, manage data in goeasy operational data store and enterprise data warehouse. This position will develop ETL (Extract, Transform and Load) with various tools on large datasets to ensure data is easily accessible, works smoothly, as well as maintain and expand the data warehouse for reporting and analysis. The Data Engineer will also work closely with the data architect on the design and architecture of our enterprise data warehouse.

Responsibilities:

Develop data set processes for data modeling, mining and production
Develop and maintain ETL processes using SSIS, Scripting and data replication technologies
Participate in development of datamarts for reports and data visualization solutions
Research opportunities for data acquisition and new uses for existing data
Integrate new data management technologies and software engineering tools into existing structures
Support the translation of business requirements for data acquisition/manipulation and provide detailed specifications that can be passed downstream for use
Develop detailed technical specifications and operational support documentation in collaboration with Business Systems Analysts, BI Engineers and Architects.
Identify and communicate technical problems, process and solutions
Create Ad-Hoc queries and reports as needed along with providing on-going analytical support for these requests
Assist in the collection and documentation of user’s requirements
Ensure that existing business processes dependent on the ODS/EDW are monitored and respond quickly to bug fixes, enhancement requests and production ETL related issues.
Dealing with the database users on a daily basis to ensure that problems are dealt with promptly and that appropriate fixes are made to resolve any problems.
Recommend ways to improve data reliability, efficiency and quality
Ensure systems meet business requirements and industry practices
Work effectively with the Business Intelligence and Data Solutions Architects, Data and BI Engineers to ensure that all approved development and deployment procedures are followed.

Qualifications:

Bachelor’s Degree in Computer Science, MIS, Computer Engineering or other Information Technology related degree
4+ years working with SQL Server or comparable relational database system
3+ years of extensive ETL development experience with SSIS and/or ADF
4+ years of experience troubleshooting within a Data Warehouse environment
Expert domain knowledge & experience in Data warehousing, encompassing data model design, dimensional modeling, naming conventions, cross-cutting concerns, common integration technologies, patterns & standards and emerging technologies.
Expert Knowledge of SQL skills to build, debug, and optimize (developing procedures, functions, SQL queries, etc.) and working with large data sets and to perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
2+ years SQL Server Database administration experience
Cloud experience (Azure) is highly preferred
Exposure and experience with Python, R, Hadoop, Azure, other Big data and advanced analytics
Knowledge of AI and ML developments/solutions/implementations
Experience with multiple programming languages (PowerShell scripting, C#, others) with basic scripting skills.
High level of technical aptitude

Inclusion and Equal Opportunity Employment

goeasy is an equal opportunity employer. In addition, goeasy is committed to providing accommodations for applicants upon request at any stage of the recruitment process in accordance with all legislative requirements throughout Canada. Please let us know if you require an accommodation due to a disability during any aspect of the recruitment process and we will work with you to address your needs.

Additional Information:

All candidates considered for hire must successfully pass a criminal background check, credit check, and validation of their work experience to qualify for hire. We thank all interested applicants, however we will only be contacting those for interview who possess the skills and qualifications outlined above.

Why should you work for goeasy?

To learn more about our great company please click the links below:

PAID1234	goeasy	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DbNJQGwjC-NEA80-qlVyEGmSv34Xx0eW4t4u9rXSv_7_tlvMU2e-Gw_wrZ47s1zPCnrySU2yuEL07wtSyx7XOwYh5VPcVL7CxRhOYwHzR-CdmyBrwbeqommOn6y9FD_uKIYpUCb9-PC_XgwGzBDrPy7SD7JQndrQc_s4FG972NhkKwJdqngY_TB5Wj8RUtD9B0UtffeLEklyXA35HRFnVfgxBGG1LQq5Eexn4E4nyiuTGDyOKjxaIk4ngjQ-ttpB-xPEwrv_ggeQuPTzk5uz2rukAt9Cj-eZrpPXnX4heZ4DJXNO4rFZ5UWdG8_oCxJvMY23a6xph84l23k2_IDLfS5r1TtwbBcZqk5fD2_kR-On-w7aItK8PUsa2MxinQFvEgMHpiRaiVy5CqXIfzovYNfC5rh3NrcCFFkvrn-906H6abW1mxAbpSU5i3gTeRYsGXlL7-BHVHOSM2uaGv6guM_apByZ2qygiLhQzee1Vs_A==&p=13&fvj=0&vjs=3	Mississauga, ON	e8f01b72a171ac29	Data Engineer
2020081829	Our client, a boutique-size consulting firm, prides itself on being at the forefront of innovation in the Big Data space. Founded in 2010, they provide thought leadership and implementation excellence within the ever-growing data and analytics world.
They take pride in having some of the most highly trained and experienced consultants in the industry which translates into optimal value for their clients. They were one of the first companies to provide analytics and data as a service, via the cloud, as early as 2010. They strive to make sure their customers are well-positioned with the best technologies/tools in the industry, constantly evaluating new and existing technology partnerships. Some of the more prominent companies they have partnered with include; Snowflake, DataRobot, MicroStrategy, Informatica, Amazon AWS & Microsoft. They continue to invest in their most valuable resource, their people. They do this through extensive training both on the job and through various educational programs.
Due to growth, they are seeking to hire an Intermediate - Senior Integration Consultant.

Desired Skills and Experience
: University/College degree in Computer Science, Mathematics, Data Science and/or Relevant Degree
: 5+ years hands-on development, configuration, scripting and administration experience with Data Integration platforms. (i.e. Informatica, Talend, DataStage, SSIS)
: BI Experience (MicroStrategy, Looker, Tableau, PowerBI) considered a nice to have
: Extensive theoretical and practical knowledge of data warehousing principles/concepts and practical development experience in all areas of the data warehousing life cycle
: Experience with Data Management, ETL, Cloud Data (AWS), Data Integration
: Knowledge of OLAP-related principles and concepts
: Strong grasp of data modeling techniques and concepts (Normalized/Denormalized, Conceptual/Logical/Physical, Star, Snowflake, Data Vault)
: Strong knowledge and experience with relational databases such as Snowflake, SQL Server, Oracle (Advanced knowledge of reading and writing SQL, Performance analysis and tuning)
: Knowledge and experience with key Big Data technologies (Hive, Presto, Spark, Kafka, NoSQL databases, Semi-structured data access patterns (Json, Parquet, XML, etc.))
: Strong Python scripting skills
: Excellent communication skills
: Great problem-solving skills
: Leadership and good client management skills

Day to Day Activities Would Include
: Conduct relevant customer interviews to determine key business requirements and objectives
: Build appropriate analytical data models based on outcomes of user interviews
: Analyze and profile data systems to build source to target data mappings
: Review ETL performance and conducts performance tuning as required on mappings/workflows or SQL
: Administration and support of data integration infrastructure
: 2nd level on-call support of ETL services as required

You will be responsible for attaining the following goals:
: Attaining a minimum of 1 new accreditation/certification per year
: Spending 80% or more of their time on billable work
: Completing 90% or more of their agile delivery tasks on time
: Demonstrating competency in 1 new relevant technology every year	2020-08-18T11:03:14.000Z	Copperstone Connect	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Ceb7pqR8iGmLuXIZQklF6pqed3xddONNM6Jjumj-ndyGK0kULQUmQ2REqfJ5KAzFVeK_es1WVg0s1sLgKW09UsvVRp0-dSizFQr1oNyK_VIqdjla00VU9GNVvu2x4v9p3hsL7NeF-6hBksbAaSgKazsaYTODGMKQQeVuYcxoOuxvFKuD0HRLD9ZozEtlooKFNjCqtgjKDXE6o4jyIljINsV6o7s9QFkuHI0Gvv5_VBoHbPEQA79ufMO-R-nQn8lWdeINoNSKQf9UUKLG7Alo89MAxIXyngj8DTFnLNZ9aqiTsm-aSSI7uYL6wjhHg7z05iwIM9YQnCFkAyeA930ZZZc6bB6Qaadz6arCxrYN0t100yU2gqOUB2f60UkOxSBuekasas9lcYNomjHNn6Ao1erAHUqVNDTqOMx1CkPWnQzyS_g5NEqSpLd6VGMdNhpILoEdFX8lE8klh_sSG1vQ9JvyPNU_Xw5C3NjAbxfIFsMizU9ZdxBmZlcDAl_OfU_JsN0d7KPijmhRCWp_sFr7KybecEOWyBfTa4_Jra-CD0eMOlLF-X9vwdqisodVk2E9SdFyKEiO1z1SI8nveZqdvmgOVcmC1_kT2rryomtlxbjkOKudbmap1opfvYngicJMkIbpeceyBx5IBZ9beil8JDG_zAx_E3XSJYBkuCJsLHxw==&p=14&fvj=0&vjs=3	Toronto, ON	Data Engineer/Integration Consultant	54e0fcc38385e3a2
2020081830	02c0e1b67825ddaf	Koho	https://ca.indeed.com/rc/clk?jk=02c0e1b67825ddaf&fccid=0b1a3a8f9692c945&vjs=3	About KOHO
KOHO is a quickly scaling FinTech company backed by leading investors and advisors from around the world. We started KOHO because we believe in doing two things:

Democratizing access to the best financial products and giving everyone a great financial foundation.

Since our journey began 5 years ago, we’ve raised more the $60M, grown the KOHO Collective to over 100 employees and created accounts for more than 250,000 Canadians.

About the Role
As our newest Data Engineer, you would bring your database design sense and meticulous standards for clean data to the table. More than just understanding the strategic importance of data in a financial context, you’re eager to roll up your sleeves and build the infrastructure to support it. We value potential as much as we value experience, so if you think you’d be a great fit we’d love to hear from you.
Responsibilities
Maintaining clean and consistent access to all our data sources
Providing a solid foundation for calculating key business metrics
Maintaining data infrastructure to keep up with the product roadmap
Understanding data lineage and governance for a variety of data sources
Communicating updates and changes to the broader data team as well as contributing to and maintaining data-related documentation
Desired Skills & Experience
Minimum of three years of experience working with data
Able to design efficient and scalable cloud architectures
Experience writing complex SQL Queries
Knowledge of ETL patterns, including testing and maintenance
Familiar with relational / non-relational database approaches and knowing which to apply where and when
Understanding of hot/warm/cold data concepts and when each is appropriate
Understanding of event-driven and stream-based processing patterns
Ability to think holistically about uses of data, designing for ease of data access
Working knowledge of one of Python, Java
Experience with Kubernetes cluster management is an asset
Experience with data processing frameworks such as Apache Spark
Nice-to-Have Skills
Experience with Google Cloud Platform (BigQuery, Dataflow, PubSub)
Knowledge of GoLang
Joining the (lovely!) KOHO Team

We invest time and resources into making sure KOHO is as good as the people we hire. Our culture is one of collaboration, creativity, and diverse perspective. Here are some of the reasons we attract the best people:
Balance Your Life - Unlimited PTO, generous vacation, flexible WFH, and a lifestyle spending accountLevel Up - Access to an onsite certified performance coachReach Your Goals - Salary assessments twice a year, annual training allowanceThe KOHO Culture - We have won 5 "Great Place to Work ®" awards since 2019

The Fine Print

We are an equal opportunity employer and value diversity and uniqueness at our company.

KOHO is trusted with highly sensitive information. Upon joining the team, you may be asked to undergo security screening including a criminal record check.	2020-08-18T11:03:16.000Z	Toronto, ON	Data Engineer
2020081831	55e49e6c1c9af157	https://ca.indeed.com/rc/clk?jk=55e49e6c1c9af157&fccid=537b899e30af3338&vjs=3	2020-08-18T11:03:17.000Z	Senior Data Engineer	What is the Opportunity?

The DNA (Data & Analytics) group is responsible for enabling RBC to become a data-driven organization. As part of this mission, DNA works with various lines of business (Personal & Commercial Banking, Wealth Management, Insurance, Capital Markets, etc…) to create and build data-driven solutions to serve our clients better. You will be part of a strong team of developers building out our reusable, core data services to deliver value to business partners through data, insights and AI.
The DNA Data Services team will build data-driven products and services/API’s, tackle challenging and interesting data-related problems using RBC's massive internal datasets (client relationships, user behavior across channels, transactions, etc….) and strategically partner with the business to enable client interactions to be informed by Artificial Intelligence (AI).

What will you do?

 Build large-scale real-time data pipelines using the latest technologies.
 Apply design thinking and an agile mindset in working with other engineers, data scientists and business stakeholders to continuously experiment, iterate and deliver on new initiatives.
 Leverage best practices in continuous integration and delivery.
 Help drive transformation by continuously looking for ways to automate existing processes and testing and optimize data quality.
 Explore new capabilities and technologies to drive innovation

What do you need to succeed?

Must-have
 Bachelor’s degree in computer science, software engineering, or equivalent
 5+ years development experience in Java
 2+ years’ experience with streaming or messaging technologies (Kafka, etc…)
 2+ years’ experience in Big Data environments (Spark, Hadoop, etc…)
 Experience building operational REST APIs
 Strong foundational knowledge of relational databases (MySQL, SQL Server, etc…) and NoSQL stores (Elasticsearch, Neo4j, MongoDB, etc…)

Nice-to-have
Knowledge of public cloud environments (AWS, Azure)
 A passion for simplifying and automating work, making things better, continuous learning, solving open-ended problems, improving efficiency and helping others

What’s in it for you?

 We thrive on the challenge to be our best, progressive thinking to keep growing, and working together to deliver trusted advice to help our clients thrive and communities prosper. We care about each other, reaching our potential, making a difference to our communities, and achieving success that is mutual.
A comprehensive Total Rewards Program including bonuses and flexible benefits, competitive compensation, commissions, and stock where applicable
Leaders who support your development through coaching and managing opportunities
Ability to make a difference and lasting impact
Work in a dynamic, collaborative, progressive, and high-performing team
A world-class training program in financial services
Flexible work/life balance options
Opportunities to do challenging work
Opportunities to take on progressively greater accountabilities
Opportunities to building close relationships with clients
Access to a variety of job opportunities across business and geographies	Toronto, ON	RBC
2020081832	https://ca.indeed.com/rc/clk?jk=5328ae30e6b04e6f&fccid=5c98f9ffc20e640f&vjs=3	2020-08-18T11:03:17.000Z	Points International	Toronto, ON	Company Description

We’re a technology company working in the loyalty e-commerce industry. Our solutions enhance the management and monetization of loyalty currencies for more than 50 of the world’s largest loyalty brands, from frequent flyer miles and hotel points to retailer and credit card rewards. Supported by our unparalleled loyalty industry experience and technological expertise, we bring state-of-the-art loyalty commerce platforms and products to individuals and businesses in today’s loyalty marketplace.
Our casual, collaborative office is where our strong workplace culture begins. Our people are what make us great, so we empower them with the freedom to think big and the resources to make things happen. We communicate directly, lead by example, and make sure our team members know how much they are appreciated. Passion for life and work is important to us, and we want to see it in you, too!

Job Description

Points is looking for a Data Engineer to join our Data Engineering team for a permanent position in our downtown Toronto office.
We’re an industry-leading web-based organization that is continuously reshaping how consumers interact with their loyalty programs. We work with the world’s largest airline, hotel, financial, and retail rewards programs, to tackle complex challenges and come up with innovative e-commerce solutions. If you’d like to be a part of it, we’d love to hear from you.
Reporting to the Team Lead, Data Engineering, you will:
Work in a scrum based team that is passionate about data.
Design and develop scalable pipelines for data consumption by downstream applications and for reporting purposes.
Improve upon existing ETL processes and monitoring to maintain data integrity and accuracy.
Automate the boring manual stuff, preferably using Python.
Support production systems to ensure a high degree of data availability.

Qualifications

Experience using GUI ETL tools (we use Talend).
Strong knowledge of SQL.
Experience with pub/sub architectures, such as Kafka.
Experience with containers and related infrastructure, such as Docker and Kubernetes.
Self-discipline and willingness to learn.
Nice to haves
Good knowledge of general software engineering principles and practices.
Experience with columnar-oriented databases, such as Vertica.
Experience integrating with services, such as Dataiku and NetSuite.
Working knowledge of Continuous Integration and Continuous Deployment concepts.
Additional Information

Building a great company culture is as vital to us as building a great business. Over the last 5 years Points has been the recipient of the following awards:
Best Workplaces (Medium) in Canada
Best Workplaces for Women.
Canada’s Top Small and Medium Employers
Greater Toronto’s Top Employers
Here are some of the perks that are included in our Points culture:
Central downtown location in the Financial District
Connected to the PATH network of shops/restaurants
We want to celebrate with you: all employees get an extra day off for their Birthdays!
Flexible work hours and casual dress every day
Marvelous Snack Cart Fridays: free refreshments and snacks!
Free coffee, tea, juice, pop, and snacks
Monthly subsidized lunch program
Green commuter and fitness subsidies
Secure bike storage with showers and towel service
Company-sponsored activities: bowling, movies, sports, paintball, and more!
Points is an equal opportunity employer and is committed to providing an accessible recruitment process. Upon request we will provide accommodation for applicants with disabilities.
All your information will be kept confidential.
No agencies please.	5328ae30e6b04e6f	Data Engineer
2020081833	https://ca.indeed.com/rc/clk?jk=e493781e9354119e&fccid=785af18d53962443&vjs=3	e493781e9354119e	FreshBooks	Senior Data Engineer (Remote)	2020-08-18T11:03:18.000Z	FreshBooks has a big vision. We launched in 2003 but we're just getting started and there's a lot left to do. We're a high performing team working towards a common goal: building an elite online accounting application to help small businesses better handle their finances. Known for extraordinary customer service and based in Toronto, Canada, FreshBooks serves paying customers in over 120 countries.

The Opportunity - Senior Data Engineer

FreshBooks is seeking a Senior Data Engineer to join our team. You will help build new features and update existing ones in our current data pipeline infrastructure. If you're committed to great work and are constantly looking for ways to improve the systems you're responsible for, we'd love to chat with you!

What you'll do:

Collaborate with data engineers and full-stack developers on cross-functional agile teams working on features for our stakeholders.
Work closely with our analytics, data science, product and other internal business teams to ensure their data needs are met.
Participate and share your ideas in technical design and architecture discussions.
Ship your code with our continuous integration process.
Provide coaching to data engineers and share and learn from your peers.
Develop your craft and build your expertise in data engineering.

What you bring:

Enthusiasm for data engineering!
Experience creating and maintaining data pipelines
Experience with AWS, or another major cloud provider such as Google Cloud Platform.
Experience with Redshift, Big Query, or similar cloud storage technologies.
Strong programming skills in Python or similar language
A strong practitioner of test-driven (and behavioural test-driven) development
Experience with Git workflows, continuous integration and automated build pipelines.
Experience working in an Agile environment.

What you might bring:

A track record of staying at the forefront of data engineering technology.
Experience with BI tools: Periscope, Looker.
Experience with Spark, Kafka, Flink, Dataflow, or other streaming technologies.
Experience with Docker, Kubernetes, Terraform, and other DevOps and infrastructure as code technologies.
A limitless imagination for where data could go and what we can do with it to make our customers and our people awesome!

Why Join Us

We're an ambitious bunch, with our eyes laser-focused on shipping extraordinary experiences to small business owners. You'll be surrounded by talented team members who share a common vision for what an amazing software company could be, and have the opportunity to help build a world-class one, right here in Toronto, Canada.

Apply now

Have we got your attention? Submit your application today and a member of our recruitment team will be in touch with you shortly!

FreshBooks is an equal opportunity employer that embraces the differences in all of our employees. We celebrate diversity and are committed to creating an inclusive environment for all FreshBookers. All applicants are evaluated based on their experience and qualifications in relation to this position.

FreshBooks provides employment accommodation during the recruitment process. Should you require any accommodation, please indicate this on your application and we will work with you to meet your accessibility needs. For any questions, suggestions or required documents regarding accessibility in a different format, please contact us at phone 416-780-2700 and/or accessibility@freshbooks.com	Toronto, ON
2020081834	Big Data Engineer
LOCATION: TORONTO

Capco – The Future. Now.

Capco is a distinctly and positively different place to work. Much more than consultants, we are active participants in the global financial services industry. Our passionate business and technology professionals enjoy a unique environment where they are actively encouraged to apply intellect, innovation, experience and teamwork. We are dedicated to fully supporting our world class clients as they respond to challenges and opportunities in: Banking, Capital Markets, Finance Risk & Compliance, Insurance, and Wealth and Investment Management. Experience Capco for yourself at capco.com

Let’s Talk About You

You want to Own Your Career. You’re serious about rising as far and as fast as your work and achievements can take you. And you’re ready to write the next chapter of your career story: a challenging and rewarding role as a Capco Big Data Engineer.


Let’s Get Down To Business

Capco is looking for talented, innovative, and creative people to join our development team to work on a number of projects and applications with a Data focus within the Digital practice.

Fitting that description, you will also need to be personally motivated to work in a team where clients become colleagues too.

Responsibilities

Produces high quality complex, deliverables with minimal input from stakeholders
Manage full software lifecycle for medium complexity projects from requirements, to design, to implementation, to testing
Develop and maintain back end solutions using cutting edge technologies and products
Work with Scrum Masters and product owners to priorities and deliver solutions using an Agile environment
Build reusable code and libraries for future use and follow emerging technologies
Mentor and train junior developers

Education/Experience

Bachelor’s degree (preference given to Computer Science, Engineering, Gaming and STEM-based majors) or equivalent experience
Five (5) or more years of experience as a Full-stack Data Engineer/developer on Data driven projects
Strong understanding of the full development lifecycle including requirements, architecture, design, development and testing
Strong development experience with Scala/Spark
Experience working with REST APIs/Springboot.
Familiarity working with Java and Hive.
Ability to balance competing priorities in a very dynamic and fast-paced environment
Excellent detail-oriented, problem solving skills and the ability to quickly learn and apply new concepts, principles and solutions
Must have excellent communication skills (verbal and written)

Show Us What You’ve Got

It will be very useful if you have some or all of the following skills:

Understanding of big data and distributed programming concepts
Experience working with ASW, GCloud, Docker, Kubernetes
Experience working with Microservices, CQRS, EventSourcing
Experience working with Spring, Akka, Spark
Experience working with Reactive Streams (Rx, Akka, Reactor)
Strong organizational and communication skills
Experience working in an Agile environment
Experience working with code versioning tools
Experience working with build, packaging and continuous integration tools and frameworks


Professional experience is important. But it’s paramount you share our belief in disruptive innovation that puts clients ahead in a tough market. From day one, your key skill will be to perceive new and better ways of doing things to give your clients an unfair advantage.


Now Take the Next Step

If you’re looking forward to progressing your career with us, then we’re looking forward to receiving your application.

Capco is well known for its thought leadership and client-centric model that distinguishes it from other consulting firms. Capco’s strong technology and digital knowledge base, it’s global experience of the Financial Service enables us to deliver projects from strategy through to delivery. We are committed to providing new areas of expertise from which our clients will greatly benefit. We have:

Access to industry-focused talent globally
Ability to leverage best-of-breed, innovative products and solutions for complex architecture and large-scale transformation
Extended global geographic market reach
Ability to capitalize on our client footprint and deep domain expertise within financial services

For more information about Capco, visit www.Capco.com.

Capco is an equal opportunity employer. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, marital status, genetic information, national origin, disability, veteran status, and other protected characteristics.	https://ca.indeed.com/rc/clk?jk=20480654089beb5a&fccid=c2a63affe8751868&vjs=3	CAPCO	CATO - Big Data Engineer	2020-08-18T11:03:19.000Z	20480654089beb5a	Toronto, ON
2020081835	Join SADA as a Data Engineer!

Your Mission

As a Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data issues facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring a combination of batch or streaming data pipelines, data lakes and data warehouses.

You will be recognized as an established contributor by your team. You will contribute design and implementation components for multiple projects. You will work mostly independently with limited oversight. You will also participate in client-facing discussions in areas of expertise.

Pathway to Success

#BeOneStepAhead: At SADA we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage adaptability. This means that not only do our engineers understand that change is inevitable, but they embrace this change to continuously broaden their skills, preparing for future customer needs.

Your success comes from your enthusiasm, insight, and positive impact. You will be given direct feedback quarterly with respect to the scope and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, your collaboration with your peers, and the consultative skill you demonstrate in customer interactions.

As you continue to execute successfully, we will build a personalized development plan together that leads you through the engineering or management growth tracks.

Expectations

Required Travel - 30% travel to customer sites, conferences, and other related events. Due to the COVID-19 pandemic, travel has been temporarily restricted.
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with a first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:
Google Professional Data Engineer Certified or able to complete within the first 45 days of employment
Required Qualifications:
Expertise in at least one of the following domain areas:
Big Data: managing Hadoop clusters (all included services), troubleshooting cluster operation issues, migrating Hadoop workloads, architecting solutions on Hadoop, experience with NoSQL data stores like Cassandra and HBase, building batch/streaming ETL pipelines with frameworks such as Spark, Spark Streaming and Apache Beam, and working with messaging systems like Pub/Sub, Kafka and RabbitMQ.
Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for minimizing downtime. May involve conversion between relational and NoSQL data stores, or vice versa.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or other customer-facing role
Useful Qualifications:
Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Applied experience operationalizing machine learning models on large datasets
Knowledge and understanding of industry trends and new technologies and ability to apply trends to architectural needs
Demonstrated leadership and self-direction - a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem
About SADA

Values: We built our core values on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA's values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

Make them rave
Be data driven
Be one step ahead
Be a change agent
Do the right thing

Work with the best: SADA has been the largest partner in North America for GCP since 2016 and has been named the 2019 and 2018 Google Cloud Global Partner of the Year. SADA has also been awarded Best Place to Work by Inc. as well as LA Business Journal!

Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, RRSP, professional development reimbursement program as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.	SADA	https://ca.indeed.com/rc/clk?jk=47eea899183d4196&fccid=b704562e07a2a03f&vjs=3	2020-08-18T11:03:20.000Z	Toronto, ON	47eea899183d4196	Data Engineer
2020081836	Requisition ID: 65995

At Scotiabank Digital Factory, the reach and opportunity of a global organization meets the passion and drive of a startup. Reinvention starts here—and it starts with you.
We believe that the pathway to success is paved with user feedback, so we’re constantly asking our customers how they want to bank. When we find an opportunity to make things people will love, we assemble teams of specialists to collaborate in Digital Factories in Canada, Chile, Mexico, Peru, and Colombia. We work fast, and we work smart. Join us to create meaningful experiences and build the bank of tomorrow for our customers around the world.
If you’d like to learn more about the Digital Factory workspace, our Digital Banking career opportunities, Communities of Practice, and open source projects, check out our microsite: https://digitalfactory.scotiabank.com/

Disclaimer
“This is for ongoing opportunities and not for an active or specific position. As such, you may not hear back from us immediately. In addition, this is a general overview of the Digital Analytics Community of Practice; therefore actual roles, in reality, will vary in their seniority and specific skill sets needed.”
What does Analytics at Scotia really mean?
We like to think of ourselves as superheroes, using the power of data to benefit the Bank and its customers! We make data-driven decisions that protect our customers, and we partner with internal clients, like our Marketing and Engineering teams, to lead data-driven initiatives that align with critical business objectives. Every project we’re involved in benefits from our keen eye for detail and endless appetite to investigate the numbers we’re working with. We focus on three major streams: Analytics and Reporting, Data Operations (Data Engineering), and Data Science.

Who would I be working with and what are they like?
In addition to working on projects that directly correlate to business objectives, we’re always striving to think of ways to improve the way our customer's bank through a good sense of precision, detail and carbon science. You’ll get to work with a variety of tools, including open-source, as well as within a variety of environments depending on the business line and initiative you’re focusing on.
A couple of key projects we’ve recently worked on include…
Search Engine for Scotiabankers: We have a lot of smart people working together at Scotiabank, but sometimes it’s hard to stay on top of information (not to mention acronyms)! We created a bank-wide search engine that links to the millions of pages that are buried in the background but hold valuable information.


Scotiabank’s new mobile banking app: While this was a collaborative effort, the Analytics and Data Science teams played an integral role in the development of the new Scotia app. With a customer-first mindset and approach, we supported the teams by helping them target audiences more intelligently. We know that a student has different needs than a first time home buyer, and we’re making sure that information is relevant based on personalization.

Sounds intriguing… but how do I fit in?
Being naturally curious and persuasive is how this team thrives!
You are able to walk us through your thought process and understand how to share your findings in business terms
You ache to solve the puzzle, and aim to present your insight once you solve it
You have an idea and you look both ways before crossing the street with it
You’re not afraid to work hard and adhere to deadlines
At times things can change on the fly so it’s important that you can adapt quickly to lots of moving parts: previous experience in an agile environment is always helpful
On the technical side, we are looking for domain knowledge, GCP, Python, R, or Hadoop
Key areas we recruit for include…

Data Scientist: You will create and deploy model and algorithm testing strategies to ensure model predictability holds.
Data Strategist: You will provide the insights, areas of opportunity and work with the stakeholders to implement the findings.

Data Engineer: You will be Integral to maintaining operational stability, and formulating techniques for quality data collection to ensure adequacy, accuracy, and legitimacy of data.
What’s in it for me?
We have an inclusive and collaborative working environment that encourages creativity, curiosity, and celebrates success!
We provide you with the tools and technology needed to create beautiful customer experiences
You'll get to work with and learn from diverse industry leaders, who have hailed from top technology companies around the world
Dress codes don't apply here, being comfortable does!
Free shuttle service to and from Union Station, which means more money in your pocket
Onsite subsidized cafeteria with a chef, so you can snack all day, every day
Onsite fitness center, so you can feel your best
Access to thousands of online and in-person courses so you can brush up on skills, or learn new ones
Clear career mapping and progression. We hire you for your talent and not just for the job. We want to see you succeed not just in your role but in your career as a whole
A competitive rewards package that includes a base salary, a performance bonus, company matching programs on pension and profit sharing, paid vacation, personal & sick days, medical, vision and dental benefits that start from day one and much more!

Location(s): Canada : Ontario : Toronto
As Canada's International Bank, we are a diverse and global team. We speak more than 100 languages with backgrounds from more than 120 countries. We value the unique skills and experiences each individual brings to the Bank, and are committed to creating and maintaining an inclusive and accessible environment for everyone. If you require accommodation (including, but not limited to, an accessible interview site, alternate format documents, ASL Interpreter, or Assistive Technology) during the recruitment and selection process, please let our Recruitment team know. If you require technical assistance please click here. Candidates must apply directly online to be considered for this role. We thank all applicants for their interest in a career at Scotiabank; however, only those candidates who are selected for an interview will be contacted.	Scotiabank	2020-08-18T11:03:20.000Z	Learn more about our Digital Analytics team!	https://ca.indeed.com/rc/clk?jk=d21400c3ecab646b&fccid=3002307a9e5b4706&vjs=3	Toronto, ON	d21400c3ecab646b
2020081837	2020-08-18T11:03:21.000Z	https://ca.indeed.com/rc/clk?jk=71cd34c1d32326c5&fccid=1747adf6142beb48&vjs=3	Manulife	Senior Data Engineer	71cd34c1d32326c5	Toronto, ON	Are you looking for unlimited opportunities to develop and succeed? With work that challenges and makes a difference, within a flexible and supportive environment, we can help our customers achieve their dreams and aspirations.
Job Description
Manulife’s Global Data Office (GDO) is seeking a Sr. Data Engineer reporting into the Director, Advanced Analytics and AI Engineering & Enablement Lead. Located in Toronto, Canada - the role will champion and support strategic and global data initiatives that strengthens Manulife’s global data and advanced analytic capabilities, foster cross-segment collaboration and communication helping build an agile data insight driven culture, and lead and nurture open data design and architecture establishing conditions for successful technical and analytic innovation. The Sr. Data Engineer will develop, maintain, and test: data pipelines, application framework, infrastructure for data generation and work closely with Data Scientists to enable their work using modern data architecture and tools.
Job Description
Manulife has a clear vision for a Global Data Strategy. By liberating and strengthening Manulife’s data capabilities we will enable deeper insights, better product and service design, and more effective business processes. The result will be exceptional experiences for our customers.

Key Responsibilities:
Leveraging new & existing Big Data & Cloud technologies contributing to the innovative design, development and management of data analytics labs supporting to increase knowledge and insight from enterprise data
Perform technical systems and data flow development in a variety of projects for complex front, middle and back office applications, with a focus on the reporting and analytics environment; this environment is built on a Microsoft Azure cloud and is underpinned by a Hortonworks Hadoop stack
Perform technical systems and data flow design for small-to-medium sized projects
Work with multiple project execution and deployment teams (e.g. Development, Business Analysis, Architecture, Release Management, Production Support)
Work closely with the technical leads and architecture teams, and align solutions that meet the departmental architectural vision
Assess the completeness and accuracy of data, identifying gaps in data, provide feedback to business and system owners with guidance and options to obtain missing information
Design, build and implement modern data architectures in development and production environments (data orchestration pipelines, data sourcing, cleansing, augmentation and quality control processes)
Translates business needs into data engineering and architecture solutions
Contributes to overall solution, integration and enterprise architectures
Build and support deploying machine learning models in development and production environments
Provide proactive data ingestion and analysis of large structured and unstructured datasets involving a wide range of systems across Group Functions (i.e., Finance, Treasury, Risk, Human Resources, Brand & Communications)
Evaluating existing and proposed data models and how to best access and query them as well as existing and proposed data interfaces and how to clearly document them, including specification of data flow models, data flow timing, data mapping, and data transformation rules including data validations and controls

Education, Experience & Skills:
Demonstrated 2-5 years of professional experience in related industry experience in working in big data/data management & understanding big data analytic tooling and environments including a University degree and or Master’s degree in Engineering, Computer Science or equivalent quantitative program
Experience in Big Data, Analytics and Business Intelligence technologies to support design, build and implementation for advanced analytics and business intelligence reporting;
Experience working with Cloudera and/or Hortonworks Hadoop stack
Experience with big data processing frameworks and techniques such as HDFS, MapReduce, Syncsort, Sqoop, Oozie, Storage formats (Avro, Parquet), Stream processing (NiFi, Kafka), etc.
Understanding of relational and warehousing database technology working with Hadoop and other major databases platforms (e.g., Hadoop, Oracle, SQLServer, Teradata, MySQL, or Postgres)
Experience in data technologies and use of data to support software development, advanced analytics and reporting. Focus on Cloud (Azure), Hadoop-based technologies and programming or scripting languages like Java, Scala, Linux, C++, PHP, Ruby Python, R and SAS.
Knowledge regarding different databases such as Hawq/HDB, MongoDB, Cassandra or Hbase.
Working knowledge of modern data streaming using Kafka, Apache Spark and data ingestion frameworks: NiFi, Hive and Pig
Experience writing complex SQL and NoSQL jobs to analyze data in both traditional DBMS (MS-SQL, Oracle) and Big Data environments (i.e., HADOOP, SPARK, or similar open source and commercial technologies)
Knowledge of non-relational (Cassandra, MongoDB) databases preferred
Predictive analytics and machine learning experience (scikit-learn, Tensorflow, MLlib, recommendation systems) preferred
Experience with integrating to back-end/legacy environments
Experience integrating business and technology teams
Knowledge and familiarity with machine learning models application and production pipelines
Collaborative attitude, willingness to work with team members; able to coach, participate in code reviews, share skills and methods
Remains current with emerging technologies, innovations and practices within the data and analytics industry
Good organizational and problem-solving abilities that enable you to manage through creative abrasion
Good verbal and written communication; effectively articulates technical vision, possibilities, and outcomes
Strong work ethic, results oriented, and accuracy / attention to detail are critical; ability to work in agile or scrum delivery environments
Exceptional oral, written and interpersonal communication skills with the ability to simplify complex technical concepts into business & value-focused language. A key requirement is to communicate clearly and consistently keeping stakeholders well-informed of progress and challenges
Excellent organizational and time management skills, strong business presence with ability to multi-task and effectively deal with competing priorities. Ability to work with minimal or no supervision while performing duties; has the ability and initiative to organize various functions and be a strong team player.
What about Perks?
Manulife has lots of perks including, but not limited to:
Competitive compensation
Retirement Savings Accounts including a RPP (Pension Plan), RRSP (Retirement Savings Plan), and TFSA (Tax Free Savings account)
Manulife Share Ownership Program with employer matching
Customizable Benefits Package including Health, Dental, Vision, and 100% of Mental Health expenses
Financial support for ongoing training, learning, and education
Monthly Innovation Days (Hackathons)
Wearing jeans to work every day
An abundance of career paths and opportunities to advance.
This is a full-time, permanent role located in Toronto, Ontario.

If you are ready to unleash your potential it’s time to start your career with Manulife/John Hancock.
About Manulife
Manulife Financial Corporation is a leading international financial services group that helps people make their decisions easier and lives better. With our global headquarters in Toronto, Canada, we operate as Manulife across our offices in Canada, Asia, and Europe, and primarily as John Hancock in the United States. We provide financial advice, insurance, and wealth and asset management solutions for individuals, groups and institutions. At the end of 2019, we had more than 35,000 employees, over 98,000 agents, and thousands of distribution partners, serving almost 30 million customers. As of March 31, 2020, we had $1.2 trillion (US$0.8 trillion) in assets under management and administration, and in the previous 12 months we made $30.4 billion in payments to our customers. Our principal operations are in Asia, Canada and the United States where we have served customers for more than 155 years. We trade as 'MFC' on the Toronto, New York, and the Philippine stock exchanges and under '945' in Hong Kong.
Manulife is an equal opportunity employer. We strive to attract, develop and retain a workforce that is as diverse as the customers we serve and to foster an inclusive work environment that embraces the strength of cultures and individuals. We are committed to fair recruitment, retention and advancement and we administer all of our practices and programs based on qualification and performance and without discrimination on any protected ground.
It is our priority to remove barriers to provide equal access to employment. A Human Resources representative will consult with applicants contacted to participate at any stage of the recruitment process who request any accommodation. Information received regarding the accommodation needs of applicants will be addressed confidentially.
2020081838	https://ca.indeed.com/rc/clk?jk=485168fc8141d4c6&fccid=efee8d7cab418992&vjs=3	485168fc8141d4c6	Toronto, ON	StackPros	StackPros Inc is seeking a candidate for a full-time role within our Data Systems Team in Toronto, Ontario.
The Cloud Data Engineer will play a key role at StackPros, required to help create and maintain industry-leading quality and efficiency of service and software delivery.
StackPros will rely on the Data Engineer to support the Data Systems team, in both data engineering and data science-related workflows. The Data Engineer will be expected to meet and exceed StackPros’ quality standards, while helping the organization rapidly expand complex Machine Learning and related applications.
Key Responsibilities:
Data Engineering-Specific Responsibilities
Participate in continuous delivery pipeline to fully automate deployment of the highly available cloud platform that supports multiple projects
Design and develop ETL workflows and datasets to be used in data visualization tools
Write complex SQL queries with multiple joins to automate and manipulate data extracts
Perform end to end Data Validation to maintain accuracy of data sets
Build tools for deployment, monitoring and operations
Troubleshoot and resolve issues in the development, test and production environments
Develop re-useable processes that can be leveraged and standardized for multiple instances
Prepare technical specifications and documentation for projects
Stay up-to-date on relevant technologies, plug into user groups, understand trends and opportunities to ensure we are using the best possible techniques and tools
Understand, implement, and automate security controls, governance processes, and compliance validation
Design, manage, and maintain tools to automate operational processes
Data Science-Specific Responsibilities
Perform exploratory data analysis to identify patterns from historical data, generate and test hypotheses, and provide product owners with actionable insights
Design experiments for product initiatives and perform statistical analysis of the results with recommendations for next steps and future experiments
Create and design dashboards by using different data visualization tools to present reports and insights, and support business decision making
Help the DRVN Intelligence Data Systems team adopt and evolve Predictive Modeling, Machine Learning and Deep Learning processes to deliver to clients in the future
Qualifications:
3+ years experience in Data Engineering
Understanding of digital ecosystems including online data collection, cloud systems and analytics tools (Google Stack, Facebook, AWS, Salesforce, Adobe Suite etc.)
Strong technical understanding of a range of marketing concepts such as cookie-based data collection, setting and leveraging audience segments, attribution modelling, AB/N & multivariate
Excellent written & verbal communication skills are essential; candidate should be comfortable presenting and participating in group discussions of concepts with internal and external stakeholders
Candidate must exhibit an analytical, detail-oriented approach to problem solving
Experience with Jira / Atlassian project management tools is an asset
Company-Wide Responsibilities:
Maintain and exceed client satisfaction with StackPros Inc.’s deliverables, day-to-day work and overall value as a partner
Cultivate opportunities for company growth, always seek areas where StackPros Inc.’s role could be expanded
Adapt to ever-changing client needs and expectations
Maintain dedication toward achieving excellence in StackPros Inc.’s delivery against client needs, and overall success as an organization
Be an enthusiastic, positive and generally awesome team mate, mentor & constantly curious learner	2020-08-18T11:03:22.000Z	Data Engineer (Cloud)
2020081839	Senior Data Engineer	467293199adbaa6d	Help us boldly shape retail in Canada

Canadian Tire Corporation’s (CTC) rich heritage of serving Canadians from coast-to-coast dates back to 1922. Our vision is to become the #1 retail brand in Canada by 2022 and we are focused on innovating and making important investments in our business, especially when it comes to our people. To reach our goal, we need the best talent to help us evolve and drive change across the business – and boldly help shape Canada’s retail industry. As we strive to be at the forefront of a complex and vastly changing retail industry, it is an exciting time to join the Canadian Tire family of companies.

The Customer Insights and Analytics Group at Canadian Tire delivers customer-focused analytical solutions and insights that enable lasting and meaningful customer relationships. We are a diverse and dynamic team of analysts, data scientists, developers, and consumer researchers united by a common goal – placing the customer at the center of every business decision. We utilize the latest data technologies and advanced analytical techniques to demystify shopper behaviour and embed those insights deep within the fabric of the business. Genuinely passionate about the transformative power of customer data and our role as the “voice” of our customers, we relentlessly push the boundaries of data analytics to ensure we understand our customers better than anyone else and deliver the experiences that will earn their continued loyalty. Our work is redefining the Canadian Tire shopping experience and is a key pillar in our mission to become the #1 retail brand in Canada.

What you’ll do

Reporting to the Manager, Data Science, the Senior Data Engineer will be responsible for driving analytical innovation and leading the technical and methodological capability within the wider team. They play a critical role in the delivery of our big data analytics roadmap and development of machine learning capabilities. The Senior Data Engineer partners with internal stakeholders to drive the adoption of advanced analytics and machine learning across the business.
Develops and maintains scalable data pipelines and builds out new API integrations to support continuing increases in data volume and complexity.
Collaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.
Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.
Writes unit/integration tests, contributes to engineering wiki, and documents work.
Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues.
Works closely with a team of frontend and backend engineers, product managers, and analysts.
Defines company data assets (data models), spark, sparkSQL, and hiveSQL jobs to populate data models.
Designs data integrations and data quality framework.
Designs and evaluates open source and vendor tools for data lineage.
Works closely with all business units and engineering teams to develop strategy for long term data platform architecture.
Who you are

We are looking for individuals who are:
Creative and courageous, with the ability to manage in an environment of change and ambiguity to help us take bold, strategic moves in this rapidly evolving retail environment
Action oriented, and comfortable taking calculated risks to better serve our customers and business
Outcome focused, critical thinkers with the ability to analyze and visualize, to ensure continuous improvement across our entire business
Collaborative team players with superior influencing skills, who build relationships easily across various stakeholder groups to move initiatives forward
If you’re curious, ready to take on new challenges and open to doing things differently to help us evolve rapidly, then this is definitely the place to be.

What you bring
Knowledge of best practices and IT operations in an always-up, always-available service
Experience with or knowledge of Agile Software Development methodologies
Excellent problem solving and troubleshooting skills
Process oriented with great documentation skills
Excellent oral and written communication skills with a keen sense of customer service
BS or MS degree in Computer Science or a related technical field
4+ years of Python development experience
4+ years of SQL experience
4+ years of experience with schema design and dimensional data modeling
Experience in PySpark
Experience in designing end to end solution
Familiar with Design patterns and best practices in designing system
Familiar with orchestration tools such as Airflow
Experience designing, building, and maintaining data processing systems
About Canadian Tire Corporation

Canadian Tire and its family of companies are boldly shaping retail in Canada and we continue to deliver a positive experience for our customers. As one of the most trusted brands in Canada, our employees take pride in the work we do across the country. It’s more than the iconic triangle that keeps our employees around. From benefits and perks, to learning and development opportunities, to our commitment to Jumpstart – these are some of the many reasons why Canadian Tire Corporation is one of Canada’s Top Employers.

To learn more about this team and the Canadian Tire family of companies follow us on LinkedIn.

#LI-CW1
Canadian Tire is an equal opportunity employer. We are committed to a diverse and inclusive workplace for all. We recognize that our future success depends on the perspectives and contributions of all our employees - their diverse backgrounds, abilities and experiences make our business stronger. If you are contacted for a job opportunity, please advise us of any accommodations needed to ensure fair and equitable access throughout the recruitment and selection process. All accommodation information provided will be treated as confidential and used only for the purpose of providing an accessible candidate experience. Loyalty & Customer Insights
Ontario-Toronto
Permanent
Full-time
Job Posting
 : Jul 29, 2020, 2:20:49 PM	Toronto, ON	Canadian Tire	2020-08-18T11:03:23.000Z	https://ca.indeed.com/rc/clk?jk=467293199adbaa6d&fccid=2e16043297423070&vjs=3
2020081840	2020-08-18T11:03:24.000Z	https://ca.indeed.com/rc/clk?jk=6efbc65db82f1b2d&fccid=a31d39aa57e3f105&vjs=3	Toronto, ON	Achievers delivers an Employee Success Platform™ that enables social recognition, which dramatically increases employee engagement and drives business success. Designed specifically to meet the needs of today’s workplace, it empowers employees to recognize each other in real time and aligns them to the goals of the company. With more than 5,000,000 annual recognitions, the Platform inspires brilliant performance in 110 countries. Visit us at www.achievers.com to learn more and join us in our mission to change the way the world works.

Why work with us?
We’re a fun-loving, passionate, and highly collaborative team
We believe in moving quickly, failing fast, and adapting to change
We’re committed to achieving technical excellence in everything we do
We value team work, learning from failure and innovation.

Why we'd want to work with you?
You have an undying passion for building world-class software
You’re positive thinking, solution focused and find opportunities instead of problems
You have superb technical chops, but you’re always striving to improve
You want to take ownership over your work and make challenging architecture decisions
You are ambitious and want to be involved in the strategic thinking of platform architecture

What would a typical day look like, you ask? Something like this:
Design, develop, and maintain the software and systems that drive our back-end systems
Extend our platform by researching and applying new technologies to solve business problems
Participate in multi-disciplinary projects with our applications and analytics teams
Contribute to our team’s growing set of development platforms, tools, and processes

Do you have what it takes? We will be responding to applicants that have:
B.Sc. or Masters (preferred) in Computer Science or related field
8+ years of relevant experience
Prior experience with microservices architecture
Experience with messaging systems, preferably Kafka or RabbitMQ
Experience with Restful API
Proficient in web frameworks and PHP as well as other server-side language such as Python, Ruby, Java, JavaScript/React etc.
Advanced knowledge of data structures and security practices
Excellent problem-solving abilities
About Achievers:

As Achievers employees, we are passionate about disruptive technology, welcome constant change, and understand the value of employee success in the workplace. We enjoy coming to work every day because we believe in our product and love our culture. Achievers is more than just a software company; we are industry leaders in the HR space.

Our headquarters are conveniently located in Liberty Village, close to bars, shops and restaurants and are highly accessible by TTC and GO Transit.

We have been recognized in numerous publications for our contributions to HR, for technical excellence and for our outstanding workplace culture:

Achievers is ranked as number 32 on the list of Top 50 Best Workplaces in Canada.
Achievers has been recognized on the 2020 list of Best Workplaces™ for Women in Canada
Achievers has been recognized on the 2020 list of Best Workplaces™ for Inclusion
Achievers has been named one of the Top 10 Employee Recognition Solution Providers Globally

Check out our platform in action here

Our employees are a diverse and inclusive team of passionate, hardworking individuals. Achievers is committed to creating an environment where our employees can do the best work of their lives. We encourage all qualified candidates to apply to join our A-Player family. Accommodations are available on request for candidates taking part in all aspects of the selection process.	Achievers	Data Engineer	6efbc65db82f1b2d
2020081841	https://ca.indeed.com/rc/clk?jk=24716d17b181ff38&fccid=28f79c18789111e8&vjs=3	24716d17b181ff38	Intermediate Data Engineer	2020-08-18T11:03:25.000Z	Company Description

The Company
Hitachi Solutions is an impact-driven global leader in consulting dedicated to delivering competitive, end-to-end solutions based on the Microsoft Cloud. Our deeply connected teams are unified by our values and our commitment to helping clients succeed and compete with the largest global enterprises. We are a division of the 38th largest company in the world and carry the strength of a vast network of interconnected Hitachi companies all while remaining nimble, agile, and ready to pivot at a moment’s notice.
Hitachi Solutions started as three founding partners and transformed into nearly 2,000 consultants, developers, and support personnel all around the globe. With 36 Microsoft Partner of the Year awards, Hitachi Solutions has established itself as a leading partner in the ever-growing landscape of technology consulting.
The Culture
Our team is a mix of curious, fun, and get it done. We celebrate collaborative thought leadership and individual talents so that our ideas are translated into real-world results.
Each day, our team members show up to work as a blank slate with the sole purpose of coloring their canvases with knowledge. We are a complex group of resilient learners and encourage one another to go beyond the limits of conventional expectations. As a member of the dynamic Hitachi Solutions team, you will be challenged, pushed, and unconditionally supported in your efforts to drive the business forward.
** THIS ROLE IS OPEN TO ANY CANDIDATES FROM WITHIN CANADA**

Job Description

As an Analytics Data Engineer for Hitachi’s Azure Cloud Enablement Team you will be responsible to deliver high quality modern data solutions while being part of a dynamic and fast-growing team consisting of endless opportunities.
The successful candidate will be a self-motivated, passionate individual who strives to be the best at what he/she does and creates a trail of ecstatic Customers for life. This person will love to learn, contribute to the team and wants to be part of something great.
Knowledge and Experience
Hands-on experience with the Azure Data Platform (Data Factory, Data Lake, Data Warehouse, Blob Storage, SQL DB, Analysis Services)
Data quality (profiling, cleansing, enriching)
Data Modeling – including design from conceptual to logical to physical data models
Considered to be an expert in T-SQL
Hands-on experience with MPP database technologies such as Azure SQL DW, Teradata Netezza, etc.
Experience with multiple components listed, required:
Power BI including DAX
Database migration from legacy systems to new solutions
DevOps
Interpreted languages (i.e. python, C-sharp, Java, Scala, etc.)
Databricks
LogicApps
PowerApps
HDInsight
D365FO / CE experience as it pertains to data extraction
Knowledge of Cap Theorum and Distributed Database Management Systems, iis considered an asset.
Opportunity for a career path into a Data Scientist role if desired

Qualifications

Required skills / qualifications
Proven ability to engage customers to understand customer challenges and needs to develop technical solutions
3+ years of hands on experience working with the Azure Platform and its relevant components
Proven experience of architecting Azure services into a solution platform on Microsoft Azure for Analytics
Minimum 5 years hands on development experience with ELT/ETL using MS SQL Server, Oracle or similar RDBMS Platform
Familiarity with data visualization tools (e.g. PowerBI, Tableau etc.)
Experience or desire to coach, mentor and provide leadership to team members
Post-secondary degree/diploma in Business, Computer Science or a related discipline;
Strong communication skills, both written and verbal
Prepared for domestic and US travel as required
Preferred considered an asset, NOT required:
Project management experience
Databricks and Spark SQL
Previous Consulting experience
Additional Information

Opportunity Benefits:
Medical and Dental Benefit Package (including Long Term and Short Term Disability)Base salary plus targeted bonus package
This position can be based anywhere in Canada, though travel might be required.	Toronto, ON	Hitachi Solutions
2020081842	281e0356c31acae9	2020-08-18T11:03:25.000Z	Brampton, ON	https://ca.indeed.com/rc/clk?jk=281e0356c31acae9&fccid=105ecfd0283f415f&vjs=3	Data Engineer	Job Description:
3-6 yearsExpertise on Spark Scala.Ability to develop ETL jobs to implement business logic using Scala (Spark Framework)Conversant with Hive Database, Able to create HQL scripts and work on Hive tables for data analysisPerformance tuning of the existing Hadoop jobs, able to trouble shoot and fix existing bugs.Good understanding of Oracle Exadata RDBMS, able to profile telecom data residing in Exadata and derive business rules.Co lace with business, have working session with business to identify and freeze business logic.Understanding / experience working on scrum based Agile set up.

The Capgemini Freelancer Gateway is enabled by a cutting-edge software platform that leads the contingent labor world for technology innovation. The software platform leverages Machine Learning and Artificial Intelligence to make sure the right people end up in the right job.

A global leader in consulting, technology services and digital transformation, Capgemini is at the forefront of innovation to address the entire breadth of clients’ opportunities in the evolving world of cloud, digital and platforms. Building on its strong 50 year heritage and deep industry-specific expertise, Capgemini enables organizations to realize their business ambitions through an array of services from strategy to operations. Capgemini is driven by the conviction that the business value of technology comes from and through people. It is a multicultural company of over 200,000 team members in more than 40 countries. The Group reported 2018 global revenues of EUR 13.2 billion.	Capgemini
2020081843	Data Engineer Mgr - Platforms	Brampton, ON	2020-08-18T11:03:26.000Z	At Rogers, we connect Canadians to a world of possibilities and the memorable moments that matter most in their lives. Every day we wake up with one purpose in mind. To bring loved ones together from across the globe. To connect people to each other and the world around them. To help an entrepreneur realize their dream. A sports fan celebrate a special moment.

Because we believe connections unite us, possibilities fuels us, and moments define us.

Rogers is seeking a Database and Platform Administrator as part of the Enterprise Data and Analytics Team who will be operating in a senior role managing a team of Big Data, Cloud and other Database Administrators. The Database and Platform Administrator will serve as a key member of the Data Engineering team and will be responsible for the design, deployment, and operations of all of the database infrastructure in Development, Pre-Production and Production environments.

Responsibilities:
Responsible for Management of all database infrastructure including: transactional databases, data warehouses, and cloud infrastructure as well as data replication and extractions from these database infrastructures.
Design of database structures and replication, failover, backup, and extraction functions.
Product evaluation and selection for all database infrastructure requirements.
Creation of operations documentation and scripts for all database infrastructures.
Deployment and tuning of monitoring for critical database availability and performance thresholds and alarms.
Ensure database servers are backed up and meet the Enterprise Recovery Time Objectives
Tuning of all databases in Development, Pre-Production, and Production environments.
Create performance baseline metrics and monitor performance problems.
Application database setup in cloud environment. Setup/configuration of database backup/recovery, database monitoring, database performance tuning, Security, problem diagnosis
Database design and DDL deployments to cloud environment and working closely with application teams
Evaluating and testing new database features, documenting and presenting to team

What You Need to Have:
8+ years of experience as an Oracle DBA supporting prod/QA/Dev environment
Experience migrating the On-Prem Databases (Oracle/Big Data) to Cloud (Azure and AWS)
Experience of all key DBA roles in Cloud (backup, recovery, alert monitoring, performance monitoring, security administration, performance tuning, other database maintenance – statistics, re-indexing, etc.
Experience in automation and scheduling of database backups and other database maintenance
Knowledge of availability zones, HA, DR solutions in the Cloud will be nice to have
Experience with Cloud portals to create and manage databases and resources

Schedule: Full time
Shift: Day
Length of Contract: No Selection
Work Location: 8200 Dixie Road (101), Brampton, ON
Travel Requirements: Up to 25%
Posting Category/Function: Technology & Information Technology
Requisition ID: 201652

Together, we'll make more possible, and these six shared values guide and define our work:

Our people are at the heart of our success
Our customers come first. They inspire everything we do
We do what’s right, each and every day
We believe in the power of new ideas
We work as one team, with one vision
We give back to our communities and protect our environment

What makes us different makes us stronger. Rogers has a strong commitment to diversity and inclusion. Everyone who applies for a job will be considered. We recognize the business value in creating a workplace where each team member has the tools to reach their full potential. At Rogers, we value the insights and innovation that diverse teams bring to work. We work with our candidates with disabilities throughout the recruitment process to ensure that they have what they need to be at their best. Please reach out to our recruiters and hiring managers to begin a conversation about how we can ensure that you deliver your best work. You matter to us! For any questions, please visit the Rogers FAQ.

Posting Notes: Information Technology & Engineering	Rogers Communications	https://ca.indeed.com/rc/clk?jk=4326ff472bad16d1&fccid=b5583ca8df6a68c3&vjs=3	4326ff472bad16d1
2020081844	The Property Registry	e8f693d8ceb0d39b	https://ca.indeed.com/rc/clk?jk=e8f693d8ceb0d39b&fccid=f9ed656b63c620fc&vjs=3	Teranet is seeking an experienced Big Data Engineer to support its Data Analytics program. In this role, the successful candidate will be responsible for the ongoing monitoring and maintenance of Teranet’s data lake and the in-house developed and third-party software tools used to maintain it. In addition, the Big Data Engineer will work with consumers of the Teranet data lake to develop data views and data feeds according to their business needs and requirements. As part of this effort, the Big Data Engineer will be responsible for publishing data sets and views to the Teranet Data Analytics visualization platform so the data are accessible to downstream consumers. The Big Data Engineer will also collaborate with Teranet’s security team to ensure proper data access controls are in place, data is properly secured, and access activities are auditable.
Key responsibilities include:
Maintain and monitor Teranet’s data lake feeds
Manage, maintain and oversee Teranet’s data lake Big Data platform (Cloudera)
Prepare the data views for downstream data lake consumers (e.g. Data Scientists and Analysts)
Optimization of Teranet’s Big Data environment, applications and data views
Design, build and test data queries for data views and feeds for downstream data lake consumers
Design, build and integrate additional data sources into the Teranet data lake
Design, build and test analysis/data models to support downstream data lake consumers
You are someone who:
Continuously seeks to improve your knowledge of data analytics technologies and best practices
Strives to understand business drivers and strategy in order to understand business requirements
Takes a collaborative approach to assignments and works well with others
Is a good communicator with strong written and oral communication skills
Clearly documents your work so that it can be readily understood by others
Takes ownership and accountability for your assignments and responsibilities
Takes pride in delivering detail-oriented, thoughtful, thorough, and quality results
Key qualifications:
Bachelor’s degree in Computer, a quantitative field, or equivalent practical experience
3-4 years working with Hadoop related technologies (Spark, Hive, MapReduce, Sqoop, Impala)
Advanced software development experience with Scala, Java, SQL and Python
Familiarity with visualization and statistical modeling tools and languages (Tableau, R)	Big Data Engineer	Toronto, ON	2020-08-18T11:03:27.000Z
2020081845	Coursera	c475abcdee2df0df	2020-08-18T11:03:29.000Z	Senior Data Engineer	Toronto, ON	https://ca.indeed.com/rc/clk?jk=c475abcdee2df0df&fccid=ef63980be6005ec6&vjs=3	Coursera is a leading online learning platform for higher education, where 64 million learners from around the world come to learn skills of the future. More than 200 of the world’s top universities and industry educators partner with Coursera to offer courses, Specializations, certificates, and degree programs. 2,500 companies trust the company’s enterprise platform Coursera for Business to transform their talent. Coursera for Government equips government employees and citizens with in-demand skills to build a competitive workforce. Coursera for Campus empowers any university to offer high-quality, job-relevant online education to students, alumni, faculty, and staff. Coursera is backed by leading investors that include Kleiner Perkins, New Enterprise Associates, Learn Capital, and SEEK Group.

Data Engineering is unique at Coursera. Our team doesn’t simply build reports on demand. Rather, we build the semantic infrastructure and products that empower our internal and external customers with the data to innovate and perform their jobs better.
We’re looking for a senior data engineer in Toronto who can help us drive data engineering efforts for our platform. In this role, you will work with cross-functional teams to design, develop, and deploy data solutions. Our ideal candidate is an independent, analytically-minded individual with strong data modeling and engineering skills, who shares our passion for education.
Your responsibilities
Architect scalable data models and build efficient and reliable ETL pipelines to bring the data into our core data lake
Design, build, and launch visualization and self-serve analytics products that empower our internal and external customers with flexible insights
Be a technical leader for the team; guide technical and architectural designs for the major team initiatives; mentor junior members of the team
Build data expertise, and partner with data scientists, product managers and engineers to define and standardize business rules and maintain high-fidelity data
Define and partner with other engineers in the development of new tools to enable our customers to understand and access data more efficiently
Work cross-functionally (eg: product managers, engineers, business teams) to support new product and feature launches
Your skills
5+ years experience in a data-related field, including data engineering, data warehousing, business intelligence, data visualization, and/or data science
Strong data engineering skills and at least one scripting language (e.g., Python)
Proficient with relational databases and SQL
Familiarity and experience with big data technologies (eg: Hive, Spark, Presto) preferred
Ability to communicate technical concepts clearly and concisely
Independence and passion for innovation and learning new technologies


If this opportunity interests you, you might like these courses on Coursera -

Big Data Specialization

Big Data Essentials: HDFS, MapReduce and Spark RDD

Data Warehousing for Business Intelligence

Coursera is an Equal Employment Opportunity Employer and considers all qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, age, marital status, national origin, protected veteran status, disability, or any other legally protected class.

If you are an individual with a disability and require a reasonable accommodation to complete any part of the application process, please contact us at accommodations@coursera.org.

Please review our CCPA Applicant Notice here.
2020081846	Machine Learning/Data Engineer	893f6b0fd5e55acb	2020-08-18T11:03:29.000Z	Xanadu Quantum Technologies Inc.	Toronto, ON	https://ca.indeed.com/rc/clk?jk=893f6b0fd5e55acb&fccid=cf8d3d60750f6a59&vjs=3	Summary of Position and Responsibilities

As part of Xanadu’s Machine Learning team, the selected candidate will be responsible for working with a multidisciplinary team of machine learning experts and quantum algorithm developers to bring machine learning models into production. They will develop, deploy, and maintain code, models, and pipelines leveraging various cloud providers and services; automate model training, testing, deployment, and monitoring; and design solution architectures for data driven applications.

Prospective applicants must have strong technical, programming, and mathematical skills. They must possess the ability to evaluate established methods and tools, learn new ones quickly, and apply their knowledge to solve practical problems. Applicants should be self-motivated and demonstrate the ability to successfully meet objectives. Familiarity with quantum computing is not essential for this position, but is a definite plus.

Basic Qualifications and Experience

MSc in Machine Learning, Mathematics, Computer Science, Physics, Engineering, or a related field.
Experience building and deploying production-grade machine learning applications at scale.
Strong software engineering skills across multiple languages (Python, Scala, Java, C++, etc.)
Experience building and supporting development environments for Machine Learning/Data Science teams.
Experience with distributed computing frameworks like Spark, Dask, or Hadoop.

Preferred Qualifications and Experience

PhD in Machine Learning, Mathematics, Computer Science, Physics, Engineering, or a related field.
Solid mathematical understanding of machine learning, statistical modelling, probability theory, and linear algebra.
Experience with frontend and backend web application development.
Passionate about agile software processes, data-driven development, reliability, testing, and continuous delivery.
Familiarity with and experience working in a fast-growing technology start-up environment.

If you are interested in this opportunity, please submit a copy of your CV along with a cover letter outlining why you think this is the right role for you!

At Xanadu, we are committed to fostering an inclusive, safe, and equitable culture that meets the needs of all individuals. We actively support a barrier-free workplace and ensure team members feel included, valued, and heard. We are dedicated to being a fair and equitable employer, and that includes the representation of women in STEM. Should you require accommodations at any point during the recruitment process please contact Human Resources at hr@xanadu.ai (mailto: hr@xanadu.ai).
2020081847	Join a team of passionate thought leaders in a dynamic and collaborative environment! New Signature's Applied Innovation team is growing fast and we're looking for our next Data Visualization Developer to join us.


What impact will you have in this role?


Every role at New Signature is equally important in the grand scheme of things and everything we do is team work. Through team work, building relationships internally and at times with clients, understanding context at all times and what is important, and getting comfortable with influence and persuasion you will stand out as an expert in your field.

As we continue to scale we're looking for the market's best Azure Data & AI specialists to help us grow our business' fastest growing practice. You'll be working with the industry's biggest players, delivering innovative greenfield Data Platform builds, Data Integration programmes and implementing bespoke High-Level Data Architectural designs.


What type of experience do you need to be successful in this role?


Strong experience using the Microsoft Azure BI Stack, with a focus on front-end visualisation (PowerBI, ADFv2 , Azure SQL DB)
Azure SQL Datawarehouse, Azure Data Lake, Azure Databricks, Analysis Services highly desirable
Knowledge of C# essential
Agile methodology experience essential
CI/CD, Azure DevOps experience, highly desirable
Customer/client-facing consulting skills
Ability to learn quickly and adapt to changes in the Azure data technology landscape
Azure Data Engineer Associate certification desirable


What personality traits and other capabilities are important for this role?


Our consultants are self-motivated and pragmatic with strong problem-solving skills and a passion for crafting great solutions coming with a wealth of experience or talent that enables quality software delivery. They understand the best approach to architecture and development is through blending technologies and methodologies appropriate to the task at hand. The goal is to contribute to the clients' long lasting success to enable us to expand our business and clientele.


Security Responsibility:

All employees must act in accordance with New Signature's corporate security standards.

ABOUT NEW SIGNATURE

New Signature is a cloud-first, full-service Microsoft partner committed to delivering innovative technology solutions that solve human challenges. Behind every interaction is our dedication to provide outstanding experiences and to build authentic relationships with those around us. We are passionate about driving transformational results for clients across all company sizes, geographies and industries. The New Signature team delivers full lifecycle solutions—from project inception and planning, through deployment to ongoing support and maintenance.

New Signature was named the top Microsoft partner in the United States and the United Kingdom in 2014 and again in the United States in 2015—becoming the first partner ever to win the prestigious US Partner of the Year award two years in a row. With over 600 individual technology certifications, New Signature is a recognized expert at the forefront of Microsoft advancements and couples these powerful technologies with exceptional services to empower our customers, colleagues, and community.

OUR CORE VALUES

Our employees are driven by our values and know that they make a positive difference every time that they help a customer to solve their challenges. Our focus on delivering great customer experiences empowers our people to build rewarding relationships that contribute to our positive work environment. You can learn more about our culture here: New Signature Culture



Human

We use our hearts and minds to collaborate for success.

We harness technology to drive business, but we never let that replace our human connections. We use our hearts and minds to collaborate for success and instill confidence in our customers through relationships forged from trust.


Generous

We are giving and respectful.

With our efforts to always be generous, we elevate our service level with empathetic and considerate communications and actions. We always find a way to support our customers and colleagues by giving of our time and talent and equally respecting the time and talent of others.


Authentic

We tell it as it is, with positive intent.

Being authentic helps to nurture our strong and trusted relationships. We are honest, transparent, and reliable. When you partner with New Signature, you are partnering with a group of purposeful, outcome-driven and results-oriented professionals.


Innovative

We push the boundaries at the intersection of people, process and technology.

For us, there are no limit to our dreams. We continually innovate and push boundaries at the intersection of people, process, and technology to bring our customers and colleagues the best solutions first.


EQUAL EMPLOYMENT OPPORTUNITY

As a Global Cloud Transformation Consultancy business, New Signature understands diversity and inclusion in the workplace brings benefits to our customers, our business and most importantly, our people. We are committed to being an inclusive employer and we provide equal employment opportunities to all employees and applicants for employment.

New Signature prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other factors protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including all aspects of the recruiting and employment life-cycle at New Signature.


EMPLOYMENT ELIGIBILITY

New Signature requires the candidate to prove eligibility to work in the United States or Canada (depending on the location of the job) within three days of being employed. All final candidates will be asked to complete a background check in both the US and Canada. These record checks can include any or all of the following: education verification, employment verification, drug screening and criminal record check. Positions that require significant travel may also require a driving record check.

For US Applicants: New Signature is an E-Verify employer. E-Verify is a web-based system that allows enrolled employers to confirm the eligibility of their employees to work in the United States. E-Verify employers verify the identity and employment eligibility of newly hired employees by electronically matching information provided by employees on the Form I-9, Employment Eligibility Verification, against records available to the Social Security Administration (SSA) and the Department of Homeland Security (DHS). E-Verify is only used upon acceptance of a job offer and completion of the Form I-9.

Click here for the E-Verify Participation Poster (available in English and Spanish)

If you require this notice in another language provided by DHS, please contact us at 202.452.5923

Click here for more information on Your Right to Work or Visit the USCIS website here for more information on E-Verify

For Canadian Applicants: New Signature is committed to working with and providing reasonable accommodation to individuals with disabilities. In accordance with the Accessibility for Ontarians with Disabilities Act, 2005 and the Ontario Human Rights Code, New Signature will provide accommodations throughout the recruitment and selection process to applicants with disabilities. To request a reasonable accommodation, please call 416.971.4267. Please ensure to provide your name, the best way to contact you, a detailed description of the nature of any accommodation that you may require (including any materials or processes that can be used to ensure your equal participation).	2020-08-18T11:03:30.000Z	Data Visualization Developer - Toronto, Canada	https://ca.indeed.com/rc/clk?jk=cb99766691c3547f&fccid=f537209340bad710&vjs=3	New Signature	cb99766691c3547f	Toronto, ON
2020081848	Manulife	Are you looking for unlimited opportunities to develop and succeed? With work that challenges and makes a difference, within a flexible and supportive environment, we can help our customers achieve their dreams and aspirations.
Job Description
Are you looking for unlimited opportunities to develop and succeed? With work that challenges and makes a difference and a flexible and supportive environment, we can help our customers achieve their dreams and aspirations. We are currently seeking a Data Engineer to join one of our fast growing teams.
A day to day breakdown of the role would be as follows: 60% Hands On Development and Analysis, 20% Business partner interaction, 20% Agile Team Collaboration.
Additional responsibilities include:
Designs and implements data architectures in production environments
You will develop solutions that process data real-time from a variety of sources and make data available to multiple partners ranging from other IT applications to business teams to end customers.
You will work jointly with the Front-End team to ensure the definition of API endpoints is well understood and their requirements are supported
Translates business needs into data architecture solutions
Develops data landscape modernization architectures and roadmaps
Review and analyze the effectiveness and efficiency of existing systems and develop strategies for improving or further utilizing these systems.
Creates, reviews, updates and presents systems models, specifications, diagrams and charts to provide direction to system programmers and manages third party vendor (managed services) relationships.
Develops standards and processes for coding, deployment, testing, and governance.
You would be a good for this this role if:
You work jointly with the Front-End team to ensure the definition of API endpoints is well understood and their requirements are supported
You value collaborative development, this can be read in the quality and readability of your code and in your thorough code reviews
You are obsessed with accurate, reliable customer data
You think about QA and testability before you implement anything
You are mindful your work doesn’t stop when something works on your local computer: you work collaboratively with DevOps, think about migration, deployment, test coverage and documentation
You are promoting a culture of self-serve data analytics by minimizing technical barriers to data access and understanding
Qualifications for this role include:
BSc in Computer Science, Statistics, Informatics, Information System, Mathematics or equivalent quantitative field preferred
Well versed in software development methodologies, testing, release management, and maintenance. take pride in optimizing and cleaning code, and leave a program in a better shape than you found it
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL). A SQL veteran,
Good understanding of Master Data Management and IBM InfoSphere.
Good knowledge of DevOps and CI practices, ability to spec and setup the right environments and deployment procedures, proficiency with Docker and Jenkins.
Excellent troubleshooting skills. Experience performing root cause analysis on internal and external data and processes to answer specific business questions and find opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Understanding of relational and warehousing database technology working with at least one of the major databases platforms, preferably DB2, SQL Server
Practical experience with big data processing frameworks and techniques such as Azure, Hortonworks, Spark, Hive, PCF,
Solid working knowledge of data processing tools using SQL, Spark, Python or similar open source and commercial technologies
Knowledge of Java/Scala especially in relation to big data open source software preferred.
ETL experience is a requirement.
Agile project methodologies
Collaborative attitude, willingness to work with team members; able to coach, participate in code reviews, share skills and methods
Constantly learns from both success and failure
Good organizational and problem-solving abilities that enable you to manage through creative abrasion
Good verbal and written communication; effectively articulates technical vision, possibilities, and outcomes
Experiments with emerging technologies and understanding how they will impact what comes next.
What about Perks?
Manulife has lots of perks including, but not limited to:
Competitive compensation
Retirement Savings Accounts including a RPP (Pension Plan), RRSP (Retirement Savings Plan), and TFSA (Tax Free Savings account)
Manulife Share Ownership Program with employer matching
Customizable Benefits Package including Health, Dental, Vision, and 100% of Mental Health expenses
Financial support for ongoing training, learning, and education
Monthly Innovation Days (Hackathons)
Wearing jeans to work every day
An abundance of career paths and opportunities to advance
This is a full-time permanent role located in Toronto, Ontario.
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status

If you are ready to unleash your potential it’s time to start your career with Manulife/John Hancock.
About Manulife
Manulife Financial Corporation is a leading international financial services group that helps people make their decisions easier and lives better. With our global headquarters in Toronto, Canada, we operate as Manulife across our offices in Canada, Asia, and Europe, and primarily as John Hancock in the United States. We provide financial advice, insurance, and wealth and asset management solutions for individuals, groups and institutions. At the end of 2019, we had more than 35,000 employees, over 98,000 agents, and thousands of distribution partners, serving almost 30 million customers. As of March 31, 2020, we had $1.2 trillion (US$0.8 trillion) in assets under management and administration, and in the previous 12 months we made $30.4 billion in payments to our customers. Our principal operations are in Asia, Canada and the United States where we have served customers for more than 155 years. We trade as 'MFC' on the Toronto, New York, and the Philippine stock exchanges and under '945' in Hong Kong.
Manulife is an equal opportunity employer. We strive to attract, develop and retain a workforce that is as diverse as the customers we serve and to foster an inclusive work environment that embraces the strength of cultures and individuals. We are committed to fair recruitment, retention and advancement and we administer all of our practices and programs based on qualification and performance and without discrimination on any protected ground.
It is our priority to remove barriers to provide equal access to employment. A Human Resources representative will consult with applicants contacted to participate at any stage of the recruitment process who request any accommodation. Information received regarding the accommodation needs of applicants will be addressed confidentially.	2020-08-18T11:03:31.000Z	Data Engineer - Infosphere MDM	ad526aacc6864e5d	Toronto, ON	https://ca.indeed.com/rc/clk?jk=ad526aacc6864e5d&fccid=1747adf6142beb48&vjs=3
2020081849	About Paytm Labs:
 At Paytm Labs, we build technologies that powers Paytm India, the world's’ fastest growing mobile payments and commerce ecosystem. In addition to, the Paytm Canada app. We use our skills and our biggest asset – data, to make our dent in this universe.

We are committed to offering the most transparent, secure, and personalized consumer experience to over 230 million users. We believe that this kind of scale, and the unique problems that it presents attracts curious candidates like yourself.

Job Description:
If working with billions of events, petabytes of data and optimizing for last millisecond is something that excites you then read on! We are looking for Data Engineers who have seen their fair share of messy data sets and have been able to structure them for building useful AI products.

You will be working on writing frameworks building for real time and batch pipelines to ingest and transform events(108 scale) from 100’s of applications every day. Our ML and Software engineers consume these for building data products like personalization and fraud detection. You will also help optimize the feature pipelines for fast execution and work with software engineers to build event driven microservices.

You will get to put cutting edge tech in production and freedom to experiment with new frameworks, try new ways to optimize and resources to build next big thing in fintech using data!
Requirements:
You have previously worked on building serious data pipelines ingesting and transforming > 10 ^6 events per minute and terabytes of data per day.
You are passionate about producing clean, maintainable and testable code part of real-time data pipeline.
You understand how microservices work and are familiar with concepts of data modelling.
You can connect different services and processes together even if you have not worked with them before and follow the flow of data through various pipelines to debug data issues.
You have worked with Spark and Kafka before and have experimented or heard about Flink/Druid/Ignite/Presto/Athena and understand when to use one over the other.
On a bad day maintaining zookeeper and bringing up cluster doesn’t bother you.
You may not be a networking expert but you understand issues with ingesting data from applications in multiple data centres across geographies, on-premise and cloud and will find a way to solve them.
Proficient in Java/Scala/Python/Spark
What we Offer!
We are proud to announce that we have been certified as a Great Place to Work!
A collaborative, open work environment that fosters ownership, creativity, and urgency
Enrolment in the Group Health Benefits plan right from Day 1, no waiting period
Fuel for the day: Weekly delivery of groceries and all types of snacks to our office
All types of signature drinks from coffee to lattes to cappuccinos
Catered lunch and desserts on a monthly basis!
Ping Pong and Pool: Become the next Paytm Labs Table Tennis/ Pool champ!
And so much more!
Don't have Paytm Canada App yet?
Check us out in the Google Play or App Store.

We thank all applicants, however, only those selected for an interview will be contacted.

Paytm Labs is committed to meeting the accessibility needs of all individuals in accordance with the Accessibility for Ontarians with Disabilities Act (AODA) and the Ontario Human Rights Code (OHRC). Should you require accommodations during the recruitment and selection process, please let us know. Paytm Labs is an equal opportunity employer.	Paytm	https://ca.indeed.com/rc/clk?jk=3cc1d76cb36c1303&fccid=4e917d9a3b14765d&vjs=3	2020-08-18T11:03:32.000Z	Toronto, ON	Data Engineer	3cc1d76cb36c1303
2020081850	The Intermediate Data Engineer works in one of the agile BI teams.
The ideal candidate will be an experienced Data Engineer that demonstrates in-depth knowledge and understanding of data warehousing, data integration, reporting and business intelligence. Open-minded and flexible and prepared to work in a very dynamic environment, supporting multiple business units.
JOB RESPONSIBILITIES:
 Creating, supporting, and maintaining ongoing operational, managerial, and executive business intelligence infrastructure.
 Attention to detail, in particular as it relates to compliance and accuracy of data.
 Developing understanding of information sources and correct interpretation of data
 Gathering, documenting and analyzing requirements from stakeholders
 Meeting and interacting with all levels of management as needed to elicit, define, analyze and document requirements for new business intelligence initiatives.
 Designing the conceptual, logical and physical data models necessary to support new reporting and data analysis
 Developing data integration processes
QUALIFICATION:
 Minimum 3 years of related experience.
 Understanding of Data Warehouse lifecycle is a must.
 Good knowledge in cloud technologies (preferably GCP)
 Advanced knowledge in Python scripting language
 Good knowledge in Message Broker systems (Kafka, RabbitMQ, PubSub)
 Excellent proficiency in writing SQL queries.
 Advanced proficiency with Microsoft BI Suite - SQL Server 2014-2019, SSIS, SSRS.
 Understanding relational and dimensional data modeling concepts.
 Strong knowledge and comprehension of technology and data management used in the process of collecting, storing and retrieving data.
 Post-secondary education, preferably in Math/Statistics or Computer Science.
 Superior writing, editing, and communication skills, capacity to interact with all levels of the organization.
 Knowledge of latest Microsoft self-service BI tools – Power BI (both desktop and cloud) an asset.
 Experience with DAX an asset
 Experience and/or personal interest in the financial industry an asset.	Intermediate Data Engineer	https://ca.indeed.com/rc/clk?jk=25bfaface01113b7&fccid=dd16cd5f7dbbced2&vjs=3	25bfaface01113b7	2020-08-18T11:03:32.000Z	Toronto, ON	QUESTRADE INC
2020081851	https://ca.indeed.com/rc/clk?jk=63c6f469ca57386c&fccid=cb94cfaf4856cc5b&vjs=3	2020-08-18T11:03:33.000Z	Senior Data Engineer	63c6f469ca57386c	Toronto, ON	BenchSci exponentially increases the speed and quality of life-saving research by empowering scientists with the world’s most advanced biomedical artificial intelligence to run more successful experiments. Backed by F-Prime and Google’s AI fund, Gradient Ventures, BenchSci uses machine learning to diagnose pharmaceutical R&D health from hidden patterns in procurement data. A turnkey application of AI with immediate, quantifiable impact, BenchSci now optimizes reagent procurement and experimental success in 15 of the top 20 pharmaceutical companies and over 4,300 leading academic centers globally.

We are currently seeking a Senior Data Engineer to join our Data Team. As part of the job, you will work on evolving our data models in several styles of datastores, improve internal tooling to allow data self-service, and operationalize production-grade data pipelines.
What you’ll do:
Scale data pipelines to allow data to go from research to platform as fast as possible
Develop data access mechanisms for downstream applications consumption
Manage sources which contain both semi-structured as well as unstructured data
Develop and apply suitable frameworks to detect data drift, and then calibrate and redeploy them to production seamlessly
Collaborate closely with other engineers to solve interesting and challenging data problems
Who we’re looking for:
5+ years working as a professional developer
Expertise in Python
Expertise with SQL
Expertise in Spark 2.x, Dataset/DataFrame API and performance tuning
Experience with cloud reference architectures and developing specialized stacks on cloud services
Experience with Pandas
You have strong cross-team communication and collaboration skills
A team player who strives to see teammates succeed together
Bonus points for:
Background in Life Science
Experience with Airflow or other workflow management systems in a distributed setup
Experience with graph data modelling and scaling graph databases
Experience with Kubernetes in production
Experience with technical design and applying architectural patterns

Here at BenchSci, these are our core values:

Focused: We focus on what will drive the greatest impact at all times.
Advancement: We believe in continuous growth, and discovering new ways to do things better. This applies to our product and business, but also to ourselves.
Speed: We recognize that without a sense of urgency, our team, our product and our mission lose their value.
Tenacity: What we’re trying to do isn’t easy, but we hire the best people, and give them the autonomy, tools, and resources to succeed. The hard work is up to them.
Transparency: We believe that sharing diverse ideas and information creates strong teams. Our success stems from research, collaboration, feedback, and trust.

BenchSci is an equal opportunity employer. We value diversity and are committed to fostering an inclusive environment. All four of our cofounders are immigrants to Canada, as are many of our employees. We welcome your fresh perspectives and ideas.	BenchSci
2020081852	https://ca.indeed.com/rc/clk?jk=750dbce57a60be39&fccid=3d38508546395f15&vjs=3	750dbce57a60be39	SOTI is committed to providing its employees with endless possibilities; learning new things, working with the latest technologies and making a difference in the world.
Job Title:
Senior Data Engineer
Location:
Mississauga
Who We Are
At SOTI, we are committed to delivering best in class mobile and IoT device management solutions. We are looking for out of the box thinkers that appreciate the art of creating great software. To us, being visionary is more important than doing things the way they’ve always been done.
What’s in it for you?
The People - From our humble origins in our founder’s basement, to our industry leading position today, SOTI has worked hard to foster a company culture that we can all believe in. A culture that emphasizes personal growth, continuous innovation and fun.
The Growth - Our environment fosters new ideas, fresh perspectives, and the ability to take them over the goal line. SOTI is a fast-paced environment with a global reach that encourages you to make your mark and be part of something big!
The Technology - You’ll get the chance to work with leading edge technologies and take on complex and interesting projects, as part of highly collaborative and agile teams. You will work alongside SOTI’s partners which include leading tech giants that will keep you on the cusp of emerging technologies.
What You’ll Do
Ability to translate and document business requirements into technical documentation, supporting document management and knowledge sharing
Ensure assigned deliverables are within business / audit control requirements
Take ownership of end to end design and all aspects related to development and ensure design and development standards and followed
Create project documentation (Detailed design, Source-to-target mappings, Implementation plans, etc.)
Develop data and database-oriented solutions in order to solve complex business problems leading to data driven decision making
Develop data integration processes to integrate disparate data sets into a cohesive data model in support of BI and analytical requirements
Works closely with data architecture to ensure proper adherence to architectural guidelines and principles
Experience You’ll Bring:
7+ years of hands-on advanced experience designing and developing BI Solutions and providing technical expertise
7+ years hands-on advanced experience using MS SQL
7+ years of experience with MS SQL Business Intelligence Stack (SSAS, SSIS, and SSRS)
5+ years hands-on advanced experience using Power BI or similar BI platforms
Experience using Cloud architecture, NoSQL databases and R/Python
Experience using building data pipelines to integrate with unstructured data sources
Experience in designing and building unstructured data stores using Azure or AWS technologies
Experience with Data Warehouse concepts, including the use of Extract, Transform, and Load (ETL) tools
Excellent analytical, troubleshooting, problem-solving and research skills
Must be able to multitask and have experience with interacting within a diverse user/customer base
Excellent written, verbal, and interpersonal communication skills
About SOTI
SOTI is the world's most trusted provider of mobile and IoT management solutions, with more than 17,000 enterprise customers and millions of devices managed worldwide. SOTI's innovative portfolio of solutions and services provide the tools organizations need to truly mobilize their operations and optimize their mobility investments. SOTI extends secure mobility management to provide a total, flexible solution for comprehensive management and security of all mobile devices and connected peripherals deployed in an organization.
At SOTI, we celebrate the uniqueness of our global teams and are proud to be an equal opportunity workplace. We are curious problem solvers who are committed to bringing the best mobile and IoT management solutions to market. We offer careers with #EndlessPossibilities.
What are you waiting for? Apply today: https://www.soti.net/careers
If you want to bring your ideas to life, apply at SOTI today.
We are committed to providing accessible employment practices that are in compliance with the requirements under the Human Rights Code and the Accessibility for Ontarians with Disabilities Act (AODA). If you require accommodation during any stage of the recruitment process, please notify People & Culture at careers@soti.net .
Please note that SOTI does not accept unsolicited resumes from recruiters or employment agencies. In the absence of a signed Services Agreement with agency/recruiter, SOTI will not consider or agree to payment of any referral compensation or recruiter fee.	2020-08-18T11:03:34.000Z	Mississauga, ON	Sr. Data Engineer	SOTI Inc.
2020081853	https://ca.indeed.com/rc/clk?jk=ad8d075517b97d64&fccid=2c209e080b2e73f5&vjs=3	ad8d075517b97d64	DATA ENGINEER (SOPHI)	DATA ENGINEER (SOPHI)

POSITION CODE: 2020-063
LOCATION: The Globe and Mail, Toronto
SALARY: Commensurate with qualifications and experience

POSITION OVERVIEW:

We’re looking for experienced individuals with deep knowledge of data streaming, serialization, databases and distributed systems, and proficient in writing custom libraries but also know when to use off-the-shelf solutions when necessary. Ideal candidates are self-motivated engineers with a passion for both business and technology innovation, more importantly they quickly adapt with changing technologies. We value people who are passionate about system design and have an eye for improving product quality. We currently work with Scala, Kotlin, Java, Python, NodeJS, Postgres, Go, Kafka, and Flink.

As a Data Engineer you will:
Develop and optimize system components for maximum performance and scalability across a vast array of environments
Have a commitment to collaborative problem solving, sophisticated design, and product quality
Ensure that system components and the overall application are robust and easy to maintain
Contribute to backlog reviews, technical solutions design and implementations
Be disciplined in implementing software in a timely manner while ensuring product quality isn’t compromised
MINIMUM QUALIFICATIONS:
Strong analysis and problem solving skills
Deep understanding of good programming practices, design patterns, Functional Programming, and Object Oriented Analysis and Design
Successfully implemented and released a large number of data pipelines and web services using modern engineering frameworks in the past 3 years
Formal training in software engineering, computer science or computer engineering.
Worked as part of a mature engineering team
IDEAL CANDIDATE:
Have strong working knowledge with Scala and/or Kotlin.
Understands reactive programming, Threads and Futures.
Successfully implemented realtime and batch analytics using Kafka, Flink, Apache Beams and/or Google DataFlow.
Strong working knowledge of data warehouses include Redshift, Snowflake, and/or Apache Druid.
Have a working knowledge with containerization and build pipelines
Successfully implemented data systems for very large data volumes such as click streams and/or IoT sensors data.


THE GLOBE AND MAIL INC. IS DEDICATED TO EQUITY IN THE WORKPLACE
At The Globe and Mail, we are committed to fostering an inclusive, accessible work environment, where all employees feel valued, respected and supported. The Globe and Mail offers accommodation for applicants with disabilities as part of its recruitment process. If you are contacted to arrange for an interview, please advise us if you require an accommodation.	The Globe and Mail	2020-08-18T11:03:34.000Z	Toronto, ON
2020081854	https://ca.indeed.com/rc/clk?jk=00df245dd038b055&fccid=87852d99a20def3f&vjs=3	Fiix	Why Fiix?
Fiix has a big goal – to create a more sustainable world. Sounds lofty right? Our mission is to make every maintenance team successful by enabling the adoption of a CMMS and we’re off to a great start. Teams that are part of the world’s most well known brands (like Toyota, Siemens, and Sara Lee) manage their maintenance activities and achieve greater results with Fiix. But we’re not stopping there. Our team is growing by leaps and bounds and we’re conquering new challenges every day. We’re looking for big thinkers with small egos to join us on our journey to create a more sustainable world.

Why we do it?
We’re a team of market disrupting, like-minded individuals. We all do things our own way, but we come together each and every day to create and deliver the long awaited answer to an antiquated industry – and we have a lot of fun while we’re at it.

We’re looking for a Data Engineer to help take Fiix’s explosive growth to a whole new level. We are looking for an experienced Data Engineer to build our data layer to support the delivery of machine learning-driven products. This is a unique opportunity to join a team of creative and passionate individuals committed to bringing AI to the predictive maintenance world.

What You Will Do:
Work with software engineering to design, build, maintain and optimize our data management and analytics pipeline
Perform data analysis, quality assessments, cleaning, imputation and data aggregation tasks
Perform feature engineering and selection to support machine learning activities
Build data processing pipelines and automate data pipelines in production environments
Work with engineers and data scientists to deploy analytics capabilities and machine learning models in production
Work with engineering and data teams to ingest and structure high throughput IoT data
Develop processes and frameworks to ensure data and model quality
Perform code reviews and testing to ensure software quality is high and requirements are met
Diagnose and repair data issues and assist customers with technical problems
What We're Looking for:
3+ years experience in a high growth software development environment developing data-driven products
Experience working on ETL, data warehousing, data modeling, data architecting, data analysis
Experience with at least some of: 1) Data streaming with Kinesis, Kafka or similar 2) ETL orchestration frameworks such as Airflow, Luigi or similar 3) Data warehouses such as Snowflake or similar
Development skills in Python, MySQL and other relational databases, NoSQL databases such as DynamoDB, Redis or similar
Experience with AWS or other cloud providerEducation background:
Bachelor’s Degree or higher in Computer Science or a related field
Equity Statement

At Fiix, we recognize that people come with a wealth of experience and talent beyond just the technical requirements of a job. If your experience is close to what you see listed here, please still consider applying. Diversity of experience and skills combined with passion is a key to innovation and excellence. Therefore, we encourage people from all backgrounds to apply to our positions. Please let us know if you require accommodations during the interview process.	Data Engineer - Applied AI	Toronto, ON	2020-08-18T11:03:35.000Z	00df245dd038b055
2020081855	https://ca.indeed.com/rc/clk?jk=24716d17b181ff38&fccid=28f79c18789111e8&vjs=3	24716d17b181ff38	Intermediate Data Engineer	Company Description

The Company
Hitachi Solutions is an impact-driven global leader in consulting dedicated to delivering competitive, end-to-end solutions based on the Microsoft Cloud. Our deeply connected teams are unified by our values and our commitment to helping clients succeed and compete with the largest global enterprises. We are a division of the 38th largest company in the world and carry the strength of a vast network of interconnected Hitachi companies all while remaining nimble, agile, and ready to pivot at a moment’s notice.
Hitachi Solutions started as three founding partners and transformed into nearly 2,000 consultants, developers, and support personnel all around the globe. With 36 Microsoft Partner of the Year awards, Hitachi Solutions has established itself as a leading partner in the ever-growing landscape of technology consulting.
The Culture
Our team is a mix of curious, fun, and get it done. We celebrate collaborative thought leadership and individual talents so that our ideas are translated into real-world results.
Each day, our team members show up to work as a blank slate with the sole purpose of coloring their canvases with knowledge. We are a complex group of resilient learners and encourage one another to go beyond the limits of conventional expectations. As a member of the dynamic Hitachi Solutions team, you will be challenged, pushed, and unconditionally supported in your efforts to drive the business forward.
** THIS ROLE IS OPEN TO ANY CANDIDATES FROM WITHIN CANADA**

Job Description

As an Analytics Data Engineer for Hitachi’s Azure Cloud Enablement Team you will be responsible to deliver high quality modern data solutions while being part of a dynamic and fast-growing team consisting of endless opportunities.
The successful candidate will be a self-motivated, passionate individual who strives to be the best at what he/she does and creates a trail of ecstatic Customers for life. This person will love to learn, contribute to the team and wants to be part of something great.
Knowledge and Experience
Hands-on experience with the Azure Data Platform (Data Factory, Data Lake, Data Warehouse, Blob Storage, SQL DB, Analysis Services)
Data quality (profiling, cleansing, enriching)
Data Modeling – including design from conceptual to logical to physical data models
Considered to be an expert in T-SQL
Hands-on experience with MPP database technologies such as Azure SQL DW, Teradata Netezza, etc.
Experience with multiple components listed, required:
Power BI including DAX
Database migration from legacy systems to new solutions
DevOps
Interpreted languages (i.e. python, C-sharp, Java, Scala, etc.)
Databricks
LogicApps
PowerApps
HDInsight
D365FO / CE experience as it pertains to data extraction
Knowledge of Cap Theorum and Distributed Database Management Systems, iis considered an asset.
Opportunity for a career path into a Data Scientist role if desired

Qualifications

Required skills / qualifications
Proven ability to engage customers to understand customer challenges and needs to develop technical solutions
3+ years of hands on experience working with the Azure Platform and its relevant components
Proven experience of architecting Azure services into a solution platform on Microsoft Azure for Analytics
Minimum 5 years hands on development experience with ELT/ETL using MS SQL Server, Oracle or similar RDBMS Platform
Familiarity with data visualization tools (e.g. PowerBI, Tableau etc.)
Experience or desire to coach, mentor and provide leadership to team members
Post-secondary degree/diploma in Business, Computer Science or a related discipline;
Strong communication skills, both written and verbal
Prepared for domestic and US travel as required
Preferred considered an asset, NOT required:
Project management experience
Databricks and Spark SQL
Previous Consulting experience
Additional Information

Opportunity Benefits:
Medical and Dental Benefit Package (including Long Term and Short Term Disability)Base salary plus targeted bonus package
This position can be based anywhere in Canada, though travel might be required.	2020-08-18T11:03:36.000Z	Toronto, ON	Hitachi Solutions
2020081856	Senior Data Engineer

Nomis is looking for an outstanding data expert to join our team. The Data Engineer will collaborate closely with our client services team to process critical data while working to power advanced analytics and enable the integration of data science across the company. You are ready to be flexible and nimble in your work, from constructing ETL pipelines for customer delivery to participating in exploratory data analysis with our Analytics team.

Who We Are & What We Build


We partner with Banks and FinTechs on their journey to best-in-class pricing technology and analytics so that they deliver more value to their customers, employees and shareholders. Our top-notch people, proven technology, and innovative analytics are tackling big data challenges at banks and lenders every day. We deliver market-leading cloud-based Pricing & Profitability Management solutions and insights for the Banking & Financial Services industry leveraging cutting-edge behavioral data science. We are a Blue Chip venture-backed company with the vision to transform the consumer banking landscape.

Responsibilities

Establish and maintain big data processing platform
Build data management applications and microservices on AWS
Design and implement Hive/Greenplum/RedShift distributed data warehouses and standard schemas
Design, develop, maintain cross-platform ETL processes and MapReduce/Hive/Spark data processing workflows
Manage and maintain reference data securely on S3 and other storage systems
Support client services teams by
Manage, customize, and automate cloud-based (AWS) data processing supporting multiple clients
Administration of relational databases, capacity plans, infrastructure and storage design
Oversee and execute data migration from existing data stores
Application/implementation of custom analytics applications and datasets
Develop code standards, guidelines, and automated test suites to ensure highest data quality and integrity

Desired Skills and Requirement

Experience with building distributed systems, query processing, and the Hadoop ecosystem
Understanding of Data warehousing - architect and design data warehouse
Expertise with data schema - logical and physical data modeling
Knowledge of ETL processes and tools
Experience with AWS or a major cloud platform such as GCP
Proficiency in: Python, SQL, Java

Strong pluses:

Experience of Business Intelligence tooling such as Tableau
Experience with data mining techniques and analytics functions
Predictive analytics experience is a PLUS
Experience with Spark 2, Apache Airflow and other modern data engineering tooling a strong plus
Experience with streaming architectures and MPP databases such as Greenplum a strong plus
Up-to-date with the open-source community w.r.t. data engineering
Experience with the following services in AWS a strong plus: EMR, Lambda, Kinesis, Firehose, S3	93a28de264933288	Nomis Solutions	Senior Data Engineer	https://ca.indeed.com/rc/clk?jk=93a28de264933288&fccid=13785db0e9e260f0&vjs=3	2020-08-18T11:03:36.000Z	Toronto, ON
2020081857	https://ca.indeed.com/rc/clk?jk=9c4363e25aeb557b&fccid=b7254da6203c84b3&vjs=3	Perpetua's mission is to give superpowers to anyone that sells online. At the moment, we help media agencies, brands, and Amazon sellers win on Amazon by analyzing large amounts of data and using AI to develop smart optimization algorithms that drive transformational sales growth.

As a Senior Data Engineer, you will be responsible for maintaining and evolving our data infrastructure to support the massive scale of data that we process to power our customers' ad campaigns. This includes advancing our data capabilities to both cut costs and increase performance, while maintaining data integrity and consistency. You will build scalable infrastructure and data pipelines to deliver a platform unparalleled in the advertising space using Google BigQuery, Looker, Airflow and Python (Django, Pandas, SciPy Stack).
What You'll Do
Interact with internal stakeholders to figure out how to make it easier for them to leverage Petabyte-scale data
Accelerate data-informed decision making to transform our product & engineering strategy
Be responsible for developing, maintaining and evolving the Data Platform
Considering technical tradeoffs and advancing our data capabilities
Being responsible for data consistency and integrity both for our internal tools and client APIs
Owning data accessibility and discovery across all parts of the company
Increasing performance of our data pipeline
Evolving our use of available tools and optimising current tools to optimise infrastructure costs
Work in a fast-paced, scaling start-up building software that is truly impactful
What You'll Have
Experience with workflow management tools like Airflow for moving and transforming data
Experience with data mapping, schema design, data structures and algorithms, data quality and integrity
You'll deeply know your datawarehouse from your datalake from your transactional database
Experience designing infrastructure to facilitate both availability and approachability of data to all users (internal and external)
Ability to source requirements from stakeholders and develop a vision for a data-driven company
Ability to build and performance tune complex database queries
Ability to solve problems in new and innovative ways
Self starter who takes initiative; owning a project from start to finish
The ability to communicate complex technical issues in a clear and concise manner
Flexibility to adjust to changing priorities, requirements, and schedules
Experience working in a fast-paced, agile environment
Company Benefits
Impactful work that will help lay the foundation for future projects
Meaningful equity at an early stage company
Ground floor opportunity
Paid-for meals
Unlimited snacks and drinks
Full benefits plus a health spending account
Top of the line technology to help you build your own workspace
Flexible time off policy
At Perpetua, we are dedicated to pursuing and hiring a diverse workforce with varied experiences, perspectives and opinions. We believe diversity helps our team perform better and enables us to build an outstanding product for our customers. We are an equal opportunity employer and are committed to work with applicants requesting accommodation at any stage of the hiring process.	9c4363e25aeb557b	Senior Data Engineer	Perpetua Labs	Toronto, ON	2020-08-18T11:03:37.000Z
2020081858	About Paytm Labs:
At Paytm Labs, we build technologies that powers Paytm India, the world's fastest growing mobile payments and commerce ecosystem. We use our skills and our biggest asset – data, to make our dent in this universe. We are committed to offering the most transparent, secure, and personalized consumer experience to over 400 million users. We believe that this kind of scale, and the unique problems that it presents attracts curious candidates like yourself.

Job Description:
We are seeking a Data Engineer with a focus on BI, for our personalization and marketing automation team. This team has an impact on every pixel in our mobile app and drives customer engagement and growth by showing relevant products, services and messaging to our customers. Candidates should have technical skills to build real-time and batch data pipelines, and strong analytical capabilities to delve into and understand trends and patterns in the data. You will be working closely with world class software engineers, machine learning engineers and business teams, and the work you do will have a direct impact on the roadmap of our product.
Responsibilities:
Build and maintain batch and real-time data pipelines to power our analytical reports and dashboards
Analysis of historical data to identify trends and support decision making, including written and verbal presentation of results and recommendations
Set up A/B tests and ensure metrics are reported accurately
Identifying data needs and driving data quality improvement projects
Evangelizing data driven decision making within the team and to business & product owners
Understanding the broad range of Paytm data resources, and knowing the right ones to use for the problems at hand
Technial Requirements:
Proficient in at least one programming language like Scala, Java or Python
Experienced in Apache Spark, or other big data processing frameworks
Proficient in SQL
Proficient in software development, and source code management (Git etc)
Qualifications:
BS degree or higher in computer science engineering, statistics, mathematics, econometrics, or a similar quantitative field
3+ years work experience in data engineering, software engineering and/or data analysis
Prior success in working with extremely large datasets using big data technologies
Demonstrated ability to directly partner with business owners to understand product requirements
Effective spoken and written communication to senior audiences, including strong data presentation and visualization skills
Detail-oriented, with an aptitude for solving unstructured problems
Nice to Haves:
Familiarity with AWS services
Experience with data streaming technologies like Kafka, Spark Streaming
What we Offer!
We are proud to announce that we have been certified as a Great Place to Work!
A collaborative, open work environment that fosters ownership, creativity, and urgency
Enrolment in the Group Health Benefits plan right from Day 1, no waiting period
Fuel for the day: Weekly delivery of groceries and all types of snacks to our office
All types of signature drinks from coffee to lattes to cappuccinos
Catered lunch and desserts on a monthly basis!
Ping Pong and Pool: Become the next Paytm Labs Table Tennis/ Pool champ!
And so much more!
Notice for Job Applicants

Following the advice of Canadian health authorities, to mitigate the risk of potential spread of COVID-19 and support social distancing, all recruiting activities including interviews and new hire onboarding will be conducted remotely. While we are doing our best to ensure reasonable response times, please expect potential delays during the recruiting process due to the current situation. Thank you for your patience and understanding during these challenging times.

Don't have Paytm Canada App yet?
Check us out in the Google Play or App Store.

We thank all applicants, however, only those selected for an interview will be contacted.

Paytm Labs is committed to meeting the accessibility needs of all individuals in accordance with the Accessibility for Ontarians with Disabilities Act (AODA) and the Ontario Human Rights Code (OHRC). Should you require accommodations during the recruitment and selection process, please let us know. Paytm Labs is an equal opportunity employer.	2020-08-18T11:03:38.000Z	Paytm	25dbbd3fbbdb8ba2	Toronto, ON	https://ca.indeed.com/rc/clk?jk=25dbbd3fbbdb8ba2&fccid=4e917d9a3b14765d&vjs=3	Data Engineer - BI & Data Science
2020081859	https://ca.indeed.com/rc/clk?jk=71cd34c1d32326c5&fccid=1747adf6142beb48&vjs=3	2020-08-18T11:03:38.000Z	Manulife	Senior Data Engineer	71cd34c1d32326c5	Toronto, ON	Are you looking for unlimited opportunities to develop and succeed? With work that challenges and makes a difference, within a flexible and supportive environment, we can help our customers achieve their dreams and aspirations.
Job Description
Manulife’s Global Data Office (GDO) is seeking a Sr. Data Engineer reporting into the Director, Advanced Analytics and AI Engineering & Enablement Lead. Located in Toronto, Canada - the role will champion and support strategic and global data initiatives that strengthens Manulife’s global data and advanced analytic capabilities, foster cross-segment collaboration and communication helping build an agile data insight driven culture, and lead and nurture open data design and architecture establishing conditions for successful technical and analytic innovation. The Sr. Data Engineer will develop, maintain, and test: data pipelines, application framework, infrastructure for data generation and work closely with Data Scientists to enable their work using modern data architecture and tools.
Job Description
Manulife has a clear vision for a Global Data Strategy. By liberating and strengthening Manulife’s data capabilities we will enable deeper insights, better product and service design, and more effective business processes. The result will be exceptional experiences for our customers.

Key Responsibilities:
Leveraging new & existing Big Data & Cloud technologies contributing to the innovative design, development and management of data analytics labs supporting to increase knowledge and insight from enterprise data
Perform technical systems and data flow development in a variety of projects for complex front, middle and back office applications, with a focus on the reporting and analytics environment; this environment is built on a Microsoft Azure cloud and is underpinned by a Hortonworks Hadoop stack
Perform technical systems and data flow design for small-to-medium sized projects
Work with multiple project execution and deployment teams (e.g. Development, Business Analysis, Architecture, Release Management, Production Support)
Work closely with the technical leads and architecture teams, and align solutions that meet the departmental architectural vision
Assess the completeness and accuracy of data, identifying gaps in data, provide feedback to business and system owners with guidance and options to obtain missing information
Design, build and implement modern data architectures in development and production environments (data orchestration pipelines, data sourcing, cleansing, augmentation and quality control processes)
Translates business needs into data engineering and architecture solutions
Contributes to overall solution, integration and enterprise architectures
Build and support deploying machine learning models in development and production environments
Provide proactive data ingestion and analysis of large structured and unstructured datasets involving a wide range of systems across Group Functions (i.e., Finance, Treasury, Risk, Human Resources, Brand & Communications)
Evaluating existing and proposed data models and how to best access and query them as well as existing and proposed data interfaces and how to clearly document them, including specification of data flow models, data flow timing, data mapping, and data transformation rules including data validations and controls

Education, Experience & Skills:
Demonstrated 2-5 years of professional experience in related industry experience in working in big data/data management & understanding big data analytic tooling and environments including a University degree and or Master’s degree in Engineering, Computer Science or equivalent quantitative program
Experience in Big Data, Analytics and Business Intelligence technologies to support design, build and implementation for advanced analytics and business intelligence reporting;
Experience working with Cloudera and/or Hortonworks Hadoop stack
Experience with big data processing frameworks and techniques such as HDFS, MapReduce, Syncsort, Sqoop, Oozie, Storage formats (Avro, Parquet), Stream processing (NiFi, Kafka), etc.
Understanding of relational and warehousing database technology working with Hadoop and other major databases platforms (e.g., Hadoop, Oracle, SQLServer, Teradata, MySQL, or Postgres)
Experience in data technologies and use of data to support software development, advanced analytics and reporting. Focus on Cloud (Azure), Hadoop-based technologies and programming or scripting languages like Java, Scala, Linux, C++, PHP, Ruby Python, R and SAS.
Knowledge regarding different databases such as Hawq/HDB, MongoDB, Cassandra or Hbase.
Working knowledge of modern data streaming using Kafka, Apache Spark and data ingestion frameworks: NiFi, Hive and Pig
Experience writing complex SQL and NoSQL jobs to analyze data in both traditional DBMS (MS-SQL, Oracle) and Big Data environments (i.e., HADOOP, SPARK, or similar open source and commercial technologies)
Knowledge of non-relational (Cassandra, MongoDB) databases preferred
Predictive analytics and machine learning experience (scikit-learn, Tensorflow, MLlib, recommendation systems) preferred
Experience with integrating to back-end/legacy environments
Experience integrating business and technology teams
Knowledge and familiarity with machine learning models application and production pipelines
Collaborative attitude, willingness to work with team members; able to coach, participate in code reviews, share skills and methods
Remains current with emerging technologies, innovations and practices within the data and analytics industry
Good organizational and problem-solving abilities that enable you to manage through creative abrasion
Good verbal and written communication; effectively articulates technical vision, possibilities, and outcomes
Strong work ethic, results oriented, and accuracy / attention to detail are critical; ability to work in agile or scrum delivery environments
Exceptional oral, written and interpersonal communication skills with the ability to simplify complex technical concepts into business & value-focused language. A key requirement is to communicate clearly and consistently keeping stakeholders well-informed of progress and challenges
Excellent organizational and time management skills, strong business presence with ability to multi-task and effectively deal with competing priorities. Ability to work with minimal or no supervision while performing duties; has the ability and initiative to organize various functions and be a strong team player.
What about Perks?
Manulife has lots of perks including, but not limited to:
Competitive compensation
Retirement Savings Accounts including a RPP (Pension Plan), RRSP (Retirement Savings Plan), and TFSA (Tax Free Savings account)
Manulife Share Ownership Program with employer matching
Customizable Benefits Package including Health, Dental, Vision, and 100% of Mental Health expenses
Financial support for ongoing training, learning, and education
Monthly Innovation Days (Hackathons)
Wearing jeans to work every day
An abundance of career paths and opportunities to advance.
This is a full-time, permanent role located in Toronto, Ontario.

If you are ready to unleash your potential it’s time to start your career with Manulife/John Hancock.
About Manulife
Manulife Financial Corporation is a leading international financial services group that helps people make their decisions easier and lives better. With our global headquarters in Toronto, Canada, we operate as Manulife across our offices in Canada, Asia, and Europe, and primarily as John Hancock in the United States. We provide financial advice, insurance, and wealth and asset management solutions for individuals, groups and institutions. At the end of 2019, we had more than 35,000 employees, over 98,000 agents, and thousands of distribution partners, serving almost 30 million customers. As of March 31, 2020, we had $1.2 trillion (US$0.8 trillion) in assets under management and administration, and in the previous 12 months we made $30.4 billion in payments to our customers. Our principal operations are in Asia, Canada and the United States where we have served customers for more than 155 years. We trade as 'MFC' on the Toronto, New York, and the Philippine stock exchanges and under '945' in Hong Kong.
Manulife is an equal opportunity employer. We strive to attract, develop and retain a workforce that is as diverse as the customers we serve and to foster an inclusive work environment that embraces the strength of cultures and individuals. We are committed to fair recruitment, retention and advancement and we administer all of our practices and programs based on qualification and performance and without discrimination on any protected ground.
It is our priority to remove barriers to provide equal access to employment. A Human Resources representative will consult with applicants contacted to participate at any stage of the recruitment process who request any accommodation. Information received regarding the accommodation needs of applicants will be addressed confidentially.
2020081860	Salentica Sr. Project Consultant	Salentica is a division of SS&C Technologies, Inc., a leading provider of software solutions and services for the international investment community. Investment managers, broker/dealers, sponsors, and custodians around the world use SS&C’s mission-critical and decision support systems. SS&C brings together experts in investments, providing superior client service from its headquarters in Windsor, CT, and subsidiaries in Canada, Australia, and Europe.
We have an immediate opening for a Salentica Project Consultant in our Jacksonville, Fl Office
Responsibilities
Lead and guide client through project implementation phases, including:
business process due diligence
Data analysis and mapping
solution design
application design
configuration
documentation of system requirements
Execute data analysis of client source data databases and client files
Execute business analysis on project implementation phases, including capture of detailed requirements for CRM
Work closely with data engineer to present data analysis in support of data migrations
Execute analysis and profiling on client data files, and where appropriate facilitate sessions with client and Salentica.
Work closely with project manager and client to execute typical business analysis tasks
Provide CRM configuration services to SS&C Salentica clients
Other project duties as assigned
Demonstrate superior communication skills
Ability to work independently as a self starter and in a team environment
Ability to work with internal teams to deliver client solutions
Working knowledge of financial services/asset management and RIA markets
Recognizes and minimizes Salentica project risk exposure.
Ensures project documents are complete, current, and stored appropriately
Position Qualifications

Industry Experience ( Wealth Management)
Bachelor’s degree, preferably in business IT systems
Ability to handle multiple projects/tasks simultaneously
Proven strong collaboration skills
Demonstrated aptitude and ability for planning and execution
Ability to recognize a client requirement, document, and outline steps for execution
Superior time management skills
Superior written, verbal communications skills
Ability to manage stakeholder expectations and report/communicate these back to the SS&C/Salentica Delivery team
Ability to take ownership of a task from initiation to completion
What will set you apart?

PMP designation
MS CRM and/or Salesforce certifications
Major IT implementation experience and understanding how systems are affected by business process change
Experience in dealing with complex business problems, identifying business/functional user requirements and recommending how to best support them through processes and applications
Ability to have influence without authority, and a strong ability to sell recommendations and solutions by stating advantages and value in business terms
Related financial service/wealth management industry experience

 SS&C Offers:
An extensive health benefits program which includes Health, Dental, and Vision
401k matching program
Generous Tuition Reimbursement and Training Allowance program
Business Casual work environment and Work-Life Balance.	e6c2ddd43685b4b3	Toronto, ON	SS&C Advent	https://ca.indeed.com/rc/clk?jk=e6c2ddd43685b4b3&fccid=609381a18aebd914&vjs=3	2020-08-18T11:03:40.000Z
2020081861	fd5da69e089b7f27	https://ca.indeed.com/rc/clk?jk=fd5da69e089b7f27&fccid=9343039b36601ad2&vjs=3	Data Engineer	Mississauga, ON	At Bond, we design creative and innovative solutions for our clients, all with the goal of helping them build ever-stronger loyalty to their brands. That can take us in some pretty amazing directions, and as a Data Engineer, you’ll have your hands on the wheel as we drive the future of loyalty.

Working on the bleeding edge of exciting technology, you're afforded the opportunity to experiment with new tools and attempt radically different approaches than traditional software engineering affords. Every day with the Data Engineering team is different and each project presents its own set of new and exciting challenges. Things shift very quickly in our industry and we rely on the Data Engineering team to keep us ahead of the curve and moving in the right direction.
Here's what we want:
Problem Solver: You are curious and loves exploring multiple approaches to find the most efficient, scalable solution and solve a problem
Collaborative: You work well with other people
Passionate: A passion for Big Data and an interest in the latest trends and developments constantly researching new tools and data technologies
Self-starter: You are comfortable helping your team get things done
Here's what you'll be doing:
Design, implement, and maintain data pipelines for extraction, transformation, and loading of data from a wide variety of data sources to various data services
Identify, design, and implement system performance improvements
Identify, design, and implement internal process improvements
Automate manual processes and optimize data delivery
Useful skills/background: You may or may not tick off every box, and that's ok. Each person brings a different background and different skills. If you think you are a good match for what we are looking for tell us why, and tell us what you are doing to improve yourself and we'll see what we can do to help!
A degree in Computer Science/Engineering or related field
2-4 years of experience in a software engineering environment
Experience with SQL and NoSQL systems
Knowledge of Hadoop, Spark, Kafka or other equivalent technologies
Proficiency in some of the following languages: Scala, Java, Python, Bash
Experience with automated testing systems
Mentorship, collaboration, and communication skills
Knowledge of data modelling, data warehousing, ETL processes, and business intelligence reporting tools
Experience working with CI/CD, containerization, and virtualization tools such as Gitlab, Jenkins, Kubernetes, Docker
Experience with tools like Databricks, Snowflake or PowerBI

Why Join Us?
Bond Brand Loyalty is proud to be recognized as one of Canada’s Best Managed Companies.

We’re 400(ish) people working tirelessly together to make the world a more loyal place. You’ll be joining a hyper-talented team with a galaxy of skillsets ranging from research to creative to digital and beyond. You’ll have an excellent opportunity to grow, learn and make an impact as we tackle some of our client’s biggest business challenges.
If you’re looking to build your career, build your skills and build bonds apply today!
Bond Brand Loyalty welcomes and encourages applications from people with disabilities. Accommodations are available on request for candidates taking part in all aspects of the selection process.	Bond Brand Loyalty	2020-08-18T11:03:40.000Z
2020081862	Data Scientist	0f973ec62ae8486f	https://ca.indeed.com/rc/clk?jk=0f973ec62ae8486f&fccid=dca2e344433f10ee&vjs=3	We make small businesses more successful through better banking.

Our company is looking for a Data Scientist to join our growing team as we enter a new phase of expansion We are a Toronto and New York-based, venture-backed startup at the heart of the FinTech movement that is shaping the way financial services are delivered.

Our product

NorthOne is a mobile, tech-powered bank account built for startups, freelancers, and small/medium-sized businesses (SMBs). Poor financial literacy has an outsized impact on the costs and failure rates amongst SMBs, and we are on a mission to eliminate these problems. We are more than a banking platform, we are the world-class Finance Department that SMBs could never afford.

Our team

We ❤ data. As a Data Scientist you'll work with our world-class Data, Engineer, Growth and Product teams. Our COO has helped build digital products used by companies like Frank And Oak, Supercell, MachineZone, BMO, and Trivago. Oh, and our CEO? 6 years at McKinsey working on digital customer experience in financial services. We're building a product that solves real pain, vs the imagined kind. Feeling it?

REQUIREMENTS

The skills required for this role:

You love using SQL and Python to answer business questions and analyze data
You live and breath report automation
World class data science experience that you can apply to product, operations, and growth marketing
You're delighted by the idea building the foundational elements of a growing data science practice
You're a strong, clear communicator who knows keep the whole team on the same page

BONUS points:

Hands on experience working with tools like Looker and Mixpanel
Financial services experience or good understanding of banking
Experience working in a startup environment or for a SaaS company
Background in management consulting
PhD or Masters degree

BENEFITS

Our mission is big and audacious, but we're assembling a team to take the challenge head-on.

As a Data Scientist you'll be joining a team that prioritizes:

People: Our company is more than just a business. We're a band of brothers and sisters supporting each other on our mission to rebuild business banking. We're really serious about mission, fit, and the people we work with. You'll be part of a rapidly scaling team that reflects these values and keeps this place special.
Diversity: You'll find yourself in an environment that values diversity and inclusivity. Excellence doesn't come in one flavor and neither should we.
Leadership: You're right in the thick of it, making critical decisions that will clear our path forward.

We offer full health/dental benefits, competitive compensation/equity, and one hell of an adventure.

If you recognize yourself in this job description, let's talk.	NorthOne	Toronto, ON	2020-08-18T11:03:41.000Z
2020081863	Cox Automotive	https://ca.indeed.com/rc/clk?jk=035a5f23df595f6f&fccid=2ca7392810684c2b&vjs=3	035a5f23df595f6f	This position will join our growing team of data and analytics experts for Cox Automotive Canada. You will be responsible for expanding and optimizing our data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. You will support our data initiatives and will ensure optimal data delivery consistent throughout ongoing projects.

As a Senior Data Engineer, you will be responsible for leading Data Engineering projects throughout their entire lifecycle, that being: initial investigation and stakeholder engagement, solution design, application development, testing and sign-off, release and maintenance

The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. They must be self-directed and comfortable supporting the data needs of Cox Automotive Canada. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives, enabling the evolution of our Data Science and Business Intelligence

Responsibilities:
Assemble large, complex data sets that meet functional / non-functional business requirements.
Recommend different ways to constantly improve data reliability and quality
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Improve upon the data ingestion pipelines, ETL jobs, and alarms to maintain data integrity and data availability.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics
Performs other job-related duties as assigned or apparent and Stay up-to-date with advances in data persistence and big data technologies
Prototype and design new data integration solutions that balance security, scalability, fault-tolerance, performance as well as cost effectiveness
Define, document and champion best practice architectural patterns and work with wider technology development teams to proactively seek out new opportunities to utilise Data Solutions technology and data assets
Be responsible for leading the delivery of multiple Data Engineering development projects at a time. Project delivery responsibility extends from design, development, testing, release and maintenance
Be responsible for leading the delivery of multiple Data Engineering development projects at a time. Project delivery responsibility extends from design, development, testing, release and maintenance
Qualifications

Qualifications

3+ years’ technical experience working in Big Data, designing solutions to complex data ingestion problems
Graduate degree in Computer Science, Computer Engineering or a related field
Big Data Certification and AWS certification would be an asset
Must have strong Data Orchestration experience using tools such has AWS Step Functions, Lambda, AWS Data Pipeline, Apache Nifi.
Advance Python Skills to develop efficient, decouple Data pipeline
In-depth knowledge of AWS Big Data Services
Must possess in-depth knowledge and hands on development experience in building Distributed Big Data Solutions including ingestion, caching, processing, consumption, logging & monitoring)
Must have a strong understanding and experience with Cloud Storage infrastructure and operationalizing AWS based storage services & solutions prefer S3 or related specific business questions and identify opportunities for improvement
Experience in building processes supporting data transformation, data structures, metadata, dependency and workload management
A successful history of manipulating, processing and extracting value from large disconnected datasets
Working knowledge of message queuing, stream processing, and highly scalable data stores
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases
Experience supporting and working with cross-functional teams in a dynamic environment
Experience deploying data pipelines with CI/CD
Experience with Snowflake Data Warehouse is plus
Organization: Cox Automotive
Primary Location: Canada-Ontario-Mississauga-2233 Argentia Rd
Employee Status: Regular
Job Level: Individual Contributor
Shift: Day Job -
Travel: Yes, 5 % of the Time
Schedule: Full-time
Unposting Date: Ongoing	2020-08-18T11:03:42.000Z	Mississauga, ON	Sr. Data Engineer
2020081864	256688ca085e919f	https://ca.indeed.com/rc/clk?jk=256688ca085e919f&fccid=5d784228b1eee537&vjs=3	Myant	Senior Data Engineer	About us:
At Myant, we are creating the world’s first textile computing platform, integrating technology directly into the only thing we’ve been wearing our entire life – clothing. SKIIN is our first consumer facing brand, and SKIIN’s vision is to enhance human ability through connected clothing - think Ironman’s suit, but comfortable. The sensors and actuators embedded within our apparel create your Digital Identity, which will be consumed by those who matter to you - your family members, doctors, coaches, other IoT devices - without you consciously having to think about it. Imagine a world where you walk into your house and the temperature automatically adjusts to your optimal body temperature, the lights adjust to your mood, you can monitor and adjust your everyday lifestyle based on your vital signs, or your doctor is aware of the onset of a disease before you even visit. The line between the digital and physical world is becoming increasingly blurry, and we believe textile is the next medium to bridge that gap.
We’re looking for people who believe in our mission to make wearable technology truly ubiquitous and convenient, so that everyone can benefit from it. We are a cross-functional team solving big challenges at the intersection of fashion, electronics, software, and data science.
Responsibilities:
Test the performance of the algorithms developed by Data Science team
Leverage native APIs for integration of AWS platforms
Take ownership of all your deliverables and communicate your results to timely project delivery
Prepare reports and some technical documentations

Qualifications Required:
Bachelor’s Degree in Computer Science, Computer Engineering, or equivalent work experience
Proficiency with JavaScript and Python language
Basic knowledge of machine learning algorithm and libraries like keras, tensorflow, sklearn
Experience in building RESTful APIs following Micro-Services Architecture
Experienced in NodeJS, PostgreSQL, and GraphQL.
Significant experience in building microservices leveraging various AWS features (AWS Lambda, IAM, SQS, DynamoDB, Kinesis, Redshift, Aurora, EC2, S3, API Gateway etc.)
Solid experience in Biomedical signal processing, and data mining related to physiological patient data is a bonus
Powered by JazzHR
8SQJftU9nl	2020-08-18T11:03:42.000Z	Toronto, ON
2020081865	33c214e6ad4ee9e2	2020-08-18T11:03:43.000Z	The Economical Insurance Group	https://ca.indeed.com/rc/clk?jk=33c214e6ad4ee9e2&fccid=d33243ac6c394a10&vjs=3	Senior Big Data Engineer	Senior Big Data Engineer
LOVE WHAT YOU DO
Work is a big part of our lives, so it makes sense to choose a company that offers truly rewarding work. At Economical Insurance®, your contributions, creativity, and energy won’t go to waste. Our passionate culture and pioneering mentality infuse everything we do, which is why our one-million-plus policyholders know we’ll protect their homes, businesses, farms, cars, and pets like they’re our own.
We’re not afraid to see how far we can push the envelope to make insurance better. Our family of companies includes Sonnet, the only Canadian home and auto insurer offering a fully online buying experience, and Petline, the largest Canadian pet insurance company. We’re also preparing to become a publicly traded company, a once-in-a-lifetime career opportunity for everyone who joins our team.
If you’re looking for a company that takes care of its people — and its customers — and has a track record of doing big things, get ready to love it here.
The Senior Big Data Engineer is accountable for the design and development of quality big data solutions for the operational and analytical business needs within Economical. Your expertise in Hadoop, Spark and related technologies will be used to optimize our current big data solution, as well as meet the future business needs. This role requires you to partner with IT, Analytics, Business Intelligence and the business lines to research, analyze, design and develop data solutions ranging in small to very highly complex.
What can you expect in this role?
Design and develop Big Data solutions for both operational and analytical requirements using Hadoop Open Source frameworks
Design, develop and integrate data ingestion, ETL/ELT data pipelines and processing/transformation processes
Work with senior stakeholders to develop a clear understanding of requirement drivers
Address non-functional requirements including performance, data governance, scalability, continuous integration, migration and compatibility

What do you bring to the role?
BSc in Computer Science, Engineering or related fields
Expertise with Hadoop distributed frameworks handling large amount of data using Spark and Hadoop ecosystems
Experience Hadoop technologies such as Oozie, Kafka, Druid, Hive, Storm, Ignite, Kudu, NiFi
Experience with data loading tools like Flume, Sqoop, as well as different layers of Hadoop Framework – Storage (HDFS, HBASE), Analysis (Hive/Kudu/Druid/Impala etc.), Map Reduce Jobs
Must have hands on Spark/Scala experience
Advanced programming skills (Python, Java, Scala or R preferred)
Strong experience with ETL tools
Minimum of 4 -5 years of experience handling variety of data (structured/unstructured), data formats (flat files, XML, JSON, relational, legacy) and data storage (HDFS, Hbase, NoSQL databases)
Strong communication skills (verbal and written) with ability to communicate across teams, internal and external at all levels
Experience in the financial sector, and specifically the P&C Insurance industry is considered a strong asset
Firm understanding of database systems – data modelling, SQL and transactional processing
Experience with API management best practices
DevOps and Agile collaboration knowledge are necessary (i.e. security, testing, version control, continuous integration, testing, etc.)
Experience with developing big data solutions in the cloud (AWS, GCP or Azure) would be great
This role can be in our Kitchener or Toronto location

We also take potential into consideration. If you don’t have this exact experience, but you know you have what it takes, be sure to give us more insight through your application and cover letter.
Go ahead and expect a lot — you deserve it.
We offer:
Competitive salaries, with potential for an annual raise and bonus
Pension and savings programs, with company-matched RRSP contributions
Generous time away, including vacation and personal needs days
Paid volunteer days and company matching on charitable donations
Educational resources, tuition assistance, and paid time off to study for exams
Two annual wellness campaigns — participants earn up to $300 each year to spend on almost anything supporting health and work-life balance (think things like spa days, daycare, pet grooming)
An unlimited employee referral bonus program
Flexible work schedule
Discounts on products and services

HOW TO APPLY
To complete the online application process, you’ll need to upload your resume and cover letter in one document. The posting will close at midnight on the deadline date; in order to successfully apply, please ensure your application is submitted by 11:59 p.m. the day before the deadline.
Our inclusive work environment welcomes diversity and supports accessibility. If you require accommodation at any time during the recruitment process, please let us know by contacting: hrsharedservices@economical.com.
Visit economical.com to learn more about us and what we’re up to.	Toronto, ON
2020081866	Headquartered in the Bay Area with offices in Austin, TX, Toronto, Canada and Jaipur, India, venture-funded Punchh is the world leader in innovative digital marketing products for brick and mortar retailers, combining AI technologies, mobile-first expertise, and Omni-Channel communications designed to dramatically increase customer lifetime value. Leading global chains in the restaurant, health and beauty sectors rely on Punchh to grow revenue by building customer relationships at every stage to becoming brand loyalists, including more than 100 different chains representing more than $12B in annual spend, 30,000 locations globally, 26M+ consumers, and 1M+ transactions daily. Punchh boasts a customer list that includes Pizza Hut, Quiznos, Coffee Bean & Tea Leaf and many more.

Title

Senior/Lead Big Data Engineer

Location

Toronto, Ontario, Canada

Reporting to

Sr. Dir. of Data

About the role

Punchh is looking for a Senior/Lead Big Data Engineer that will play a critical role in leading Punchh's data innovations. He/she will help create cutting-edge Big Data solutions by leveraging his/her prior industrial experience. This role requires close collaboration with the Machine Learning, Software Engineering, and Product Departments. You will be given the opportunity to not only serve internal teams, but also our business clients as well.

What You'll Do

Punchh is seeking to hire Big Data Engineer at either a senior or tech lead level. Reporting to the Director of Big Data, he/she will play a critical role in leading Punchh's big data innovations. By leveraging prior industrial experience in big data, he/she will help create cutting-edge data and analytics products for Punchh's business partners.

This role requires close collaborations with data, engineering, and product organizations. His/her job functions include

Work with large data sets and implement sophisticated data pipelines with both structured and structured data.
Collaborate with stakeholders to design scalable solutions.
Manage and optimize our internal data pipeline that supports marketing, customer success and data science to name a few.
A technical leader of Punchh's big data platform that supports AI and BI products.
Work with infra and operations team to monitor and optimize existing infrastructure
Occasional business travels are required.

What You'll Need

5+ years of experience as a Big Data engineering professional, developing scalable big data solutions.
Advanced degree in computer science, engineering or other related fields.
Demonstrated strength in data modelling, data warehousing and SQL.
Extensive knowledge with cloud technologies, e.g. AWS and Azure.
Excellent software engineering background. High familiarity with software development life cycle. Familiarity with GitHub/Airflow.
Advanced knowledge of big data technologies, such as programming language (Python, Java), relational (Postgres, mysql), NoSQL (Mongodb), Hadoop (EMR) and streaming (Kafka, Spark).
Strong problem solving skills with demonstrated rigor in building and maintaining a complex data pipeline.
Exceptional communication skills and ability to articulate a complex concept with thoughtful, actionable recommendations.

Benefits

Healthcare coverage
Life and AD&D insurance
Competitive salaries, bonus and stock options
Professional development
Paid Time off
Paid holidays
Free lunch in the office.

Punchh is proud to provide equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. If you'd like more information about your EEO rights as an applicant, please click here.

We also provide reasonable accommodations to individuals with disabilities in accordance with applicable laws.
Notice to recruiters and placement agencies: If you are a recruiter or placement agency, please do not submit résumés to any person or email address at Punchh prior to having a signed agreement with Human Resources. Punchh is not liable for and will not pay placement fees for candidates submitted by any agency other than its approved recruitment partners. Also, any résumés sent to us without an agreement in place will be considered your company's gift to Punchh and may be forwarded to our Talent Acquisition team.	0a0e41fb84eb4708	Punchh	Toronto, ON	2020-08-18T11:03:44.000Z	https://ca.indeed.com/rc/clk?jk=0a0e41fb84eb4708&fccid=e1b2607798446d2b&vjs=3	Senior/Lead Big Data Engineer
2020081867	https://ca.indeed.com/rc/clk?jk=5dae399d2651bf86&fccid=353eb997fc901045&vjs=3	At Veeva, we build enterprise cloud technology that powers the biggest names in the pharmaceutical, biotech, consumer goods, chemical & cosmetics industries. Our customers make vaccines, life-saving medicines, and life-enhancing products that make a difference in everyday lives. Our technology has transformed these industries; enabling them to get critical products and services to market faster. Our core values, Do the Right Thing, Customer Success, Employee Success, and Speed, guide us as we make our customers more efficient and effective in everything they do.

The Role

Veeva Systems is looking for experienced data engineers to build a cloud-based data analytics solution for the life science industry. If you are passionate about data and are eager to design and build data platforms from the ground up this is the role for you. The data analytics platform will provide data ingestion, data storage and rich data analytics capabilities with elegant visualization dashboards.
What You'll Do
Design and implement AWS based ETL processes to onboard data into our data lake from a variety of internal and external sources for our new data analytics platform.
Design data models and data services for optimal storage and retrieval.
Implement scalable data lake interfaces, microservices, and rest based API for querying and storing structured data.
Integrate new technologies to support advanced analytic use cases.
Requirements
5+ years’ experience in Python or Java, preferably at an enterprise cloud software company
Proven ability to write clean, testable, readable code in a team environment
Hands-on experience with building data pipelines in a programming language like Java or Python
3+ years of experience in relational databases with a mastery of SQL
Experience in data modelling, ETL development (pref. Apache Spark), and Data warehousing
Nice to Have
AWS Services (S3, Redshift, Elastic Search)
Experience with large scale big data pipeline – ETL / Kafka / Spark / MapReduce / Hadoop
Familiarity with Open API Specifications and Swagger
Experience working in an agile environment
Experience working in a startup
Perks & Benefits
 Conveniently located in downtown Toronto Snacks, beverages, and weekly lunches from local restaurants Team events and rec league sports teams Allocations for continuous learning & development Health & wellness programs Weekly yoga classes Ping pong and other games

Veeva’s headquarters is located in the San Francisco Bay Area with offices in more than 15 countries around the world.

Veeva Systems is an equal opportunity employer. Accordingly, we are committed to fair and accessible employment practices. Veeva Systems welcomes and encourages applications from people with disabilities. Accommodations are available upon request for candidates taking part in all aspects of the selection process.	2020-08-18T11:03:45.000Z	Senior Data Engineer	Toronto, ON	5dae399d2651bf86	Veeva Systems
2020081868	Location: Markham, Ontario
Job Description:
Job Summary
This role requires a dynamic individual with experience and passion for using data and analytics to drive business results and help us build the foundation to use our rapidly expanding data lake to improve business decisions. As a member of the Marketing team, the Data Engineer will be a key contributing resource to continually measure, evaluate, and make recommendations on our website marketing efforts as well as ecommerce user-experience.
Responsibilities
• Design, implement, track performance and refresh advanced analytic automated segmentation and predictive models
• Monitor Data quality with IT to ensure robust and accurate data
• Partner with IT to define data solutions from new data sources, and build requirements for extraction into the data lake
• Develop a strong acumen in source and downstream data storage system, and understand how the data is associated with business actions and potential solutions
• Will function as the primary liaison between marketing and IT
Experience and Education
• 5+ years designing data processes to automate organizational decisions ideally in a retail organization
• Bachelor's Degree in Statistics, Business, Quantitative Economics, Mathematics, Marketing, Economics, Engineering, Operations Research or similar programs
Competencies and Skills
• Strong Personal Drive for Excellence
• Excellent organizational and time management skills, with the ability to manage multiple priorities in a high demand environment
• Great sense of urgency and accountability, results-oriented with strong execution skills
• An independent problem solver, must be able to find creative solutions to unusual or unprecedented questions
• Quick learner – eagerness to learn about new tools and business systems to help craft solid solutions
Technology
• Experience in relational and cloud data storage using advanced SQL, using technologies such as Snowflake, Oracle, SQL Server
• Must be proficient in R, Python, Alteryx, Azure ML, or similar Machine Learning and data processing technology,
• Solid understanding of BI tools such as Tableau, MicroStrategy or QlikView
• Experience in Spark, Kafka, JAVA a strong plus
• Proficiency in processing Web Analytic raw data, ideally from Google Analytics 360, a large plus
• Snowflake, Orcale, SQL ServerSnowflake, Oracle, SQL Server
• Must be proficient in either R or Python
• Solid understanding in data visualization tools such as Tableau, MicroStrategy or QlikView
• Experience in Spark, Kafka, JAVA a strong plus
• Very comfortable with MS office (Word/Excel/Access/PowerPoint)
• Understanding of using SQL against relational databases
• Experience in data visualization and reporting tools, such as Tableau
#INDC	Markham, ON	2020-08-18T11:03:45.000Z	e1337dfa4d5c3f3b	Pet Valu	https://ca.indeed.com/rc/clk?jk=e1337dfa4d5c3f3b&fccid=95581e84c2b81b02&vjs=3	Data Engineer
2020081869	SADA	https://ca.indeed.com/rc/clk?jk=5c06518576e750fc&fccid=b704562e07a2a03f&vjs=3	2020-08-18T11:03:46.000Z	Senior Data Engineer	Toronto, ON	Join SADA as a Sr. Data Engineer!


Your Mission

As a Sr. Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data problems facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring batch or streaming data pipelines, data lakes and data warehouses.

You will be expected to run point on whole projects, end-to-end, and to mentor less experienced Data Engineers. You will be recognized as an expert within the team and will build a reputation with Google and our customers. You will demonstrate repeated delivery of project architectures and critical components that other engineers demur to you for lack of expertise. You will also participate in early-stage opportunity qualification calls, as well as lead client-facing technical discussions for established projects.

Pathway to Success

#BeOneStepAhead: At SADA we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our engineers know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs.

Your success starts by positively impacting the direction of a fast-growing practice with vision and passion. You will be measured quarterly by the breadth, magnitude, and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, how well you collaborate with your peers, and the consultative polish you bring to customer interactions.

As you continue to execute successfully, we will build a customized development plan together that leads you through the engineering or management growth tracks.

Expectations

Required Travel - 30% travel to customer sites, conferences, and other related events. Due to the COVID-19 pandemic, travel has been temporarily restricted.
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:
Google Professional Data Engineer Certified or able to complete within the first 45 days of employment

Required Qualifications:
Mastery in at least one of the following domain areas:
Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or customer-facing role

Useful Qualifications:
Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Experience operationalizing machine learning models on large datasets
Demonstrated leadership and self-direction - a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem

About SADA

Values: We built our core values on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA's values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

Make them rave
Be data driven
Be one step ahead
Be a change agent
Do the right thing

Work with the best: SADA has been the largest partner in North America for GCP since 2016 and has been named the 2019 and 2018 Google Cloud Global Partner of the Year. SADA has also been awarded Best Place to Work by Inc. as well as LA Business Journal!

Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match, professional development reimbursement program as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.	5c06518576e750fc
2020081870		Big Data Developer	2020-08-18T11:03:47.000Z	668db5396375205e	https://ca.indeed.com/rc/clk?jk=668db5396375205e&fccid=d2841a5c0380b93d&vjs=3	Toronto, ON	CGI
2020081871	Are you looking for unlimited opportunities to develop and succeed? With work that challenges and makes a difference, within a flexible and supportive environment, we can help our customers achieve their dreams and aspirations.
Job Description
Are you a go-getter who has a passion in building next gen data pipelines and provide Big data solution for business problems? Are you a big fan of simplification and automation?
Manulife is seeking an awesome Lead Data Engineer , with Big Data experience as well as strong understanding of data-ingestion, data curation and both Batch & Stream Data processing, to join our rapidly expanding IT Organization and assist us as we work to be a digital leader in our industries!
Skills and Experience
You will have the following skills and experience:
Lead development teams to define and build data pipelines
Expert in building and operationalizing BigData platforms in cloud using one of the public clouds, preferably MS Azure.
Hands on experience with Big Data streaming frameworks and tools (Spark Streaming, Storm, Kafka, etc.)
Expert in Hadoop ecosystem and toolset – Sqoop, Nifi, Pig, Spark, HDFS, Hive, HBase, etc.
Expert in automating data pipelines in a Big Data ecosystem, DevOps and CICD.
Experience in developing Hadoop integrations (batch or streaming) for data ingestion, data mapping and data processing capabilities
Experience programming in both compiled languages (Java, Scala) and scripting languages (Python or R)
Expert in developing Big Data set processes for data modeling, mining and production
Experience in working with key partners including business and technology to establish definition of success, goals, key use cases and aligning dev team on strategic priorities.
Excellent communication and interpersonal skills
Excellent analytics, problem solving and solutioning skills
A capacity for constant learning from both success and failure, remaining open to change and continuous improvement
Good to Haves
Experience in Exploratory data analysis; Query and process Big Data, provide reports, summarize and visualize the data
Experience in Canary deployments, 0-downtime, 0-dataloss, hot-hot DR
Experience in designing solutions for Big Data warehouses
Experience with Hadoop security frameworks like Knox, Ranger.
Experience with Hadoop metadata frameworks and security policies such as Ranger, Atlas
Experience in data profiling and analysis
Exposure to and an understanding of Agile scrum methodologies and experience of working in an Agile team
Experience in Big Data performance analysis, tuning and capacity planning
Experience in designing business intelligence systems, dashboard reporting, and analytical reporting is a plus
Experience with the Hortonworks Data Platform (version 2.5)
Experience in using Git flow.
Basic understanding of following will be useful but not required:
Exposure to and basic understanding of collaboration tools like Slack, Skype, Teams, and JIRA
What about Perks?
Manulife has lots of perks including, but not limited to:
Competitive compensation
Retirement Savings Accounts including a RPP (Pension Plan), RRSP (Retirement Savings Plan), and TFSA (Tax Free Savings account)
Manulife Share Ownership Program with employer matching
Customizable Benefits Package including Health, Dental, Vision, and 100% of Mental Health expenses
Financial support for ongoing training, learning, and education
Monthly Innovation Days (Hackathons)
Wearing jeans to work every day
An abundance of career paths and opportunities to advance
A flexible work environment with flex hours, work from home arrangements, distributed teams, and condensed work week arrangements.
This is a full time permanent role and the team is located in Kitchener/Waterloo, Ontario. There is opportunity for Toronto based people to work in this role, however there would be travel to Kitchener / Waterloo twice per week.
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
About Manulife
Manulife Financial Corporation is a leading international financial services group that helps people achieve their dreams and aspirations by putting customers' needs first and providing the right advice and solutions. We operate primarily as John Hancock in the United States and Manulife elsewhere. We provide financial advice, insurance, as well as wealth and asset management solutions for individuals, groups and institutions. At the end of 2017, we had approximately 34,000 employees, 73,000 agents, and thousands of distribution partners, serving more than 26 million customers. As of December 31, 2017, we had over $1.04 trillion (US$829.4 billion) in assets under management and administration, and in the previous 12 months we made $26.7 billion in payments to our customers. Our principal operations are in Asia, Canada and the United States where we have served customers for more than 100 years. With our global headquarters in Toronto, Canada, we trade as 'MFC' on the Toronto, New York, and the Philippine stock exchanges and under '945' in Hong Kong

If you are ready to unleash your potential it’s time to start your career with Manulife/John Hancock.
About Manulife
Manulife Financial Corporation is a leading international financial services group that helps people make their decisions easier and lives better. We operate primarily as John Hancock in the United States and Manulife elsewhere. We provide financial advice, insurance, as well as wealth and asset management solutions for individuals, groups and institutions. At the end of 2018, we had more than 34,000 employees, over 82,000 agents, and thousands of distribution partners, serving almost 28 million customers. As of March 31, 2019, we had over $1.1 trillion (US$849 billion) in assets under management and administration, and in the previous 12 months we made $29.4 billion in payments to our customers.
Our principal operations in Asia, Canada and the United States are where we have served customers for more than 100 years. With our global headquarters in Toronto, Canada, we trade as 'MFC' on the Toronto, New York, and the Philippine stock exchanges and under '945' in Hong Kong.

Manulife is an equal opportunity employer. We strive to attract, develop and retain a workforce that is as diverse as the customers we serve and to foster an inclusive work environment that embraces the strength of cultures and individuals. We are committed to fair recruitment, retention and advancement and we administer all of our practices and programs based on qualification and performance and without discrimination on any protected ground.
It is our priority to remove barriers to provide equal access to employment. A Human Resources representative will consult with applicants contacted to participate at any stage of the recruitment process who request any accommodation. Information received regarding the accommodation needs of applicants will be addressed confidentially.	https://ca.indeed.com/rc/clk?jk=4ecf6d90140603ee&fccid=1747adf6142beb48&vjs=3	Lead Data Engineer	Manulife	Toronto, ON	4ecf6d90140603ee	2020-08-18T11:03:50.000Z
2020081872	https://ca.indeed.com/rc/clk?jk=1dd125b777e13132&fccid=cdf5f442bc9a18df&vjs=3	1dd125b777e13132	Data Solution Engineer
Onix helps customers transform and evolve their business through the use of cloud services. As part of an entrepreneurial team in this rapidly growing business, you will help shape the future of how technology is used in the workplace.
Primary Responsibilities
Lead strategic cloud application development discovery session to help customers understand the value of cloud application development and how to position it within their organization.
Proactively help customers address all technical issues that may arise throughout the entire pre-sales cycle.
Ability to facilitate demonstrations, proof of concepts and public-facing presentations.
Use Google Cloud Platform tools to build Enterprise-grade Big Data solutions.
Architect new cloud-based data pipelines.
Ability to bring together multiple data sources into a unified data warehouse.
Apply analytics and visualizations to customer data sets.
Create customer and partner connections to help grow Onix name recognition in the data space.
Help customers understand the right technologies for their use case.
Establish strategic customer relationships and become their go-to trusted advisor for Big Data needs.
Assist in strategic direction and planning for growth of the Cloud Data Team.
Quickly architect sound cloud solutions to radically different customer environments.
Establish strategic customer relationships and become the technical go-to resource for answers as well as a trusted cloud advisor.
In-depth understanding and the ability to demonstrate expertise in designing, deploying, and maintaining custom enterprise web applications.
In-depth understanding and the ability to demonstrate expertise in using a variety of development languages.
In-depth understanding and the ability to demonstrate expertise to determine the best migration path from legacy or on-prem applications into public cloud environments.
Review and analyze customer architecture at the domain and product level and translate and evolve them into cloud-ready applications.
Staying in constant communication with customers to ensure Onix is addressing all of their needs during the pre-sales cycle.
Learning and maintaining an in-depth understanding of current and new development technologies and industry standards.
Assist account manager with technical discovery and responsible for all technical scoping activities during the creation of the Statement of Work.
Ability to frequently travel throughout the United States and Canada. Up to 25%.
Required Skills and Experience
Degree in Computer Science or Math (or related technical major) or equivalent practical experience (math strongly preferred).
Experience with large data sets and Enterprise-grade databases (structured and unstructured)
Experience architecting and building data pipelines.
Deep understanding of the ETL (extract, transform, load) process.
Experience extracting data from multiple sources via APIs and scripting.
Experience transforming data through field mapping, programmatic rulesets, and data integrity checking.
Able to expertly convey ideas and concepts to others.
Excellent communication skills (verbal, written and presentation)
Creative problem-solving skills and the ability to design solutions not immediately apparent.
Ability to participate in multiple projects concurrently.
Customer-oriented and shows a bias for action.
Able to function in a highly dynamic team that moves rapidly from idea to planning to implementation.
Highly adaptable with the ability to learn new technologies quickly without direct oversight.
Strong knowledge of Python Machine Learning standard libraries.
Mastery of N-dimensional NumPy arrays.
Mastery of pandas data frames
Ability to perform element-wise vector and matrix operations on NumPy arrays.
Strong knowledge of Anaconda, Virtualenv, and Jupyter Notebooks
Good functional Knowledge of Tensorflow programming model.
Strong understanding of all commonly used Machine Learning models and the main algorithms that compose the models.
Ability to rapidly prototype proofs-of-concept and technical demonstrations.
Ability to conduct technical BD/ML workshops enabling the audience ( researchers, Doctoral and postdoctoral CS Ph.Ds, ML Ph.Ds, Mathematicians, Scientist etc) to adopt the cloud technologies to for development and implementation of BD/ML for scientific research.
Good knowledge of common networking concepts.
Strong customer-facing communication skills.
Experience in writing software in Java or Python.
Familiarity with web-related technologies (web applications, web services, service-oriented architectures) and network/web related protocols.
Creative problem-solving skills and a drive to solve difficult issues.
Ability to stay positive and motivated while under pressure.
Ability to participate in multiple projects concurrently.
Excellent communication skills (verbal, written and presentation)
Customer-oriented and shows a bias for action.
Provide on-time, well-executed work that leads to excellent customer satisfaction.
Able to expertly convey ideas and concepts to others.
Highly adaptable with the ability to learn new technologies quickly without direct oversight.
Good understanding of the built-in data types. ( lists, dictionaries, tuples sets).
Preferred Skills and Experience
Google Cloud Platform Data Engineer Certification.
Experience with Big Data, PaaS, and IaaS technologies.
Experience in and understanding of data and information management - especially as it relates to IaaS and PaaS.
Experience architecting and developing software for scalable, distributed systems.
Understanding of the public cloud market and pain points driving enterprise cloud adoption.
It is the policy of Onix to ensure equal employment opportunity in accordance with the Ohio Revised Code 125.111 and all applicable federal regulations and guidelines. Employment discrimination against employees and applicants due to race, color, religion, sex, (including sexual harassment), national origin, disability, age (40 years old or more), military status, or veteran status is illegal.
Onix will only employ those who are legally authorized to work in the United States or Canada. This is not a position for which sponsorship will be provided. Individuals with temporary visas such as E, F-1, H-1, H-2, L, B, J, or TN or who need sponsorship for work authorization now or in the future, are not eligible for hire.	Toronto, ON	2020-08-18T11:03:51.000Z	Solution Engineer - Data	Onix Networking Corp
2020081873	Senior Core Data Engineer - 291021	acf8d2a1bace698b	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0BreR47D9bMWJ28XlwS8rs2_GIFY3-vSdy_Xwl-swcV-nLMwXF3u7ntxl6CekgI_C941PK3ME-fZaKkO57-1wkBLP1YJrJbCIBfYcJqB9KIDoDrVOrCvprx2SXVax1W2nFLHP0vh9N_anvZ2h9WgJlNpd33YWMXSiFgb7vn4ARF07JQ8p6VjM8lAErqNfeXrxaaiYXOFW79QdhILSSGjwYNQJIrSiVPlciPbJJ99bgy1tMlc19ZmoJPzubILEyLFgSokHaEG91oaYC6ZcIPAfNvYvDEp658ypACRlUsu8WIQNcUr08RceiZwjL7ysJEaBDWwJ4BE79kuzwtRUmV8SIQ-Ikm_4JtrVsHX8Gb5hiAFm31Gorc6KSLxjasgUj14WTgbiv3FQL7LUb5CUDN8Mg0fBNdjp7bgOY-ayvShjwL96B9x8FX9-JIFDer6NpC8QSz0yByofh9-OobPW2OXZNYYL_FBsXE6f1woFzQTcrC9KLWGlBLx7-EZzdV7t6BSh1KqiZck1PLRLnGcjwSUam_qtFSO_snDzgpgtEe1D7NFWpeMzXSof9hsdj-NMQLpsRMWXqh549dY6uIkQfG1MQs1Zb2uiqqCK6Ut5urmjcuLE9EKKCiEcM5IaAR9G3zm4g=&p=13&fvj=0&vjs=3	Procom	Mississauga, ON	Sr. Core Network Data Engineer
On behalf of our client in the telecom sector, Procom is looking for a Core Network Data Engineer.
Sr. Core Network Data Engineer - Job Description
Design, maintain, operate wireless packet core network with the “Best Network” objectives in mind.
Complete change and implementation activity on network elements, changing resource/connectivity allocations and verifying successful network integration of any changes.
Monitor health/status of HSPA (GGSN, SGSN, GPRS DNS), LTE (MME, PGW, SGW), VoWIFI (AAA, EPDG) network elements, react to alarms, and perform corrective actions as necessary.
Deploying NFVs in an IaaS Openstack environment.
CCNA to CCNP level IP networking, facilitating monitoring, verification and troubleshooting of connectivity between core data network elements, resource allocations and mobile data call path from RAN to target destinations
Innovate new and creative solutions to improve the way we work and deliver our projects.
Develop solutions to automate repetitive tasks and improve working efficiency within the team.
Some night shift work and weekend/holiday coverage will be required for maintenance window implementations and operational support.
Sr. Core Network Data Engineer - Mandatory Skills
Degree/Diploma in Computer Science, or Engineering.
Practical knowledge with DevOps tools and programming/scripting languages such as Python, Bash, PowerShell, Go, Ansible, Git.
Experience working with cloud computing platform such as Openstack
Understanding or experience of Agile, Lean, DevOps, adaptive working environment.
Understanding of 3GPP and IP networking protocols
Experience working on Linux operating system such as Ubuntu, Red Hat
Ability to work and adapt in a continuously changing work environment.
Proven interpersonal and leadership skills with a passion to see the work through the eyes of the customer.
Sr. Core Network Data Engineer - Assignment Start Date
ASAP – 12 months to start
Sr. Core Network Data Engineer – Assignment Location
Mississauga, ON	2020-08-18T11:03:52.000Z
2020081874	The Data Engineer will be responsible for expanding and optimizing the data and data pipeline architecture across the enterprise. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, data architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing Express Script Canada’s data architecture to support our next generation of products and data initiatives.

ESSENTIAL FUNCTIONS:
Create and maintain optimal data pipeline architecture.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Optimize the value of technology investments in data management for the business by aligning the business architectures with technology architectures. Identify and drive key technology investments to meet business objectives, remediate technology and process gaps. Determine feasibility, cost and time required, compatibility with current system, and system capabilities.
Identify product, technology and process gaps in current data & technology architectures and recommend solutions to bridge gaps between the business and the data & technology deployed to support the business.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Coordinate with data users and key stakeholders across ESC’s Lines of Business to refine and achieve various long-term objectives for data architecture
Work with stakeholders including Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product.
Work with data and analytics experts to strive for greater functionality in our data systems.

QUALIFICATIONS:
Minimum 6 years of experience in a large-scale, multi-platform, multi-tier processing environment with a Bachelor’s degree in Information Systems or related field
Extensive experience on projects implementing Master Data Management and Enterprise Content Management techniques and platforms.
Expert domain knowledge & experience in data warehousing, reporting and advanced analytics platforms, encompassing data model design, dimensional modeling, master data management, cross-cutting concerns, common integration technologies, patterns & standards and emerging technologies.
Advanced knowledge of data architecture and analytics principles and practices for application development and data warehousing purposes.
Advanced knowledge of data model development and governance methods and practices
Previous data management experience in a data warehouse and data lake environment.
Experience with data center migrations, enterprise database consolidation, data warehouse migration and/or consolidation to data lakes or integrated data architectures.
Experience building multi-cloud analytics and technology strategies.
Highly proficient in Relational Database Management Systems
(RDBMS) and Big Data, data architecture, data modeling (including dimensional),
data warehousing, object-oriented methodologies, and client/server development.
Advanced knowledge of PowerDesigner or similar Data Modelling Tool
Advanced knowledge of Talend or similar Data Integration Tool
Hands-on experience using data technologies to implement data profiling tools and modern database implementations including Enterprise Data Lakes, graphDB, key-value pair, column-store, Big Table, RDF, In-Memory DB, etc..
Working knowledge of Hadoop platforms (Hortonworks or Cloudera) and key technologies like Apache Nifi, Kafka, Spark, Pig, Hive, NoSQL databases like MongoDB, Cassandra or Hbase.
Experience implementing data visualization tools like Tableau, Qlik, etc.
Willingness to work a flexible schedule to accommodate project deadlines and travel requirements

Assets:
Knowledge of the group health insurance (pharmacy, dental, other health) industry or adjudication systems is an asset
Knowledge of advanced analytics tools like R, SAS, and machine learning algorithms.
Knowledge of one or more programming or scripting languages like Java, C, C#, .Net, Javascript, PHP, Python
Knowledge of the DAMA Book of Knowledge
Knowledge of Big Data & Logical Warehouse architecture
Knowledge of TOGAF, Zachman or other
architecture frameworks

ABOUT EXPRESS SCRIPTS CANADA

Express Scripts Canada, a registered business name of ESI Canada, an Ontario partnership indirectly controlled by Express Scripts, Inc. (Nasdaq: ESRX), is one of Canada’s leading providers of health benefits management services. From its corporate headquarters in Mississauga, Ontario, just outside Toronto, Express Scripts Canada provides a full range of integrated pharmacy benefit management (PBM) services to insurers, third-party administrators, plan sponsors and the public sector, including health-claims adjudication and processing services, Home Delivery Pharmacy Services, benefit-design consultation, drug-utilization review, formulary management, and medical and drug-data analysis services, to better facilitate the best possible health outcomes at the lowest possible cost.

It will be a condition of employment that the successful candidate receives the Enhanced Reliability Clearance from the Federal Government. The candidate will be required to provide supporting documentation in order to receive Clearance if required.

We offer a competitive salary, along with a positive work environment built on solid corporate values, integrity, mutual respect, collaboration, passion, service and alignment.

We are an equal opportunity employer that promotes a diverse, inclusive and accessible workplace. By embracing diversity, we build a more effective organization that empowers our employees to be the best that they can be.

We are committed to creating a working environment that is barrier-free and we are prepared to provide accommodation for people with disabilities. Thank you for your interest in this position, however only qualified candidates will be contacted for an interview. No telephone calls please.

For more information about Express Scripts Canada, visit its Web site at www.express-scripts.ca

About Cigna
Cigna Corporation (NYSE: CI) is a global health service company dedicated to improving the health, well-being and peace of mind of those we serve. We offer an integrated suite of health services through Cigna, Express Scripts, and our affiliates including medical, dental, behavioral health, pharmacy, vision, supplemental benefits, and other related products. Together, with our 74,000 employees worldwide, we aspire to transform health services, making them more affordable and accessible to millions. Through our unmatched expertise, bold action, fresh ideas and an unwavering commitment to patient-centered care, we are a force of health services innovation.

When you work with Cigna, you’ll enjoy meaningful career experiences that enrich people’s lives while working together to make the world a healthier place. What difference will you make? To see our culture in action, search #TeamCigna on Instagram.	Cigna	https://ca.indeed.com/rc/clk?jk=60d7283f8c769ec7&fccid=afbf8c270610a38a&vjs=3	2020-08-18T11:03:53.000Z	Senior Data Engineer	Mississauga, ON	60d7283f8c769ec7
2020081875	Fleet Complete	https://ca.indeed.com/rc/clk?jk=2ef0d527757decfa&fccid=c0b5558e336243b3&vjs=3	Data Developer	2ef0d527757decfa	Toronto, ON	COMPANY OVERVIEW:
Success stories like this, don’t happen every day. From humble beginnings as a courier industry solutions provider in Canada, Fleet Complete quickly grew to be one of the world’s leaders in telematics and connected mobility solutions for a wide variety of industries with fleets, assets and mobile workers.

Today, with 20 years in the industry, Fleet Complete is one of the fastest-growing IoT (Internet of Things) companies across the globe, operating in 17 countries with offices in Canada, Netherlands, Denmark, Belgium, Estonia, Latvia, Lithuania and Australia. Fleet Complete continues to win employer, innovation thanks to our relentless customer-centric approach and commitment to company values of Innovation, Quality, Customers, Productivity, People and Community.

Thanks to strong partnerships and sound investments, our trusted Fleet and Mobile workforce platform provides real-time insights, visibility, employee safety and overall operational efficiency. This helps organizations, municipalities and businesses of all sizes to modernize their operations with ease. Fleet Complete is known for hiring, growing and empowering talented people who develop innovative products, build powerful relationships and provide personalized support that is unparalleled in our industry. Join Fleet Complete on our next chapter and we can work together to "help fleets thrive".

Proud to be named one of Greater Toronto’s Top Employers for 2020: http://content.eluta.ca/top-employer-fleet-complete

ESSENTIAL DUTIES & RESPONSIBILITIES:

Create and maintain optimal data pipeline architecture for legacy and the new architecture of IoT streaming data (Telematics and other automotive sensors)
Assemble large, complex data sets that meet functional / non-functional business requirements for data and application products
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics
Work with the Product team and other stakeholders to assist with data-related technical issues and support their data infrastructure needs
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader

QUALIFICATIONS:
All applicants must possess the following:

5+ years of experience in a Data Engineer role
A degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field (Graduate degree would be a plus)
Experience with cloud technologies. Specifically, AWS technologies such as S3, Glacier, Lambda, Athena, Redshift
Experience with object-oriented & functional scripting languages including Python and Java
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Strong analytic skills related to working with unstructured datasets
Build processes supporting data transformation, data structures, metadata, dependency and workload management
A successful history of manipulating, processing and extracting value from large disconnected datasets
Strong project management and organizational skills
Experience supporting and working with cross-functional teams in a dynamic environment
Understands and helps drive business impact via data systems and their resulting output

Fleet Complete will provide support in its recruitment processes to applicants with disabilities, including accommodation that takes into account an applicant's accessibility needs. If you require accommodation during the interview process, please contact the Recruitment Team, 866-649-7949.

Fleet Complete is an equal opportunity employer committed to diversity and inclusion. We are pleased to consider all qualified applicants for employment without regard to race, color, religion, sex, national origin, age, disability, protected veterans’ status or any other legally protected factors.	2020-08-18T11:03:54.000Z
2020081876	bcb75a8b3d33578b	Markham, ON	BGIS	Senior Data Engineer	2020-08-18T11:03:55.000Z	https://ca.indeed.com/rc/clk?jk=bcb75a8b3d33578b&fccid=84c23fbcabc14d59&vjs=3	Job Field:
Information Technology
Job Type:
Full-time
Building Location:
Length of Assignment:

SUMMARY

The Senior Data Engineer is a deep technical expert in building complex data warehousing and business intelligence applications. At this level, the incumbent demonstrates a passion and in-depth knowledge of large, complex application development methodologies. They motivate themselves and the team to refine their skills and adopt best practices for developing pragmatic software solutions for the organization. Leading the charge, they continue to raise the bar on mastery of business intelligence application development within the team and the organization.

KEY DUTIES & RESPONSIBILITIES

Programming

Uses in-depth knowledge of advanced programming techniques, design patterns and hardware/software interfaces to develop business intelligence and data warehouse applications.
Designs, tests and integrates data warehouse and BI modules and resolves programming errors using various debugging tools and techniques.
Provides guidance/mentors on programming practices and techniques to individuals and cross-functional teams.
Provides support, guidance and production assurance for very complex or urgent problems.
Performs work with minimum supervision, and work is assigned in terms of technical objectives.
Prepares technical documentation (e.g., user guides, technical specifications).
Assists in the design of business solutions.

Analysis

Conducts impact analysis for proposed changes to or problems across the system.
Leads team discussions in the analysis and collaboration to clarify and improve specifications or to identify alternative programming solutions.

Continuous Improvement

Makes recommendations or decisions on architecture, application design, standards and process improvements.
Enforces team and organizational standards and practices (e.g. at walkthroughs and peer code reviews).
Engages in continuous learning by developing and executing on a learning plan.
Takes responsibility for individual and the team's results.
Advocates for quality in all aspects of development efforts based on the team's definition of quality.

Risk Management

Estimates and prioritizes work to maximize value while taking into account risk, effort and dependencies.
Raises impediments, risks, and issues as early as possible and work with the team to mitigate as needed.

KNOWLEDGE & SKILLS

University graduation and minimum 5-10 years of relevant experience
Demonstrates in-depth knowledge of Microsoft BI architecture, established data warehouse development methodologies, multi-dimensional data modelling, OLAP, metadata management, data security, predictive analysis and big data processing. Extensive experience in one of these cloud data warehouses (Snowflake, bigtable, Redshift), Data Vault 2.0 methodology, steaming data processing, BI components in SQL Server 2016+, TSQL, and DAX, Power BI.
A good working knowledge of application security, C#, python, PowerShell, metadata management, NoSQL, and data security.
Experience in programming and debugging complex data warehouse and BI applications as part of a multi-disciplinary team environment (following an agile framework such as SCRUM preferred) based on Microsoft Team Foundation Server and git.
Experience in writing unit tests to support production code using a unit test framework.
Experience with database management (i.e. database design, schema creation, concurrency and performance considerations).
Takes ownership and initiative and collaborates well with a team of peers.
Demonstrates a commitment to continuous learning (e.g. user groups, blogs, conferences, community awareness, and next generation tooling).
Able to clearly communicate in both a verbal and written form within a predominantly English working environment.
Has a positive, passionate, idea generating attitude when faced with challenges.

Licenses and/or Professional Accreditation

Certification in Microsoft technologies preferred
2020081877	Senior Core Data Engineer - 291021	acf8d2a1bace698b	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0BreR47D9bMWJ28XlwS8rs2_GIFY3-vSdy_Xwl-swcV-nLMwXF3u7ntxl6CekgI_C941PK3ME-fZaKkO57-1wkBLP1YJrJbCIBfYcJqB9KIDoDrVOrCvprx2SXVax1W2nFLHP0vh9N_anvZ2h9WgJlNpd33YWMXSiFr72DYPgu52qjtiiAK38sZ71vgsyP2g5bftK9T4po3TvTDtddjMUSJgooiJritP2VDvvU1P6_eoqxZee3MQjbHzFvr889K0aG8xmQdDQbZGVczqcI4ZZH7VUzQcJ-9KwJrAn2QlhlB1_f473Qj-Qn3c6Z175oZc67Jknl8n0cmE1A7TsIvcp7XcDKJ1J5jKXMMGF84nTTL39lIE3feQsWI-OCuQ0BaDaR63xVyUegtdlttrqlAj_Wwh7FXB70cCru-FG4rGt2zYYa1izB0evdWU91okoM15QYR2TkGGZzTmiU6zNRklZrL6y26qKw9GkYtkb9EkxJDPR1jBzpH17SPt-fN4uAjS9gVnTBzRW1HlySsfc_gmD3-tNyZrfRz8us4wk58u7nd95t8OzPhvCuDRPzBvSeIcGJrLh23J5dZcM_L2nKoWRvhgXcG2Z90GBRRd6QazAVL4Wh3AU6hOmiH4Vt6Fd0tCvg=&p=2&fvj=0&vjs=3	Procom	Mississauga, ON	Sr. Core Network Data Engineer
On behalf of our client in the telecom sector, Procom is looking for a Core Network Data Engineer.
Sr. Core Network Data Engineer - Job Description
Design, maintain, operate wireless packet core network with the “Best Network” objectives in mind.
Complete change and implementation activity on network elements, changing resource/connectivity allocations and verifying successful network integration of any changes.
Monitor health/status of HSPA (GGSN, SGSN, GPRS DNS), LTE (MME, PGW, SGW), VoWIFI (AAA, EPDG) network elements, react to alarms, and perform corrective actions as necessary.
Deploying NFVs in an IaaS Openstack environment.
CCNA to CCNP level IP networking, facilitating monitoring, verification and troubleshooting of connectivity between core data network elements, resource allocations and mobile data call path from RAN to target destinations
Innovate new and creative solutions to improve the way we work and deliver our projects.
Develop solutions to automate repetitive tasks and improve working efficiency within the team.
Some night shift work and weekend/holiday coverage will be required for maintenance window implementations and operational support.
Sr. Core Network Data Engineer - Mandatory Skills
Degree/Diploma in Computer Science, or Engineering.
Practical knowledge with DevOps tools and programming/scripting languages such as Python, Bash, PowerShell, Go, Ansible, Git.
Experience working with cloud computing platform such as Openstack
Understanding or experience of Agile, Lean, DevOps, adaptive working environment.
Understanding of 3GPP and IP networking protocols
Experience working on Linux operating system such as Ubuntu, Red Hat
Ability to work and adapt in a continuously changing work environment.
Proven interpersonal and leadership skills with a passion to see the work through the eyes of the customer.
Sr. Core Network Data Engineer - Assignment Start Date
ASAP – 12 months to start
Sr. Core Network Data Engineer – Assignment Location
Mississauga, ON	2020-08-18T11:03:56.000Z
2020081878	https://ca.indeed.com/rc/clk?jk=82b02bab68b5d127&fccid=a4e4e2eaf26690c9&vjs=3	ARE YOU READY to step up and take your technology expertise to the next level?

There is never a typical day at Accenture, but that’s why we love it here! This is an extraordinary chance to begin a rewarding career at Accenture Technology. Immersed in a digitally compassionate and innovation-led environment, here is where you can help top clients shift to the New using leading-edge technologies on the most ground-breaking projects imaginable.
Interested in building end-to-end marketing solutions for clients? Bring your talent and join Data which operates in the Interactive, Mobility and Analytics space. You will have opportunities to get involved in digital marketing, eCommerce and end-to-end mobility capabilities to help clients to improve productivity and more!

WORK YOU’LL DO
Work across the Service Delivery Lifecycle to analyze, design, build, test, implement and/or maintain multiple system components or applications for Accenture or our clients
Adapts existing methods and procedures to create possible alternative solutions to moderately complex problems
Uses considerable judgment to determine solution and seeks guidance on complex problems
Determines methods and procedures on new assignments with guidance
Manages small teams and/or work efforts (if in an individual contributor role) at a client or within Accenture
Experience mentoring and managing other Data Engineers, ensuring modern data engineering best practices are being followed

WHO WE´RE LOOKING FOR?
Minimum 8 years of experience as a Data Engineer with Data Warehouse Experience with Azure DW, Synapse, Oracle, Redshift, PostgreSQL etc.
Demonstrated strength in SQL, data modeling, ETL development, and data warehousing
Must have experience with one of the Cloud Technologies (Azure or AWS)
Azure cloud includes Spark, Python, Databricks, Synapse, Snowflake, Data Factory and ADLS
AWS cloud includes Glue, EC2, EMR, Athena, redshift, Snowflake, S3, Spark, Python and Databricks
Experience with Big Data technologies like MapReduce, Pig, Hive, HBase, Sqoop, Flume, YARN, Kafka, Storm and etc.
3+ years of experience with at least one SQL language such as T-SQL or PL/SQL
3+ years of work experience with ETL and data modeling
Experience in real-time analytics application
Experience in both batch and stream processing technologies
Coding proficiency in at least one modern programming language (Python, Ruby, Scala, Java, etc.)
Machine learning experience with Spark or similar
Must be eligible for security clearance
Professional Skills Qualifications:
Proven success in contributing to a team-oriented environment.
Proven ability to work creatively in a problem-solving environment.
Desire to work in an information systems environment.
Demonstrated teamwork and collaboration in professional setting; either military or civilian.
WHAT´S IN IT FOR YOU?
Competitive benefits, including a fair and balanced parental leave policy.
Fantastic opportunities to develop your career across industries with local and global clients.
Performance achievement and career mentorship: our performance management process focuses on your strengths, progress and career possibilities.
Opportunities to get involved in corporate citizenship initiatives, from volunteering to charity work.
To learn more about Accenture, and how you will be challenged and inspired from Day 1, please visit our website at accenture.ca/careers.
It is currently our objective to assign our people to work near where they live. However, given the nature of our business and our need to serve our clients, our employees must be available to travel when needed.

Accenture does not discriminate on the basis of race, religion, color, sex, age, non-disqualifying physical or mental disability, national origin, sexual orientation, gender identity or expression, or any other basis covered by local law. Accenture is committed to providing employment opportunities to current or former members of the armed forces.

We are committed to employment equity. We encourage all people, including women, visible minorities, persons with disabilities and persons of aboriginal descent to apply.

Accenture is a leading global professional services company, providing a broad range of services and solutions in strategy, consulting, digital, technology and operations. Combining unmatched experience and specialized skills across more than 40 industries and all business functions — underpinned by the world’s largest delivery network — Accenture works at the intersection of business and technology to help clients improve their performance and create sustainable value for their stakeholders. With 469,000 people serving clients in more than 120 countries, Accenture drives innovation to improve the way the world works and lives. Visit us at www.accenture.com.	82b02bab68b5d127	Toronto, ON	Azure/AWS Data Engineering Manager	2020-08-18T11:03:57.000Z	Accenture
2020081879	https://ca.indeed.com/rc/clk?jk=fb3fb7f802f5a80a&fccid=d76467c879ed211c&vjs=3	About Etraveli Group
Etraveli is one of the leading global flight centric Online Travel Agencies (OTAs) with €4bn+in annual gross sales. We also operate flygresor.se, the #1 metasearcher in Sweden and Tripstack, the independent B2B arm of the group offering a variety of complex technology solutions.
Our diverse, dynamically growing team of 1000+ talented professionals is always on the lookout for more members to join our ranks and explore unlimited business opportunities together! Our 110 websites in 70+ countries across the globe include (but are not limited to) gotogate.com, pamediakopes.gr, mytrip.com, flightnetwork, supersavertravel.se, trip.ru & flygresor.se.
About Tripstack
TripStack is a brand under the Etraveli Group, It is revolutionizing the travel & technology field with a mission to challenge the status quo by solving difficult problems and changing the way millions of people travel.
We offer unique flight content, consisting of Virtual Interline & Low-Cost carriers in one search - this enables our partners to offer the end consumer the most relevant flight options at the lowest price.
Our company’s core values are what form our foundation. They inform how we work, how we execute, how we choose our future teammates and how we present ourselves to the world as TripStack employees.


Playing to Win.
#1 within flights
At TripStack we play as a team, we win as a team and we constantly aim to become better. We challenge convention, ourself and our teammates. We dare to think big and we value those who think differently.

Drive for Excellence

Good is not good enough.
We show a never give up attitude and we move fast, knowing we sometimes make mistakes, but that is needed to keep a high pace and to solve complicated stuff. And what we do is complicated, but by driving for high quality & excellence we simplify the product offering, enabling the end user more and cheaper ways to travel.


Act with Ownership

Have pride in your work
At TripStack we expect all to take accountability for your work but also accountability for the team work. We value proactivity, employees who take initiative to get things done- also when it is out of normal scope - it help us achieve our goals.
TripStack is part of the Etraveli Group, a global online travel agency and the fastest growing in Europe whose presence across the web spans 50 countries.
The Role
We are looking for an experienced data engineer to join our Virtual Interlining team for TripStack. Virtual Interlining is a technology we provide that combines flights from different carriers that don’t traditionally work together to go from point A to B via C. These unique fares provide significantly lower prices to the end consumer and much higher flight margins to our partners. Did you know that there are nearly 45 million flights operated worldwide on an annual basis? Indexing that data to provide customers with better flight options is a daunting and exciting mission.

You will use various methods to transform raw data into useful data systems. For example, you will create algorithms and conduct statistical analysis. Overall, you will strive for efficiency by aligning data systems with business goals.
To succeed in this role, you should have strong analytical and programming skills and the ability to combine data from different sources. Data engineer skills also include familiarity with several programming languages (preferably golang, python) and knowledge of machine learning methods would be considered an asset. You should have a good understanding of data warehousing concepts, having worked with large datasets and designed star/snowflake schemas. Experience with data processing tools like Apache Hadoop, Spark, Apache Druid( real-time OLAP) would be considered an asset.

Responsibilities:

Analyze and organize raw data
Build data systems and pipelines
Evaluate business needs and objectives
Interpret trends and patterns
Conduct data analysis and report on results
Prepare data for prescriptive and predictive modeling
Build prototypes
Combine raw information from different sources- flat files, databases, NoSQL caches
Explore ways to enhance data quality and reliability
Identify opportunities for data acquisition and prototype them using machine learning
Collaborate with Data Scientists and Architects on several projects
Requirements
Bachelor’s degree in Computer Science or technical discipline required. Master’s degree not mandatory but would be considered an asset.
4 years previous experience as a data engineer or in a similar role
Technical expertise with data models, data mining, and segmentation techniques
Knowledge of programming languages (Golang, Java, and Python)
Hands-on experience with SQL database design and proficient at writing SQL queries specifically Postgres
Knowledge of Apache Hadoop, Spark an asset
Knowledge of real-time OLAP like Apache Druid is an asset
Exceptional numerical and analytical skills	Etraveli Group	Toronto, ON	2020-08-18T11:03:58.000Z	Data Engineer	fb3fb7f802f5a80a
2020081880	https://ca.indeed.com/rc/clk?jk=1aca9ca80ec23fec&fccid=1f9d0530a51ff611&vjs=3	1aca9ca80ec23fec	Expert BI/ETL Engineer (Tech Lead Cloud Data Engineer)	Mississauga, ON	2020-08-18T11:03:58.000Z	What will you contribute?
Reporting to the Senior Manager, Development, the role of the Expert BI Developer is to ensure the effective design and delivery of the Student Lending reporting solution. This hands-on role serves as a Technical Lead for the Business Analytics and Reporting team providing development, technical guidance, review and support.
Responsibilities & Deliverables:

Your deliverables as an Expert BI Developer will include, but are not limited to, the following:
Develop and deliver a robust Reporting and Business Analytics framework, well aligned with the company’s long-term strategic goals for data architecture vision.
Ensure the solution supports Student Lending client data needs and can be easily extended to newly acquired clients and their standards.
Liaise with vendors and service providers to select the products or services that best meet company cost and performance goals related to data architecture and analytics
Working closely with both enterprise level and project level team - data owners, stewards, users, business analysts, developers, quality analysts, department managers, architects and other stakeholders to understand reporting requirements and ensure strategic goals and tactical implementation are in alignment.
Translate project requirements into functional and non-functional specifications for BI reports and applications.
Lead conceptual and physical design, development and implementation of enterprise level BI and ETL framework, conforming to well defined business, technical rules and SLAs, preserving reusability of artefacts, single version of truth, centralization of logic, testability and well-designed error handling.
Prepare all necessary documentation that clearly describes solution and Meta data.
Monitor, tune up and administer BI Environments for quality and optimal performance purpose. Debug, monitor and troubleshoot BI solutions.
Be aware of and comply with all corporate and department policies, procedures and standards that apply to your work area.
Ensure that Reporting and Business Analytics strategies and architectures are in regulatory compliance .
Required Skills & Experience:
8+ years' of hands-on experience developing BI and Reporting Solutions.
Experience with business requirements analysis, entity relationship planning, data modeling, database design, reporting structures.
Direct experience in implementing enterprise data management processes, procedures, and support on data monitoring.
Understanding of large scale DB and reporting solution design, Source to Target Mappings, distributed DB design, multi environment structures, logical DB partitioning strategy, data archiving and retention, design and development of reporting semantic layer and view objects and logic
Expert knowledge of MS SQL
Expert hands-on experience with Azure Cloud Data Engineering suite: ADF, Databricks, Azure Data Lake, Spark, Azure SQL and Azure SQL Data Warehouse
Experience with DAX, Tabular and Power BI.
Experience with data processing flowcharting techniques.
Experience developing and maintaining ETL tools and platforms such as SSIS, Azure ADF
Experience with data architecting, large-scale data modeling, and business requirements gathering/analysis.
Strong understanding of BI Reporting & ETL technologies, relational and dimensional data structures, Big Data hands-on experience, principles, and best practices.
Strong familiarity with metadata management and associated processes.
Demonstrated expertise with repository creation, and data and information system life cycle methodologies.
Understanding of Web services (SOAP, XML, REST, JSON, UDDI).
Good knowledge of applicable data privacy practices and laws.
#LI-MG1
*************************************************************************************************************
The above statements describe the general nature and level of work being performed by people assigned to this job. They are not intended to be an exhaustive list of all responsibilities, duties, and skills required. Reasonable accommodations may be made to enable qualified individuals with disabilities to perform the essential job functions. If you need assistance or an accommodation due to disability please contact your recruitment partner.
*************************************************************************************************************	Finastra
2020081881	https://ca.indeed.com/rc/clk?jk=38794e819475c388&fccid=3002307a9e5b4706&vjs=3	Senior Manager, Customer Insights - Analytics	Scotiabank	38794e819475c388	Requisition ID: 88492

Join the Global Community of Scotiabankers to help customers become better off.

Position:
The Senior Manager role in the Advanced Analytics team is designed for individuals with a curiosity for deriving insights out of data and applying them to address business opportunities, in partnership with global teams and business lines driving innovation and advanced analytics across the Enterprise.

The Senior Manager contributes to the development and refinement of business strategies and programs of enterprise banking by providing data-driven insights to the decision makers. This individual is responsible for compiling, aggregating, testing and validating different hypotheses from business lines and analytics teams and converting them into practical business intelligence and recommendations to identify and seize new business opportunities. She/he also manages analytical tools with both high-level dashboard and detailed drilldown analysis functionalities for various groups of internal users.

The Senior Manager will work closely with key internal stakeholders as well as various Strategy, Finance and Analytics departments on all matters related to customer, product and functions portfolio to drive growth aligned to the data and analytics strategic priorities. They will require a combination of business focus, strong analytical and problem-solving skills and programming knowledge to quickly draw insights from large, disparate data sources to support Bank strategies.
WHAT’S IN IT FOR YOU?
Opportunity to make an impact in the transformation of Scotiabank
Exposure to global teams and business lines where analytics techniques are being applied
Hands-on and end to end practical projects which provide an opportunity to gain new knowledge and develop skills
A compensation program with competitive salary, opportunities for annual performance incentives based on performance thresholds, a competitive benefits program and continuing education programs

Requirements & Qualifications:
5+ years of related experience in strategy, data analytics, financial services or related field
Strong knowledge of the banking industry is a must
Post-secondary degree in Business, Computer Science, Engineering or equivalent. MBA will be an asset
Experience cleaning, transforming and visualizing large data sets working with various data formats (e.g. unstructured logs, XML, JSON, flat files)
Hands-on experience with Big Data ecosystem tools (e.g. Hive, Pig, Sqoop and Spark) and strong skills for querying relational databases (e.g. SQL Server, DB2, MySQL, SAS)
Production experience with experimental design, statistical analysis, machine learning and predictive modeling (e.g. cross-sell, upsell, attrition, acquisition and lookalike models)
Experience with common machine Learning libraries in Python, R, Spark
Experience building, using and implementing visualization tools like PowerBI and Tableau
Strong collaboration skills with ability to translate technical knowledge into business value
Exceptional written and communication skills with ability to prepare presentations for Senior Management
Data Engineer and/or Dev Ops experience is a Plus

Accountabilities:
Work in an Agile environment to deploy new solutions
Perform self-initiated data mining and analysis to identify previously unknown trends, cause-effect relationships, and insights to support the development and refining of business strategies and programs
Will report newly-found business insights to Directors with recommendations on how to leverage these insights for the benefit of the Bank and driving growth strategy
Will Interview and work with Business Lines, Analytics and Enterprise Strategy and decision makers to gather BI reporting and analytics requirements; act as a subject matter expert for analytics and BI initiatives
Collaborate with business lines and other stakeholders and identify opportunities to drive business value by leveraging analytics
Develop models with Decision Sciences to validate business hypotheses
Acquire new data needed for new analytics initiatives working with Enterprise Data Management Office
Create and apply model and algorithm testing strategies to measure conduct multi-variate testing and A/B testing to measure effectiveness of models and make ongoing changes
Design, implement, and maintain dynamic dashboards as a tool for Executives and decision makers to get up-to-date information on the situation and changes in the business line and the industry
Will build analytics tools, create training materials, communicate and launch the results to key stakeholders
Continuously improve and manage the analytics / BI platform(s) by collecting and analyzing feedback and making recommendations
Liaise with Analytics, Enterprise Data Management Office, Enterprise Strategy to work on cross-departmental analytics initiatives
Support analytical use-case delivery, as well as customer/financial benefits tracking and communication
Coach a team of data scientist for analytics. This includes providing them guidance on how to work in agile environment, infuse analytics in building solutions and transforming the Bank’s digital capabilities, determine the analytics tools and technology that should be applied in each solution and coaching the team as they work with colleagues from across the organization.
Location(s): Canada : Ontario : Toronto
As Canada's International Bank, we are a diverse and global team. We speak more than 100 languages with backgrounds from more than 120 countries. Our employees are committed to a superior customer experience and use the Bank’s six guiding sales practice principles to ensure they act with honesty and integrity.

At Scotiabank, we value the unique skills and experiences each individual brings to the Bank, and are committed to creating and maintaining an inclusive and accessible environment for everyone. If you require accommodation (including, but not limited to, an accessible interview site, alternate format documents, ASL Interpreter, or Assistive Technology) during the recruitment and selection process, please let our Recruitment team know. If you require technical assistance, please click here. Candidates must apply directly online to be considered for this role. We thank all applicants for their interest in a career at Scotiabank; however, only those candidates who are selected for an interview will be contacted.	Toronto, ON	2020-08-18T11:03:59.000Z
2020081882	SADA	Regional Cloud Engineering Manager	2020-08-18T11:04:00.000Z	Toronto, ON	f9ab2bbaf19601e6	https://ca.indeed.com/rc/clk?jk=f9ab2bbaf19601e6&fccid=b704562e07a2a03f&vjs=3	Join SADA as a Regional Cloud Engineering Manager!

Your Mission

As a Regional Cloud Engineering Manager, you will manage a team of Cloud Engineers and work collaboratively with other architects and engineers to design, prototype and lead the deployment of scalable Google Cloud Platform (GCP) architectures. You will work with engineering teams, customers and sales teams to qualify potential engagements, craft robust architectural proposals, and help craft Statements of Work (SOWs) that engineering teams can successfully execute. You're also hands-on, able to conduct experiments and build functioning prototypes that prove out ideas and build confidence in the solutions you advocate.

You will be a recognized expert within SADA and will develop a reputation with customers as well as the Google Cloud sales and professional services organizations for the quality of your work. You will also lead early-stage opportunity technical qualification calls, as well as lead client-facing technical discussions. You'll enable your team to unblock clients from adopting GCP in accordance with best practices, ensuring client satisfaction via technical excellence.

You will also be the face of SADA sales engineering for the region. This means building relationships with our clients, engineers, sellers, and partners to ensure a proactive approach to serving their needs. You'll be expected to provide guidance to each of these groups as well as to present strategies and overviews to our internal audience. You'll be the senior most technical resource during the sales process for our most important clients and will be expected to present complex technical solutions to our clients' business stakeholders.

In addition, you'll be presenting the work you and your team accomplish via case studies, webinars, speaking roles at conferences, etc.


Pathway to Success

#BeAChangeAgent: You are a rainmaker! You are way out in front of our delivery organization, meeting with the spectrum of corporate and enterprise customers that need our consultative services. You have your finger on the pulse of their technical needs and take pride in helping them solve their real-world problems on GCP.

You will be measured quarterly by a combination of (a) the volume of GCP revenue your team generates and (b) the satisfaction of our clients and sales teams.


Expectations

Required Travel - 30% travel to customer sites, conferences, and other related events. Due to the COVID-19 pandemic, travel has been temporarily restricted.

Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives

Training - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.


Job Requirements

Required Credentials:

Google Professional Cloud Architect Certified and/or Google Professional Data Engineer Certified, or able to complete one of the above within the first 45 days of employment.

Required Qualifications:

Mastery in at least one of the following domain areas as well as general expertise across all:
Infrastructure Modernization: migrating n-tiered workloads from on-prem and other clouds to GCP with near-zero-downtime (includes full spectrum of lift and shift to complete re-platforming scenarios), or building hybrid-cloud solutions based on Anthos and containerization technologies, such as Docker, Kubernetes, and Istio
Application Development: building custom web and mobile applications on top of the GCP stack
Data Engineering: data warehouse modernization (including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines and reporting/analytic tools), OLTP/OLAP data migrations, or backup, restore and disaster recovery solutions.
Experience providing oversight and direction of cloud projects
Experience leading technical design sessions, architecting and documenting technical solutions that are aligned with client business objectives, and identifying gaps between the client's current and desired end states
Experience strategizing, designing, architecting and leading the deployment of scalable solutions on GCP
Experience across multiple cloud platforms: GCP, AWS, Azure
Experience with container engines: Kubernetes, Docker, AWS Elastic Container Service
Experience with automation technologies including Terraform, Google Cloud Deployment Manager, AWS Cloud Formation or Microsoft Azure Automation
Experience working with engineering and sales teams to elicit customer requirements
Ability to communicate across business units and the ability to interface with and communicate complex technical concepts to a broad range of internal and external stakeholders
Time management skills with the ability to manage multiple streams and lead less experienced architects
Experience as a technical consultant or another customer-facing technical role

Useful Qualifications:

Experience managing teams of engineers
Hands-on experience designing and recommending elegant solutions that drive business outcomes
Experience building, designing and migrating complex cloud architectures
Strong aptitude for learning new technologies and techniques with a willingness and capability to skill up the team
A constant desire to learn
Ability to lead an in-depth client meeting/workshop across a broad range of topics including discovery, cloud compliance, and security
Deep understanding of best practices, design patterns, reference and compliance architectures with an uncanny ability to build and recommend these as needed
Knowledge and understanding of industry trends, new technologies and the ability to apply these to customer architectures to drive outcomes
Highly self-motivated and able to work independently as well as leading a team to success.

About SADA

Values: We built our core values on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA's values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.
Make them rave
Be data driven
Be one step ahead
Be a change agent
Do the right thing
Work with the best: SADA has been the largest partner in North America for GCP since 2016 and has been named the 2019 and 2018 Google Cloud Global Partner of the Year. SADA has also been awarded Best Place to Work by Inc. as well as LA Business Journal!

Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match, professional development reimbursement program as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.
2020081883	SADA	2020-08-18T11:04:01.000Z	https://ca.indeed.com/rc/clk?jk=2fef127f6d4b3e92&fccid=b704562e07a2a03f&vjs=3	Senior Pre-Sales Cloud Engineer	Join SADA as a Senior Pre-Sales Cloud Engineer!

Your Mission

As a Senior Pre-Sales Cloud Engineer at SADA, you will work collaboratively with other architects and engineers to design, prototype and lead the deployment of scalable Google Cloud Platform (GCP) architectures. You will work with engineering teams, customers and sales teams to qualify potential engagements, craft robust architectural proposals, and deliver Statements of Work (SOWs) that engineering teams can successfully execute. You're also hands-on, able to conduct experiments and build functioning prototypes that prove out ideas and build confidence in the solutions you advocate.

You will be a recognized expert within SADA and will develop a reputation with customers as well as the Google Cloud sales and professional services organizations for the quality of your work. You will demonstrate repeated delivery of project architectures that other engineers and architects demur to you for lack of expertise. You will also lead early-stage opportunity technical qualification calls, as well as lead client-facing technical discussions.

Pathway to Success

#BeAChangeAgent: You are a rainmaker! You are way out in front of our delivery organization, meeting with the spectrum of corporate and enterprise customers that need our consultative services. You have your finger on the pulse of their technical needs and take pride in helping them solve their real-world problems on GCP.

You will be measured quarterly by a combination of (a) the volume of signed SOWs that you shepherd through the sales funnel, and (b) the level of customer satisfaction measured at the end of each engagement.

As you continue to execute successfully, we will build a customized development plan together that leads you through the solution architecture or management growth tracks.

Expectations

Required Travel - 30% travel to customer sites, conferences, and other related events. Due to the COVID-19 pandemic, travel has been temporarily restricted.
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives
Training - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:
Google Professional Cloud Architect Certified and/or Google Professional Data Engineer Certified, or able to complete one of the above within the first 45 days of employment.
Required Qualifications:
Mastery in at least one of the following domain areas:
Infrastructure Modernization: migrating n-tiered workloads from on-prem and other clouds to GCP with near-zero-downtime (includes full spectrum of lift and shift to complete re-platforming scenarios), or building hybrid-cloud solutions based on Anthos and containerization technologies, such as Docker, Kubernetes, and Istio
Application Development: building custom web and mobile applications on top of the GCP stack
Data Engineering: data warehouse modernization (including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines and reporting/analytic tools), OLTP/OLAP data migrations, or backup, restore and disaster recovery solutions.
Experience providing oversight and direction of cloud projects
Experience leading technical design sessions, architecting and documenting technical solutions that are aligned with client business objectives, and identifying gaps between the client's current and desired end states
Experience strategizing, designing, architecting and leading the deployment of scalable solutions on GCP
Experience across multiple cloud platforms: GCP, AWS, Azure
Experience with container engines: Kubernetes, Docker, AWS Elastic Container Service
Experience with automation technologies including Terraform, Google Cloud Deployment Manager, AWS Cloud Formation or Microsoft Azure Automation
Experience working with engineering and sales teams to elicit customer requirements
Ability to communicate across business units and the ability to interface with and communicate complex technical concepts to a broad range of internal and external stakeholders
Time management skills with the ability to manage multiple streams and lead less experienced architects
Experience as a technical consultant or another customer-facing technical role
Useful Qualifications:
Hands-on experience designing and recommending elegant solutions that drive business outcomes
Experience building, designing and migrating complex cloud architectures
Strong aptitude for learning new technologies and techniques with a willingness and capability to skill up the team
Ability to lead an in-depth client meeting/workshop across a broad range of topics including discovery, cloud compliance, and security
Deep understanding of best practices, design patterns, reference and compliance architectures with an uncanny ability to build and recommend these as needed
Knowledge and understanding of industry trends, new technologies and the ability to apply these to customer architectures to drive outcomes
Highly self-motivated and able to work independently as well as in a team environment

About SADA

Values: We built our core values on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA's values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

Make them rave
Be data driven
Be one step ahead
Be a change agent
Do the right thing

Work with the best: SADA has been the largest partner in North America for GCP since 2016 and has been named the 2019 and 2018 Google Cloud Global Partner of the Year. SADA has also been awarded Best Place to Work by Inc. as well as LA Business Journal!

Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match, professional development reimbursement program as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.	2fef127f6d4b3e92	Toronto, ON
2020081884	SADA	40aaab8c4c4a4804	2020-08-18T11:04:01.000Z	https://ca.indeed.com/rc/clk?jk=40aaab8c4c4a4804&fccid=b704562e07a2a03f&vjs=3	Technical Account Manager, Google Cloud Platform	Toronto, ON	Join SADA as a Technical Account Manager (TAM)!

Our technical competency is what sets SADA apart from other partners. We are transparent in our approach, celebrate our diverse workforce, and strengthen our competencies through training and development. As a TAM at SADA, you will collaborate with some of the most innovative organizations in their pursuit of a cloud-first approach.

This position requires a collaborative stakeholder with experience in infrastructure design, implementation, data engineering, and support. They must be able to architect solutions which include multi-cloud and hybrid-cloud scenarios. (S)he must be consultative, while anticipating potential needs and liabilities around growth and diversification. The ideal candidate is able to understand objectives/watchpoints and apply Google Cloud technology to solve for such outcomes.
If you like the idea of thinking strategically to help clients succeed with Google's cutting-edge solutions, apply now!

Accountabilities:
Provide robust and scalable technology solutions to enable our customers to scale their operations into GCP.
Advise customers on technology standards, methodologies and processes as they relate to infrastructure, application architecture, and data engineering.
Design and develop infrastructure blueprints for the implementation of new solutions that bring customers storage and compute workloads from cloud and non-cloud environments to GCP.
Participate in proof of concept development to assist in defining technology direction for our customers.
Conduct regular touchpoints with clients to review their cloud strategy and provide updates on best practices and new products.
Build solutions which leverage novel approaches to existing business and technology challenges.
Qualifications & Previous Experience

Must have:
Systems Engineering, System Administration, or Systems Architecture experience
Strong working knowledge of cloud offerings and solutions (Google Cloud Platform, Microsoft Azure, Amazon AWS)
Deep understanding of TCP, IP and other network protocols
Familiarity with DNS, DHCP and other network services
Experience administering a variety of Linux distributions
Experience with information security practices and procedures
Strong working knowledge of VMware, Hyper-V, KVM, or other virtual software
Ability to define infrastructure as code using tools like Google Deployment Manager, Terraform, Chef, etc
Strong scripting abilities via BASH, Python, etc
Knowledge of network topology and associated technologies
Mature understanding of DevOps best practices for cloud-native build and release pipelines
Strong technical aptitude and the ability to digest advanced technical topologies and concepts
Preferable:
A Bachelor of Science Degree in Computer Science or equivalent experience
Ability to write architectural design documents or review design documents provided by others
Knowledge of monitoring systems, capacity planning, and performance optimization across a variety of technologies (such as traditional compute, serverless, and data systems)
Knowledge and understanding of industry trends and new technologies and ability to apply trends to architectural needs
Full understanding of compute theory from hardware through serverless abstraction
Experience working with containerization technologies (Kubernetes, Docker, etc)
Certifications (strongly preferred, any of the below):
Google Certified Professional Cloud Architect
Google Certified Professional Data Engineer
AWS Certified Solutions Architect - Professional
AWS Certified DevOps Engineer - Professional


About SADA Systems, Inc

Values: We built our core values on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA's values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

Make them rave
Be data driven
Be one step ahead
Be a change agent
Do the right thing

Work with the best: SADA has been the largest partner in North America for GCP since 2016 and has been named the 2019 and 2018 Google Cloud Global Partner of the Year. SADA has also been awarded Best Place to Work by Inc. as well as LA Business Journal!

Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match, professional development reimbursement program as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.
2020081885	https://ca.indeed.com/rc/clk?jk=00df245dd038b055&fccid=87852d99a20def3f&vjs=3	Fiix	Why Fiix?
Fiix has a big goal – to create a more sustainable world. Sounds lofty right? Our mission is to make every maintenance team successful by enabling the adoption of a CMMS and we’re off to a great start. Teams that are part of the world’s most well known brands (like Toyota, Siemens, and Sara Lee) manage their maintenance activities and achieve greater results with Fiix. But we’re not stopping there. Our team is growing by leaps and bounds and we’re conquering new challenges every day. We’re looking for big thinkers with small egos to join us on our journey to create a more sustainable world.

Why we do it?
We’re a team of market disrupting, like-minded individuals. We all do things our own way, but we come together each and every day to create and deliver the long awaited answer to an antiquated industry – and we have a lot of fun while we’re at it.

We’re looking for a Data Engineer to help take Fiix’s explosive growth to a whole new level. We are looking for an experienced Data Engineer to build our data layer to support the delivery of machine learning-driven products. This is a unique opportunity to join a team of creative and passionate individuals committed to bringing AI to the predictive maintenance world.

What You Will Do:
Work with software engineering to design, build, maintain and optimize our data management and analytics pipeline
Perform data analysis, quality assessments, cleaning, imputation and data aggregation tasks
Perform feature engineering and selection to support machine learning activities
Build data processing pipelines and automate data pipelines in production environments
Work with engineers and data scientists to deploy analytics capabilities and machine learning models in production
Work with engineering and data teams to ingest and structure high throughput IoT data
Develop processes and frameworks to ensure data and model quality
Perform code reviews and testing to ensure software quality is high and requirements are met
Diagnose and repair data issues and assist customers with technical problems
What We're Looking for:
3+ years experience in a high growth software development environment developing data-driven products
Experience working on ETL, data warehousing, data modeling, data architecting, data analysis
Experience with at least some of: 1) Data streaming with Kinesis, Kafka or similar 2) ETL orchestration frameworks such as Airflow, Luigi or similar 3) Data warehouses such as Snowflake or similar
Development skills in Python, MySQL and other relational databases, NoSQL databases such as DynamoDB, Redis or similar
Experience with AWS or other cloud providerEducation background:
Bachelor’s Degree or higher in Computer Science or a related field
Equity Statement

At Fiix, we recognize that people come with a wealth of experience and talent beyond just the technical requirements of a job. If your experience is close to what you see listed here, please still consider applying. Diversity of experience and skills combined with passion is a key to innovation and excellence. Therefore, we encourage people from all backgrounds to apply to our positions. Please let us know if you require accommodations during the interview process.	Data Engineer - Applied AI	2020-08-18T11:04:03.000Z	Toronto, ON	00df245dd038b055
2020081886	1f6d9f1ab7c60335	The Senior Big Data Engineer DevOps role reports to the Director, Data Sciences DevOps. The role is part of the DevOps team in charge of the daily operations of various integration technologies. The role is responsible for supporting workloads running on the Hadoop environment and associated technologies. This position will be responsible for monitoring the batch jobs, resolving incidents, optimizing workloads, tuning jobs, and making job enhancements.

The role will focus on production support and will also take part in the DevOps rotation for making enhancements. Also, the role will be involved in R&D of emerging technologies with the application administrators and technical architects.

Strategic:
Evaluate tools and technologies in the context of the future state architecture, and evolving business requirements
Responsible for review of project artifacts during the transition phase and ensure operational needs are met
Research & development about new Hadoop & Analytical technologies
Review solution and technical designs
Propose best practices/standards
Benchmark the performance in line with the non-functional requirements
Previous experience in developing and deploying operational procedures, tuning guides and best practices documentation
Attention to detail to review project deliverables for completeness, quality, and compliance with established project standards

Candidate Profile:
3+ years developing and supporting applications leveraging the Hadoop stack
3+ of experience with data integration/ETL tools such as DataStage or alternatives
SDLC knowledge in both waterfall and agile methodologies
Hands-on experience with source code management system (SVN, Git) and continuous integration tools (Jenkins)
Experience on following tools: Hive, SQL, Spark, Kafka, Flume, Sqoop, HBase ,Pig, HDFS, R, NoSQL
Experience on handling data processing, delivering distributed and highly scalable application
Experience with HortonWorks Hadoop Distribution
Experience with large scale domain or enterprise solution analysis development, selection and implementation
Experience with high-volume, transaction processing software applications
Good understanding of workload management, schedulers, scalability and distributed platform architectures
Experience in software development and architecture experience using Java EE technologies (Application Server, Enterprise Service Bus, SOA, Messaging, Data Access Layers)
Experience in scripting languages & automation such as bash, PERL, and Python
Experience in data warehousing, analytics, and business intelligence/visualization/presentation
Experience using SQL against relational databases
Working knowledge of search technologies like Lucene, Solr
5+ years of hands-on experience on Linux, AIX, and z/OS
Professional Skill Requirements
Excellent communication skills (both written and oral) combined with strong interpersonal skills
Strong analytical skills and thought processes combined with the ability to be flexible and work analytically in a problem-solving environment
Attention to detail
Strong organizational & multi-tasking skills
DISCLAIMER:
Acosta/Mosaic North America is an Equal Opportunity Employer
The above statements are intended to describe the general nature and level of work being performed by people assigned to this classification. They are not intended to be construed as an exhaustive list of all responsibilities, duties, and skills required of personnel so classified. Mosaic reserves the right to modify all or part of any job descriptions at its discretion in order to meet and or exceed the needs of the business.
We are committed to providing accommodations for persons with disabilities. If you require accommodation, we will work with you to meet your needs, to the extent required by law.

Qualifications


Primary Location: CA-ON-Mississauga
Work Locations: Acosta-Mosaic Mississauga Corporate Office 2700 Matheson Blvd. E. W. Tower 2nd Floor Mississauga L4W 4V9
Job: Information Technology
Organization: CoE - Client Information Services - Canada
Shift: Standard
Job Type: Full-time
 Day Job
Job Posting: Aug 4, 2020, 10:54:13 AM	Senior Big Data Engineer	2020-08-18T11:04:03.000Z	Mississauga, ON	https://ca.indeed.com/rc/clk?jk=1f6d9f1ab7c60335&fccid=1c4aa3d5a92746d4&vjs=3	Mosaic North America
2020081887		2020-08-18T11:04:04.000Z	Big Data Developer	668db5396375205e	https://ca.indeed.com/rc/clk?jk=668db5396375205e&fccid=d2841a5c0380b93d&vjs=3	Toronto, ON	CGI
2020081888	SADA	https://ca.indeed.com/rc/clk?jk=5c06518576e750fc&fccid=b704562e07a2a03f&vjs=3	2020-08-18T11:04:07.000Z	Senior Data Engineer	Toronto, ON	Join SADA as a Sr. Data Engineer!


Your Mission

As a Sr. Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data problems facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring batch or streaming data pipelines, data lakes and data warehouses.

You will be expected to run point on whole projects, end-to-end, and to mentor less experienced Data Engineers. You will be recognized as an expert within the team and will build a reputation with Google and our customers. You will demonstrate repeated delivery of project architectures and critical components that other engineers demur to you for lack of expertise. You will also participate in early-stage opportunity qualification calls, as well as lead client-facing technical discussions for established projects.

Pathway to Success

#BeOneStepAhead: At SADA we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our engineers know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs.

Your success starts by positively impacting the direction of a fast-growing practice with vision and passion. You will be measured quarterly by the breadth, magnitude, and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, how well you collaborate with your peers, and the consultative polish you bring to customer interactions.

As you continue to execute successfully, we will build a customized development plan together that leads you through the engineering or management growth tracks.

Expectations

Required Travel - 30% travel to customer sites, conferences, and other related events. Due to the COVID-19 pandemic, travel has been temporarily restricted.
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:
Google Professional Data Engineer Certified or able to complete within the first 45 days of employment

Required Qualifications:
Mastery in at least one of the following domain areas:
Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or customer-facing role

Useful Qualifications:
Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Experience operationalizing machine learning models on large datasets
Demonstrated leadership and self-direction - a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem

About SADA

Values: We built our core values on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA's values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

Make them rave
Be data driven
Be one step ahead
Be a change agent
Do the right thing

Work with the best: SADA has been the largest partner in North America for GCP since 2016 and has been named the 2019 and 2018 Google Cloud Global Partner of the Year. SADA has also been awarded Best Place to Work by Inc. as well as LA Business Journal!

Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match, professional development reimbursement program as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.	5c06518576e750fc
2020081889	Headquartered in the Bay Area with offices in Austin, TX, Toronto, Canada and Jaipur, India, venture-funded Punchh is the world leader in innovative digital marketing products for brick and mortar retailers, combining AI technologies, mobile-first expertise, and Omni-Channel communications designed to dramatically increase customer lifetime value. Leading global chains in the restaurant, health and beauty sectors rely on Punchh to grow revenue by building customer relationships at every stage to becoming brand loyalists, including more than 100 different chains representing more than $12B in annual spend, 30,000 locations globally, 26M+ consumers, and 1M+ transactions daily. Punchh boasts a customer list that includes Pizza Hut, Quiznos, Coffee Bean & Tea Leaf and many more.

Title

Senior/Lead Big Data Engineer

Location

Toronto, Ontario, Canada

Reporting to

Sr. Dir. of Data

About the role

Punchh is looking for a Senior/Lead Big Data Engineer that will play a critical role in leading Punchh's data innovations. He/she will help create cutting-edge Big Data solutions by leveraging his/her prior industrial experience. This role requires close collaboration with the Machine Learning, Software Engineering, and Product Departments. You will be given the opportunity to not only serve internal teams, but also our business clients as well.

What You'll Do

Punchh is seeking to hire Big Data Engineer at either a senior or tech lead level. Reporting to the Director of Big Data, he/she will play a critical role in leading Punchh's big data innovations. By leveraging prior industrial experience in big data, he/she will help create cutting-edge data and analytics products for Punchh's business partners.

This role requires close collaborations with data, engineering, and product organizations. His/her job functions include

Work with large data sets and implement sophisticated data pipelines with both structured and structured data.
Collaborate with stakeholders to design scalable solutions.
Manage and optimize our internal data pipeline that supports marketing, customer success and data science to name a few.
A technical leader of Punchh's big data platform that supports AI and BI products.
Work with infra and operations team to monitor and optimize existing infrastructure
Occasional business travels are required.

What You'll Need

5+ years of experience as a Big Data engineering professional, developing scalable big data solutions.
Advanced degree in computer science, engineering or other related fields.
Demonstrated strength in data modelling, data warehousing and SQL.
Extensive knowledge with cloud technologies, e.g. AWS and Azure.
Excellent software engineering background. High familiarity with software development life cycle. Familiarity with GitHub/Airflow.
Advanced knowledge of big data technologies, such as programming language (Python, Java), relational (Postgres, mysql), NoSQL (Mongodb), Hadoop (EMR) and streaming (Kafka, Spark).
Strong problem solving skills with demonstrated rigor in building and maintaining a complex data pipeline.
Exceptional communication skills and ability to articulate a complex concept with thoughtful, actionable recommendations.

Benefits

Healthcare coverage
Life and AD&D insurance
Competitive salaries, bonus and stock options
Professional development
Paid Time off
Paid holidays
Free lunch in the office.

Punchh is proud to provide equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. If you'd like more information about your EEO rights as an applicant, please click here.

We also provide reasonable accommodations to individuals with disabilities in accordance with applicable laws.
Notice to recruiters and placement agencies: If you are a recruiter or placement agency, please do not submit résumés to any person or email address at Punchh prior to having a signed agreement with Human Resources. Punchh is not liable for and will not pay placement fees for candidates submitted by any agency other than its approved recruitment partners. Also, any résumés sent to us without an agreement in place will be considered your company's gift to Punchh and may be forwarded to our Talent Acquisition team.	0a0e41fb84eb4708	Punchh	Toronto, ON	https://ca.indeed.com/rc/clk?jk=0a0e41fb84eb4708&fccid=e1b2607798446d2b&vjs=3	2020-08-18T11:04:08.000Z	Senior/Lead Big Data Engineer
2020081890	The Data Engineer will be responsible for expanding and optimizing the data and data pipeline architecture across the enterprise. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, data architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing Express Script Canada’s data architecture to support our next generation of products and data initiatives.

ESSENTIAL FUNCTIONS:
Create and maintain optimal data pipeline architecture.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Optimize the value of technology investments in data management for the business by aligning the business architectures with technology architectures. Identify and drive key technology investments to meet business objectives, remediate technology and process gaps. Determine feasibility, cost and time required, compatibility with current system, and system capabilities.
Identify product, technology and process gaps in current data & technology architectures and recommend solutions to bridge gaps between the business and the data & technology deployed to support the business.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Coordinate with data users and key stakeholders across ESC’s Lines of Business to refine and achieve various long-term objectives for data architecture
Work with stakeholders including Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product.
Work with data and analytics experts to strive for greater functionality in our data systems.

QUALIFICATIONS:
Minimum 6 years of experience in a large-scale, multi-platform, multi-tier processing environment with a Bachelor’s degree in Information Systems or related field
Extensive experience on projects implementing Master Data Management and Enterprise Content Management techniques and platforms.
Expert domain knowledge & experience in data warehousing, reporting and advanced analytics platforms, encompassing data model design, dimensional modeling, master data management, cross-cutting concerns, common integration technologies, patterns & standards and emerging technologies.
Advanced knowledge of data architecture and analytics principles and practices for application development and data warehousing purposes.
Advanced knowledge of data model development and governance methods and practices
Previous data management experience in a data warehouse and data lake environment.
Experience with data center migrations, enterprise database consolidation, data warehouse migration and/or consolidation to data lakes or integrated data architectures.
Experience building multi-cloud analytics and technology strategies.
Highly proficient in Relational Database Management Systems
(RDBMS) and Big Data, data architecture, data modeling (including dimensional),
data warehousing, object-oriented methodologies, and client/server development.
Advanced knowledge of PowerDesigner or similar Data Modelling Tool
Advanced knowledge of Talend or similar Data Integration Tool
Hands-on experience using data technologies to implement data profiling tools and modern database implementations including Enterprise Data Lakes, graphDB, key-value pair, column-store, Big Table, RDF, In-Memory DB, etc..
Working knowledge of Hadoop platforms (Hortonworks or Cloudera) and key technologies like Apache Nifi, Kafka, Spark, Pig, Hive, NoSQL databases like MongoDB, Cassandra or Hbase.
Experience implementing data visualization tools like Tableau, Qlik, etc.
Willingness to work a flexible schedule to accommodate project deadlines and travel requirements

Assets:
Knowledge of the group health insurance (pharmacy, dental, other health) industry or adjudication systems is an asset
Knowledge of advanced analytics tools like R, SAS, and machine learning algorithms.
Knowledge of one or more programming or scripting languages like Java, C, C#, .Net, Javascript, PHP, Python
Knowledge of the DAMA Book of Knowledge
Knowledge of Big Data & Logical Warehouse architecture
Knowledge of TOGAF, Zachman or other
architecture frameworks

ABOUT EXPRESS SCRIPTS CANADA

Express Scripts Canada, a registered business name of ESI Canada, an Ontario partnership indirectly controlled by Express Scripts, Inc. (Nasdaq: ESRX), is one of Canada’s leading providers of health benefits management services. From its corporate headquarters in Mississauga, Ontario, just outside Toronto, Express Scripts Canada provides a full range of integrated pharmacy benefit management (PBM) services to insurers, third-party administrators, plan sponsors and the public sector, including health-claims adjudication and processing services, Home Delivery Pharmacy Services, benefit-design consultation, drug-utilization review, formulary management, and medical and drug-data analysis services, to better facilitate the best possible health outcomes at the lowest possible cost.

It will be a condition of employment that the successful candidate receives the Enhanced Reliability Clearance from the Federal Government. The candidate will be required to provide supporting documentation in order to receive Clearance if required.

We offer a competitive salary, along with a positive work environment built on solid corporate values, integrity, mutual respect, collaboration, passion, service and alignment.

We are an equal opportunity employer that promotes a diverse, inclusive and accessible workplace. By embracing diversity, we build a more effective organization that empowers our employees to be the best that they can be.

We are committed to creating a working environment that is barrier-free and we are prepared to provide accommodation for people with disabilities. Thank you for your interest in this position, however only qualified candidates will be contacted for an interview. No telephone calls please.

For more information about Express Scripts Canada, visit its Web site at www.express-scripts.ca

About Cigna
Cigna Corporation (NYSE: CI) is a global health service company dedicated to improving the health, well-being and peace of mind of those we serve. We offer an integrated suite of health services through Cigna, Express Scripts, and our affiliates including medical, dental, behavioral health, pharmacy, vision, supplemental benefits, and other related products. Together, with our 74,000 employees worldwide, we aspire to transform health services, making them more affordable and accessible to millions. Through our unmatched expertise, bold action, fresh ideas and an unwavering commitment to patient-centered care, we are a force of health services innovation.

When you work with Cigna, you’ll enjoy meaningful career experiences that enrich people’s lives while working together to make the world a healthier place. What difference will you make? To see our culture in action, search #TeamCigna on Instagram.	Cigna	https://ca.indeed.com/rc/clk?jk=60d7283f8c769ec7&fccid=afbf8c270610a38a&vjs=3	Senior Data Engineer	Mississauga, ON	60d7283f8c769ec7	2020-08-18T11:04:08.000Z
2020081891	https://ca.indeed.com/rc/clk?jk=5dae399d2651bf86&fccid=353eb997fc901045&vjs=3	At Veeva, we build enterprise cloud technology that powers the biggest names in the pharmaceutical, biotech, consumer goods, chemical & cosmetics industries. Our customers make vaccines, life-saving medicines, and life-enhancing products that make a difference in everyday lives. Our technology has transformed these industries; enabling them to get critical products and services to market faster. Our core values, Do the Right Thing, Customer Success, Employee Success, and Speed, guide us as we make our customers more efficient and effective in everything they do.

The Role

Veeva Systems is looking for experienced data engineers to build a cloud-based data analytics solution for the life science industry. If you are passionate about data and are eager to design and build data platforms from the ground up this is the role for you. The data analytics platform will provide data ingestion, data storage and rich data analytics capabilities with elegant visualization dashboards.
What You'll Do
Design and implement AWS based ETL processes to onboard data into our data lake from a variety of internal and external sources for our new data analytics platform.
Design data models and data services for optimal storage and retrieval.
Implement scalable data lake interfaces, microservices, and rest based API for querying and storing structured data.
Integrate new technologies to support advanced analytic use cases.
Requirements
5+ years’ experience in Python or Java, preferably at an enterprise cloud software company
Proven ability to write clean, testable, readable code in a team environment
Hands-on experience with building data pipelines in a programming language like Java or Python
3+ years of experience in relational databases with a mastery of SQL
Experience in data modelling, ETL development (pref. Apache Spark), and Data warehousing
Nice to Have
AWS Services (S3, Redshift, Elastic Search)
Experience with large scale big data pipeline – ETL / Kafka / Spark / MapReduce / Hadoop
Familiarity with Open API Specifications and Swagger
Experience working in an agile environment
Experience working in a startup
Perks & Benefits
 Conveniently located in downtown Toronto Snacks, beverages, and weekly lunches from local restaurants Team events and rec league sports teams Allocations for continuous learning & development Health & wellness programs Weekly yoga classes Ping pong and other games

Veeva’s headquarters is located in the San Francisco Bay Area with offices in more than 15 countries around the world.

Veeva Systems is an equal opportunity employer. Accordingly, we are committed to fair and accessible employment practices. Veeva Systems welcomes and encourages applications from people with disabilities. Accommodations are available upon request for candidates taking part in all aspects of the selection process.	Senior Data Engineer	Toronto, ON	2020-08-18T11:04:10.000Z	5dae399d2651bf86	Veeva Systems
2020081892	If you are looking to join one of Canada’s fastest growing companies, goeasy Ltd. is the place for you! Recognized as one of Canada’s Most Admired Corporate Cultures, one of Canada’s Top 50 Fintech’s and one of North America’s Most Engaged Workplaces, we want the best and brightest to join our team.

We are a publicly traded company on the TSX with over 4000% shareholder return since 2001, goeasy operates two main business units. easyfinancial is our consumer lending business that offers secured and unsecured installment loans of up to $35,000 and easyhome is Canada’s largest merchandise lease-to-own company. It is our mission to provide everyday Canadians the chance for a better tomorrow, today by giving them access to the credit they need and by offering them a second chance when they have been turned down by banks and traditional lenders. With a retail network of nearly 400 locations across Canada and over 1900 employees, we are able to build lasting relationships with our customers as we help them rebuild their credit and graduate towards prime rates and a brighter financial future.

The Data Engineer on the Data Science and Business Insights team will build, integrate data from various resources, manage data in goeasy operational data store and enterprise data warehouse. This position will develop ETL (Extract, Transform and Load) with various tools on large datasets to ensure data is easily accessible, works smoothly, as well as maintain and expand the data warehouse for reporting and analysis. The Data Engineer will also work closely with the data architect on the design and architecture of our enterprise data warehouse.

Responsibilities:

Develop data set processes for data modeling, mining and production
Develop and maintain ETL processes using SSIS, Scripting and data replication technologies
Participate in development of datamarts for reports and data visualization solutions
Research opportunities for data acquisition and new uses for existing data
Integrate new data management technologies and software engineering tools into existing structures
Support the translation of business requirements for data acquisition/manipulation and provide detailed specifications that can be passed downstream for use
Develop detailed technical specifications and operational support documentation in collaboration with Business Systems Analysts, BI Engineers and Architects.
Identify and communicate technical problems, process and solutions
Create Ad-Hoc queries and reports as needed along with providing on-going analytical support for these requests
Assist in the collection and documentation of user’s requirements
Ensure that existing business processes dependent on the ODS/EDW are monitored and respond quickly to bug fixes, enhancement requests and production ETL related issues.
Dealing with the database users on a daily basis to ensure that problems are dealt with promptly and that appropriate fixes are made to resolve any problems.
Recommend ways to improve data reliability, efficiency and quality
Ensure systems meet business requirements and industry practices
Work effectively with the Business Intelligence and Data Solutions Architects, Data and BI Engineers to ensure that all approved development and deployment procedures are followed.

Qualifications:

Bachelor’s Degree in Computer Science, MIS, Computer Engineering or other Information Technology related degree
4+ years working with SQL Server or comparable relational database system
3+ years of extensive ETL development experience with SSIS and/or ADF
4+ years of experience troubleshooting within a Data Warehouse environment
Expert domain knowledge & experience in Data warehousing, encompassing data model design, dimensional modeling, naming conventions, cross-cutting concerns, common integration technologies, patterns & standards and emerging technologies.
Expert Knowledge of SQL skills to build, debug, and optimize (developing procedures, functions, SQL queries, etc.) and working with large data sets and to perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
2+ years SQL Server Database administration experience
Cloud experience (Azure) is highly preferred
Exposure and experience with Python, R, Hadoop, Azure, other Big data and advanced analytics
Knowledge of AI and ML developments/solutions/implementations
Experience with multiple programming languages (PowerShell scripting, C#, others) with basic scripting skills.
High level of technical aptitude

Inclusion and Equal Opportunity Employment

goeasy is an equal opportunity employer. In addition, goeasy is committed to providing accommodations for applicants upon request at any stage of the recruitment process in accordance with all legislative requirements throughout Canada. Please let us know if you require an accommodation due to a disability during any aspect of the recruitment process and we will work with you to address your needs.

Additional Information:

All candidates considered for hire must successfully pass a criminal background check, credit check, and validation of their work experience to qualify for hire. We thank all interested applicants, however we will only be contacting those for interview who possess the skills and qualifications outlined above.

Why should you work for goeasy?

To learn more about our great company please click the links below:

PAID1234	goeasy	Mississauga, ON	https://ca.indeed.com/rc/clk?jk=e8f01b72a171ac29&fccid=6df7987324612088&vjs=3	e8f01b72a171ac29	2020-08-18T11:04:11.000Z	Data Engineer
2020081893	Are you looking for unlimited opportunities to develop and succeed? With work that challenges and makes a difference, within a flexible and supportive environment, we can help our customers achieve their dreams and aspirations.
Job Description
Are you a go-getter who has a passion in building next gen data pipelines and provide Big data solution for business problems? Are you a big fan of simplification and automation?
Manulife is seeking an awesome Lead Data Engineer , with Big Data experience as well as strong understanding of data-ingestion, data curation and both Batch & Stream Data processing, to join our rapidly expanding IT Organization and assist us as we work to be a digital leader in our industries!
Skills and Experience
You will have the following skills and experience:
Lead development teams to define and build data pipelines
Expert in building and operationalizing BigData platforms in cloud using one of the public clouds, preferably MS Azure.
Hands on experience with Big Data streaming frameworks and tools (Spark Streaming, Storm, Kafka, etc.)
Expert in Hadoop ecosystem and toolset – Sqoop, Nifi, Pig, Spark, HDFS, Hive, HBase, etc.
Expert in automating data pipelines in a Big Data ecosystem, DevOps and CICD.
Experience in developing Hadoop integrations (batch or streaming) for data ingestion, data mapping and data processing capabilities
Experience programming in both compiled languages (Java, Scala) and scripting languages (Python or R)
Expert in developing Big Data set processes for data modeling, mining and production
Experience in working with key partners including business and technology to establish definition of success, goals, key use cases and aligning dev team on strategic priorities.
Excellent communication and interpersonal skills
Excellent analytics, problem solving and solutioning skills
A capacity for constant learning from both success and failure, remaining open to change and continuous improvement
Good to Haves
Experience in Exploratory data analysis; Query and process Big Data, provide reports, summarize and visualize the data
Experience in Canary deployments, 0-downtime, 0-dataloss, hot-hot DR
Experience in designing solutions for Big Data warehouses
Experience with Hadoop security frameworks like Knox, Ranger.
Experience with Hadoop metadata frameworks and security policies such as Ranger, Atlas
Experience in data profiling and analysis
Exposure to and an understanding of Agile scrum methodologies and experience of working in an Agile team
Experience in Big Data performance analysis, tuning and capacity planning
Experience in designing business intelligence systems, dashboard reporting, and analytical reporting is a plus
Experience with the Hortonworks Data Platform (version 2.5)
Experience in using Git flow.
Basic understanding of following will be useful but not required:
Exposure to and basic understanding of collaboration tools like Slack, Skype, Teams, and JIRA
What about Perks?
Manulife has lots of perks including, but not limited to:
Competitive compensation
Retirement Savings Accounts including a RPP (Pension Plan), RRSP (Retirement Savings Plan), and TFSA (Tax Free Savings account)
Manulife Share Ownership Program with employer matching
Customizable Benefits Package including Health, Dental, Vision, and 100% of Mental Health expenses
Financial support for ongoing training, learning, and education
Monthly Innovation Days (Hackathons)
Wearing jeans to work every day
An abundance of career paths and opportunities to advance
A flexible work environment with flex hours, work from home arrangements, distributed teams, and condensed work week arrangements.
This is a full time permanent role and the team is located in Kitchener/Waterloo, Ontario. There is opportunity for Toronto based people to work in this role, however there would be travel to Kitchener / Waterloo twice per week.
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
About Manulife
Manulife Financial Corporation is a leading international financial services group that helps people achieve their dreams and aspirations by putting customers' needs first and providing the right advice and solutions. We operate primarily as John Hancock in the United States and Manulife elsewhere. We provide financial advice, insurance, as well as wealth and asset management solutions for individuals, groups and institutions. At the end of 2017, we had approximately 34,000 employees, 73,000 agents, and thousands of distribution partners, serving more than 26 million customers. As of December 31, 2017, we had over $1.04 trillion (US$829.4 billion) in assets under management and administration, and in the previous 12 months we made $26.7 billion in payments to our customers. Our principal operations are in Asia, Canada and the United States where we have served customers for more than 100 years. With our global headquarters in Toronto, Canada, we trade as 'MFC' on the Toronto, New York, and the Philippine stock exchanges and under '945' in Hong Kong

If you are ready to unleash your potential it’s time to start your career with Manulife/John Hancock.
About Manulife
Manulife Financial Corporation is a leading international financial services group that helps people make their decisions easier and lives better. We operate primarily as John Hancock in the United States and Manulife elsewhere. We provide financial advice, insurance, as well as wealth and asset management solutions for individuals, groups and institutions. At the end of 2018, we had more than 34,000 employees, over 82,000 agents, and thousands of distribution partners, serving almost 28 million customers. As of March 31, 2019, we had over $1.1 trillion (US$849 billion) in assets under management and administration, and in the previous 12 months we made $29.4 billion in payments to our customers.
Our principal operations in Asia, Canada and the United States are where we have served customers for more than 100 years. With our global headquarters in Toronto, Canada, we trade as 'MFC' on the Toronto, New York, and the Philippine stock exchanges and under '945' in Hong Kong.

Manulife is an equal opportunity employer. We strive to attract, develop and retain a workforce that is as diverse as the customers we serve and to foster an inclusive work environment that embraces the strength of cultures and individuals. We are committed to fair recruitment, retention and advancement and we administer all of our practices and programs based on qualification and performance and without discrimination on any protected ground.
It is our priority to remove barriers to provide equal access to employment. A Human Resources representative will consult with applicants contacted to participate at any stage of the recruitment process who request any accommodation. Information received regarding the accommodation needs of applicants will be addressed confidentially.	https://ca.indeed.com/rc/clk?jk=4ecf6d90140603ee&fccid=1747adf6142beb48&vjs=3	Lead Data Engineer	Manulife	Toronto, ON	4ecf6d90140603ee	2020-08-18T11:04:12.000Z
2020081894	https://ca.indeed.com/rc/clk?jk=1dd125b777e13132&fccid=cdf5f442bc9a18df&vjs=3	1dd125b777e13132	Data Solution Engineer
Onix helps customers transform and evolve their business through the use of cloud services. As part of an entrepreneurial team in this rapidly growing business, you will help shape the future of how technology is used in the workplace.
Primary Responsibilities
Lead strategic cloud application development discovery session to help customers understand the value of cloud application development and how to position it within their organization.
Proactively help customers address all technical issues that may arise throughout the entire pre-sales cycle.
Ability to facilitate demonstrations, proof of concepts and public-facing presentations.
Use Google Cloud Platform tools to build Enterprise-grade Big Data solutions.
Architect new cloud-based data pipelines.
Ability to bring together multiple data sources into a unified data warehouse.
Apply analytics and visualizations to customer data sets.
Create customer and partner connections to help grow Onix name recognition in the data space.
Help customers understand the right technologies for their use case.
Establish strategic customer relationships and become their go-to trusted advisor for Big Data needs.
Assist in strategic direction and planning for growth of the Cloud Data Team.
Quickly architect sound cloud solutions to radically different customer environments.
Establish strategic customer relationships and become the technical go-to resource for answers as well as a trusted cloud advisor.
In-depth understanding and the ability to demonstrate expertise in designing, deploying, and maintaining custom enterprise web applications.
In-depth understanding and the ability to demonstrate expertise in using a variety of development languages.
In-depth understanding and the ability to demonstrate expertise to determine the best migration path from legacy or on-prem applications into public cloud environments.
Review and analyze customer architecture at the domain and product level and translate and evolve them into cloud-ready applications.
Staying in constant communication with customers to ensure Onix is addressing all of their needs during the pre-sales cycle.
Learning and maintaining an in-depth understanding of current and new development technologies and industry standards.
Assist account manager with technical discovery and responsible for all technical scoping activities during the creation of the Statement of Work.
Ability to frequently travel throughout the United States and Canada. Up to 25%.
Required Skills and Experience
Degree in Computer Science or Math (or related technical major) or equivalent practical experience (math strongly preferred).
Experience with large data sets and Enterprise-grade databases (structured and unstructured)
Experience architecting and building data pipelines.
Deep understanding of the ETL (extract, transform, load) process.
Experience extracting data from multiple sources via APIs and scripting.
Experience transforming data through field mapping, programmatic rulesets, and data integrity checking.
Able to expertly convey ideas and concepts to others.
Excellent communication skills (verbal, written and presentation)
Creative problem-solving skills and the ability to design solutions not immediately apparent.
Ability to participate in multiple projects concurrently.
Customer-oriented and shows a bias for action.
Able to function in a highly dynamic team that moves rapidly from idea to planning to implementation.
Highly adaptable with the ability to learn new technologies quickly without direct oversight.
Strong knowledge of Python Machine Learning standard libraries.
Mastery of N-dimensional NumPy arrays.
Mastery of pandas data frames
Ability to perform element-wise vector and matrix operations on NumPy arrays.
Strong knowledge of Anaconda, Virtualenv, and Jupyter Notebooks
Good functional Knowledge of Tensorflow programming model.
Strong understanding of all commonly used Machine Learning models and the main algorithms that compose the models.
Ability to rapidly prototype proofs-of-concept and technical demonstrations.
Ability to conduct technical BD/ML workshops enabling the audience ( researchers, Doctoral and postdoctoral CS Ph.Ds, ML Ph.Ds, Mathematicians, Scientist etc) to adopt the cloud technologies to for development and implementation of BD/ML for scientific research.
Good knowledge of common networking concepts.
Strong customer-facing communication skills.
Experience in writing software in Java or Python.
Familiarity with web-related technologies (web applications, web services, service-oriented architectures) and network/web related protocols.
Creative problem-solving skills and a drive to solve difficult issues.
Ability to stay positive and motivated while under pressure.
Ability to participate in multiple projects concurrently.
Excellent communication skills (verbal, written and presentation)
Customer-oriented and shows a bias for action.
Provide on-time, well-executed work that leads to excellent customer satisfaction.
Able to expertly convey ideas and concepts to others.
Highly adaptable with the ability to learn new technologies quickly without direct oversight.
Good understanding of the built-in data types. ( lists, dictionaries, tuples sets).
Preferred Skills and Experience
Google Cloud Platform Data Engineer Certification.
Experience with Big Data, PaaS, and IaaS technologies.
Experience in and understanding of data and information management - especially as it relates to IaaS and PaaS.
Experience architecting and developing software for scalable, distributed systems.
Understanding of the public cloud market and pain points driving enterprise cloud adoption.
It is the policy of Onix to ensure equal employment opportunity in accordance with the Ohio Revised Code 125.111 and all applicable federal regulations and guidelines. Employment discrimination against employees and applicants due to race, color, religion, sex, (including sexual harassment), national origin, disability, age (40 years old or more), military status, or veteran status is illegal.
Onix will only employ those who are legally authorized to work in the United States or Canada. This is not a position for which sponsorship will be provided. Individuals with temporary visas such as E, F-1, H-1, H-2, L, B, J, or TN or who need sponsorship for work authorization now or in the future, are not eligible for hire.	Toronto, ON	Solution Engineer - Data	2020-08-18T11:04:13.000Z	Onix Networking Corp
2020081895	05f7bf2a7458c7fc	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DQQnUBuQBSuyaIQhpC59TW7hrTbBg8v-nGtzzV8dunbQHh9VTp-k89gJ8q3B9UEXLHcYbAG67d8jpOK4E3ok8olcOhfyyiChWMI3BPvUFdra3ljwgLS_vFsJXlW6jHt2NpQMzd2p9F6BXGm9ND3feUN42yGy4tVXBhZi8rYpSJy9XMpcN_ZhwrBzAXZqCp-Fc5zu75uDhsfAk8c9zvEoyTpAAIXBJ5ExqaJ1zXawRU7jBPh_ftNIx-ygM_BZVnTmkDtYMnGGqkP9RSmjh5RTqsJgWd0CrmCprxGTIaUH4KKzOZ1otSlLZ1B-o2daRDf_CISM_OFQRjf6zlap9NH8_sG4Fc670hewXlKM3VxjmfM0tZ5EJmSSr21XkAY_moO2RdkQBFxJ6nEIZtclib5RVDsdpjJ_byKIBxDVlrBw8fFlc1Nrt6LU4hovfp17mdtZhiGrJNtw_dyt_S7n_lfbhbm6ClrLtfgiU-CU85qruVWyxH6HGzsns3MEFixuAPleYTri_WcbJmFoNEYK8st-aalAmX4qrvvdslulcfHslVfwtIbdr4yXGhJzLSQVF_Lp__joBgB8dsS2P74FvK8a3Az5iJSrqKAdfVKlj40JmUXAupocFX3f6UKwBzzBwPS-7YR5nV_rGsC_wOrLcButGtpMfT8IVaPgwxoiB9eY--Rm-rko1juvV3UQqx9DxcndZTgwjLWzm1AE9d_m4X26tDtZaAvfcPLZ4rXSmHsKwEOLZDQ2_mXdyTQAazDzjrvqslKsbLYfh5g0omJAUkL5ml51JfwqXDL1ta8nwSuGKh10Dp-70UugCsA87dJx2NG6hMgwuh75rzAQrdBgwOOZPV11crVzIqLXUf-PSfxirBdA==&p=10&fvj=0&vjs=3	Job Title- Data Engineer
Location- Downtown, Toronto
Type - Full Time Permanent
Salary - Negotiable + Benefits



Focus on data architecture, best practices, reliability, security, and complianceImprove and extend ETL, data processing, and analytics processesFacility with PowerBI, including creating dashboards and data sourcesDeveloping high complexity, fast performing SELECT queries.Developing T-SQL procedures, functions, triggers, jobs, scripts, etc.Development of Advanced T-SQL such as temporal tables, PIVOTs, recursive table expressions and more.Modeling and implementing Data Mart solution for Power BI analytics
Managing indexes, statistics, query plans alerts, database activity, and overall performance activity.In-depth experience working with relational databases, such as Microsoft SQL Server or PostgreSQLEnthusiasm for applying good data design, testing, documentation, and support practicesExperience building and optimizing data pipelines, architectures, and data setsKnowledge of message queueing, stream processing, and data stores/warehousesWorking knowledge of AWS products related to data engineeringBachelor's degree in Computer Science, Software Engineering or an equivalent
Excellent communication skills - both written and verbal; ability to speak in Spanish is a bonus

To apply please send an email to sheetalk@tes.net	Toronto, ON	Data Engineer	2020-08-18T11:04:13.000Z	TES - The Employment Solution
2020081896	If you are looking to join one of Canada’s fastest growing companies, goeasy Ltd. is the place for you! Recognized as one of Canada’s Most Admired Corporate Cultures, one of Canada’s Top 50 Fintech’s and one of North America’s Most Engaged Workplaces, we want the best and brightest to join our team.

We are a publicly traded company on the TSX with over 4000% shareholder return since 2001, goeasy operates two main business units. easyfinancial is our consumer lending business that offers secured and unsecured installment loans of up to $35,000 and easyhome is Canada’s largest merchandise lease-to-own company. It is our mission to provide everyday Canadians the chance for a better tomorrow, today by giving them access to the credit they need and by offering them a second chance when they have been turned down by banks and traditional lenders. With a retail network of nearly 400 locations across Canada and over 1900 employees, we are able to build lasting relationships with our customers as we help them rebuild their credit and graduate towards prime rates and a brighter financial future.

The Data Engineer on the Data Science and Business Insights team will build, integrate data from various resources, manage data in goeasy operational data store and enterprise data warehouse. This position will develop ETL (Extract, Transform and Load) with various tools on large datasets to ensure data is easily accessible, works smoothly, as well as maintain and expand the data warehouse for reporting and analysis. The Data Engineer will also work closely with the data architect on the design and architecture of our enterprise data warehouse.

Responsibilities:

Develop data set processes for data modeling, mining and production
Develop and maintain ETL processes using SSIS, Scripting and data replication technologies
Participate in development of datamarts for reports and data visualization solutions
Research opportunities for data acquisition and new uses for existing data
Integrate new data management technologies and software engineering tools into existing structures
Support the translation of business requirements for data acquisition/manipulation and provide detailed specifications that can be passed downstream for use
Develop detailed technical specifications and operational support documentation in collaboration with Business Systems Analysts, BI Engineers and Architects.
Identify and communicate technical problems, process and solutions
Create Ad-Hoc queries and reports as needed along with providing on-going analytical support for these requests
Assist in the collection and documentation of user’s requirements
Ensure that existing business processes dependent on the ODS/EDW are monitored and respond quickly to bug fixes, enhancement requests and production ETL related issues.
Dealing with the database users on a daily basis to ensure that problems are dealt with promptly and that appropriate fixes are made to resolve any problems.
Recommend ways to improve data reliability, efficiency and quality
Ensure systems meet business requirements and industry practices
Work effectively with the Business Intelligence and Data Solutions Architects, Data and BI Engineers to ensure that all approved development and deployment procedures are followed.

Qualifications:

Bachelor’s Degree in Computer Science, MIS, Computer Engineering or other Information Technology related degree
4+ years working with SQL Server or comparable relational database system
3+ years of extensive ETL development experience with SSIS and/or ADF
4+ years of experience troubleshooting within a Data Warehouse environment
Expert domain knowledge & experience in Data warehousing, encompassing data model design, dimensional modeling, naming conventions, cross-cutting concerns, common integration technologies, patterns & standards and emerging technologies.
Expert Knowledge of SQL skills to build, debug, and optimize (developing procedures, functions, SQL queries, etc.) and working with large data sets and to perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
2+ years SQL Server Database administration experience
Cloud experience (Azure) is highly preferred
Exposure and experience with Python, R, Hadoop, Azure, other Big data and advanced analytics
Knowledge of AI and ML developments/solutions/implementations
Experience with multiple programming languages (PowerShell scripting, C#, others) with basic scripting skills.
High level of technical aptitude

Inclusion and Equal Opportunity Employment

goeasy is an equal opportunity employer. In addition, goeasy is committed to providing accommodations for applicants upon request at any stage of the recruitment process in accordance with all legislative requirements throughout Canada. Please let us know if you require an accommodation due to a disability during any aspect of the recruitment process and we will work with you to address your needs.

Additional Information:

All candidates considered for hire must successfully pass a criminal background check, credit check, and validation of their work experience to qualify for hire. We thank all interested applicants, however we will only be contacting those for interview who possess the skills and qualifications outlined above.

Why should you work for goeasy?

To learn more about our great company please click the links below:

PAID1234	goeasy	2020-08-18T11:04:14.000Z	Mississauga, ON	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DbNJQGwjC-NEA80-qlVyEGmSv34Xx0eW4t4u9rXSv_7_tlvMU2e-Gw_wrZ47s1zPCnrySU2yuEL07wtSyx7XOwYh5VPcVL7CxRhOYwHzR-CdmyBrwbeqommOn6y9FD_uKIYpUCb9-PC_XgwGzBDrPy7SD7JQndrQeQeeZZhCHDE7j2BLBjaGpoLMQEDYLsp8YB-hhjx2mCFkqLZk2BkZA6F19RoZqL9eee7Isjh8WDY_AO1bqfaf7CMgK_1fJGzyTa7fTRqYL_lf4eS0nmkZgbIS-vJmuqaLL5F4sFEiv46TH35GTJOuAE5s5evFVgknUJZlj_Y3ebE1pbKlLLVHFg8JMN-mQNdb2c9yRed2COzUkYszt04Eqtei0kmJMIz_at1gwne34ZwQng97sAUdjgF9SPK4ZWbycjvAoJgGC4s9dxt8z7rkCtSHYFH3xdD2szu_4ZrWqF9A0xizk5-lD1W6rErIJieZnjvz4jFbuE-Q==&p=11&fvj=0&vjs=3	e8f01b72a171ac29	Data Engineer
2020081897	Myticas Consulting	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Bt274jo0r_e0cCCqhskoQvohXG3bfDAo8rIdE-Msu3yuVfyF2oiDFKHFOU-vxVulAfe8kPL7riFwoXa1bsfN-szjxO_8qUQWbPsJOnTg0EkutIoK2S4NNLL24JQh9e4DsE9hv907k2_zDbvj0a8WAj5oPSV5J0YhjPRMFQVzYj6d82f2qaX8rE0nf5QmhkhsQESCM4FEAKUsEjUhZnIGQQ4usWcWRE4k09BuVjZPizHtNRGJQHNZLKiBASz0D4WmUCMYt_Qs-bN96aPGUMR2XORQ1j2SVCvRHQ9AYrgAPVLHsomXrAIO_zxiCrH3A0DniElYuQZgCkN44Pj_-9lcUfNllXyyc786dysBZlgMBxv_uvg373RLA5MWczfJne1ZrpRAWOPyFXC4DlUlgRKuk3RVkwJFE5RW0lfJmLZdjzNXcIo71ZhHSvArx1ywszwRNrmfPNIVMnyg==&p=12&fvj=0&vjs=3	BHJOB15656_15145 - Data Engineer w/ DevOps - Azure Cloud	Toronto, ON	Our recruiting team at Myticas Consulting is looking for an experienced Data Engineer who would be interested in a remote contract opportunity offered within the Ottawa, Toronto and Montreal regions.

Responsibilities:

Defining and reviewing security design requirements for cloud infrastructure and application components.
Evaluating architecture patterns from security perspective.
Building and implementing security controls to enable enforcement of compliance with Cloud Control Objectives, using custom Azure policies and integrated controls in DevOps processes

Qualifications:

Strong Data Engineer w/ DevOps expertise + Azure Cloud Experience
Must know how to code and stand up scripts.
Experience with Data Digestions
Experience writing scripts to automate (infrastructure)
ARM Templating Expertise
Azure Synapse Expertise
Support developing automated DevOps processes and procedures for the following Azure components:
Azure Synapse (Azure DW) & Studio (private preview)
Azure Data Catalog Gen 2 (Babylon – private preview)
Azure Data Lake Storage Gen 2
Azure ML
ML Flow
Azure SQL Analysis Service
Azure Databricks
ADF data pipelines for data loading to AzSQL/Synapse
ADF data pipelines for connecting to on-prem data sources for data

Candidates looking to apply for this role are to send us an updated version of their resume in confidence. Our team will be sure to review all applicants and follow up accordingly at the conclusion of the review process.

Job is also known as: Data Engineer, DevOps Engineer, Azure Cloud Engineer, Azure Engineer, Cloud Engineer

INDMY	a32b7c599156a218	2020-08-18T11:04:15.000Z
2020081898	2020-08-18T11:04:16.000Z	Our client, a boutique-size consulting firm, prides itself on being at the forefront of innovation in the Big Data space. Founded in 2010, they provide thought leadership and implementation excellence within the ever-growing data and analytics world.
They take pride in having some of the most highly trained and experienced consultants in the industry which translates into optimal value for their clients. They were one of the first companies to provide analytics and data as a service, via the cloud, as early as 2010. They strive to make sure their customers are well-positioned with the best technologies/tools in the industry, constantly evaluating new and existing technology partnerships. Some of the more prominent companies they have partnered with include; Snowflake, DataRobot, MicroStrategy, Informatica, Amazon AWS & Microsoft. They continue to invest in their most valuable resource, their people. They do this through extensive training both on the job and through various educational programs.
Due to growth, they are seeking to hire an Intermediate - Senior Integration Consultant.

Desired Skills and Experience
: University/College degree in Computer Science, Mathematics, Data Science and/or Relevant Degree
: 5+ years hands-on development, configuration, scripting and administration experience with Data Integration platforms. (i.e. Informatica, Talend, DataStage, SSIS)
: BI Experience (MicroStrategy, Looker, Tableau, PowerBI) considered a nice to have
: Extensive theoretical and practical knowledge of data warehousing principles/concepts and practical development experience in all areas of the data warehousing life cycle
: Experience with Data Management, ETL, Cloud Data (AWS), Data Integration
: Knowledge of OLAP-related principles and concepts
: Strong grasp of data modeling techniques and concepts (Normalized/Denormalized, Conceptual/Logical/Physical, Star, Snowflake, Data Vault)
: Strong knowledge and experience with relational databases such as Snowflake, SQL Server, Oracle (Advanced knowledge of reading and writing SQL, Performance analysis and tuning)
: Knowledge and experience with key Big Data technologies (Hive, Presto, Spark, Kafka, NoSQL databases, Semi-structured data access patterns (Json, Parquet, XML, etc.))
: Strong Python scripting skills
: Excellent communication skills
: Great problem-solving skills
: Leadership and good client management skills

Day to Day Activities Would Include
: Conduct relevant customer interviews to determine key business requirements and objectives
: Build appropriate analytical data models based on outcomes of user interviews
: Analyze and profile data systems to build source to target data mappings
: Review ETL performance and conducts performance tuning as required on mappings/workflows or SQL
: Administration and support of data integration infrastructure
: 2nd level on-call support of ETL services as required

You will be responsible for attaining the following goals:
: Attaining a minimum of 1 new accreditation/certification per year
: Spending 80% or more of their time on billable work
: Completing 90% or more of their agile delivery tasks on time
: Demonstrating competency in 1 new relevant technology every year	Copperstone Connect	Toronto, ON	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Ceb7pqR8iGmLuXIZQklF6pqed3xddONNM6Jjumj-ndyGK0kULQUmQ2REqfJ5KAzFVeK_es1WVg0s1sLgKW09UsvVRp0-dSizFQr1oNyK_VIqdjla00VU9GNVvu2x4v9p3hsL7NeF-6hBksbAaSgKazsaYTODGMKQQeVuYcxoOuxvFKuD0HRLD9ZozEtlooKFOnRmDvuaGVgRegXS1JCeG0lZIMr689S5Abs-rrGqzbM3ipI_ajI_Hy3wfN2ugBESF7kAI4LIYwScxMb4ghkpYT2F6ZkVGBTR-m1yqIdA-R2fzMyZ2zVIqH7sHsOnOY5q9LPFw0mI5BmmsilrYwea-juhps5_kKatdWN2pgWl-FZr6196NBpFYbwcupib7eZ7h62eymlaOg3gWwyc3spLXFYPjjc4SU4jVBdsnN5muwjglw9DEmlkkMntrkypi1bg4ioyzuWDnn4zV3DmbCFnVax-I1Y11rPTTtbuk_2s18RVW_Vz1N0huATBVf_-XNWcp_4Gy4Zx5zCxT2COVbQgyd-eyCEb5pJiglyV3q-WS4yFUHi_7RRYi25V3V5eR2aO1hrFbRV6Fw0E8ADYZoUUnFFaWUw8u0Uv54UlYV6b5S5bqMeyqduAcoPyRQKT6eJz57vJtXSDY_-B0jxKxn5bFkH_J7a52WKA0ui8Xz0OJ08g==&p=13&fvj=0&vjs=3	Data Engineer/Integration Consultant	54e0fcc38385e3a2
2020081899	North York, ON	Role: Data Engineer (Intermediate 4-6 years)Structure: Contract (6 months initially)Location: North York, ONHours: Monday - Friday 40 hours (remote during COVID-19)Pay: $50.00 p/h inc.The role: We are looking for Intermediate Data Engineers who have roughly between 4-6 years experience who ideally has experience moving data from Google cloud over to Big query.However, if you are someone who has good experience with ETL and Data Pipeline Design & Data Validation experience and can script in Python and create strong SQL Queries then this will also be a potential match for this role.Ideal Qualifications to be successful in this role: - Degree in Computer Science, Engineering, Information Systems (or equivalent combination of skill and experience)- 4-6 years of experience working with data architecture projects- Excellent SQL coding and experience with a broad array of development tools and platforms including exposure to a big data environment tools/languages, such as SQL, R, SAS, Python, etc.- Experience creating relational database design and data models- Experience with various analytical data platforms and technologies- Experience with using REST API, Cloud (GCP Ideally)- Experience in custom or structured data integration design, implementation, and maintenance- Experience with business intelligence tools such as Qlikview, Tableau and Microstrategy (Nice to have for Data Visualizations)- Previous experience working in retail or eCommerce is highly preferred- Ability to curate data to tell stories and provide business insightsPlease apply with an updated resume and ensure the required skills you are able to speak to for this position are included.For more roles like this please go to www.corgta.com/find-a-job/Job Type: Full-timeSalary: $45.00-$50.00 per hour	Data Engineer (Intermediate 4-6 years) - Contract - Remote during COVID	997673b8effa647f	2020-08-18T11:04:18.000Z	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Dkh866153otbRJ8nVMuEXnXPd315JS377__3VyD6HnNQkmcaOHMemmy5pLrNV6j0WaVWEWWabnp4PP6-2u77DLfT_FCIGvCW8CQmQW140Z7EVU32Jnwl42FALsxnbc4n5M6FOTfoW2OoTMOcGG21HZGOoXL7fWT8eaT67VlcClh6Pj9F1SyGchh90WS8fV1eoOaE0dCD2uX6ND7dYTc1o6HZbaPiBoDP_2BCVxoZt9RtdJQnQ5gg2tnN9t017RpsumAvaK3UWzdZhwD2254I04fxZXlmQ-HD2evFcmYEGj9oESaoi8bkkXgwAyzFOpkU9YLty9oLMprJKyNR2JgxlO0h5RvM1w6FUBI_am2cX1w_sb1vl_oj9UGS8MohUUv11WHaVXL91sc3lLDDnP3HIdW05REBOAvkUIQpRaZ_HXQrWEXrQ3-0OpWv2aAbNKAHLEER6oDPDMwH5or0HeYTQ9kkGIBjWbU_w=&p=14&fvj=1&vjs=3	CorGTA
20200818100	2020-08-18T11:04:19.000Z	If you are looking to join one of Canada’s fastest growing companies, goeasy Ltd. is the place for you! Recognized as one of Canada’s Most Admired Corporate Cultures, one of Canada’s Top 50 Fintech’s and one of North America’s Most Engaged Workplaces, we want the best and brightest to join our team.

We are a publicly traded company on the TSX with over 4000% shareholder return since 2001, goeasy operates two main business units. easyfinancial is our consumer lending business that offers secured and unsecured installment loans of up to $35,000 and easyhome is Canada’s largest merchandise lease-to-own company. It is our mission to provide everyday Canadians the chance for a better tomorrow, today by giving them access to the credit they need and by offering them a second chance when they have been turned down by banks and traditional lenders. With a retail network of nearly 400 locations across Canada and over 1900 employees, we are able to build lasting relationships with our customers as we help them rebuild their credit and graduate towards prime rates and a brighter financial future.

The Data Engineer on the Data Science and Business Insights team will build, integrate data from various resources, manage data in goeasy operational data store and enterprise data warehouse. This position will develop ETL (Extract, Transform and Load) with various tools on large datasets to ensure data is easily accessible, works smoothly, as well as maintain and expand the data warehouse for reporting and analysis. The Data Engineer will also work closely with the data architect on the design and architecture of our enterprise data warehouse.

Responsibilities:

Develop data set processes for data modeling, mining and production
Develop and maintain ETL processes using SSIS, Scripting and data replication technologies
Participate in development of datamarts for reports and data visualization solutions
Research opportunities for data acquisition and new uses for existing data
Integrate new data management technologies and software engineering tools into existing structures
Support the translation of business requirements for data acquisition/manipulation and provide detailed specifications that can be passed downstream for use
Develop detailed technical specifications and operational support documentation in collaboration with Business Systems Analysts, BI Engineers and Architects.
Identify and communicate technical problems, process and solutions
Create Ad-Hoc queries and reports as needed along with providing on-going analytical support for these requests
Assist in the collection and documentation of user’s requirements
Ensure that existing business processes dependent on the ODS/EDW are monitored and respond quickly to bug fixes, enhancement requests and production ETL related issues.
Dealing with the database users on a daily basis to ensure that problems are dealt with promptly and that appropriate fixes are made to resolve any problems.
Recommend ways to improve data reliability, efficiency and quality
Ensure systems meet business requirements and industry practices
Work effectively with the Business Intelligence and Data Solutions Architects, Data and BI Engineers to ensure that all approved development and deployment procedures are followed.

Qualifications:

Bachelor’s Degree in Computer Science, MIS, Computer Engineering or other Information Technology related degree
4+ years working with SQL Server or comparable relational database system
3+ years of extensive ETL development experience with SSIS and/or ADF
4+ years of experience troubleshooting within a Data Warehouse environment
Expert domain knowledge & experience in Data warehousing, encompassing data model design, dimensional modeling, naming conventions, cross-cutting concerns, common integration technologies, patterns & standards and emerging technologies.
Expert Knowledge of SQL skills to build, debug, and optimize (developing procedures, functions, SQL queries, etc.) and working with large data sets and to perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
2+ years SQL Server Database administration experience
Cloud experience (Azure) is highly preferred
Exposure and experience with Python, R, Hadoop, Azure, other Big data and advanced analytics
Knowledge of AI and ML developments/solutions/implementations
Experience with multiple programming languages (PowerShell scripting, C#, others) with basic scripting skills.
High level of technical aptitude

Inclusion and Equal Opportunity Employment

goeasy is an equal opportunity employer. In addition, goeasy is committed to providing accommodations for applicants upon request at any stage of the recruitment process in accordance with all legislative requirements throughout Canada. Please let us know if you require an accommodation due to a disability during any aspect of the recruitment process and we will work with you to address your needs.

Additional Information:

All candidates considered for hire must successfully pass a criminal background check, credit check, and validation of their work experience to qualify for hire. We thank all interested applicants, however we will only be contacting those for interview who possess the skills and qualifications outlined above.

Why should you work for goeasy?

To learn more about our great company please click the links below:

PAID1234	goeasy	Mississauga, ON	e8f01b72a171ac29	Data Engineer	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DbNJQGwjC-NEA80-qlVyEGmSv34Xx0eW4t4u9rXSv_7_tlvMU2e-Gw_wrZ47s1zPCnrySU2yuEL07wtSyx7XOwYh5VPcVL7CxRhOYwHzR-CdmyBrwbeqommOn6y9FD_uKIYpUCb9-PC_XgwGzBDrPy7SD7JQndrQeQeeZZhCHDE_Mb3_StFCSZWRKidM9jvTJGa_9dYqAiWUhItITjROrOa-c7tk7r-MqzsjL-zeOE1iDI2PiaY1G91kyqAkCM8qb8TJcomradxung9dq57Me8iwJx600dtc8Ei_peuvsEQxaX8Lmbd9rXrZ46LCHW486bFyuEKFDV-cN1BEexkVEjapyxh89LhK2mCtCPy7f57l3yQmmRAdRDFCDkSsizoz5-W83cmvHW6y9_ls7BsbVHX241N5Rj3J38W2m-evBkUFYR8sRPTtQIiJT4hx7rEKhS3Y28-CCaHmMco4paC-oH-SsZKi11KJK7v-EoJ-aJ8w==&p=0&fvj=0&vjs=3
20200818101	Fleet Complete	https://ca.indeed.com/rc/clk?jk=2ef0d527757decfa&fccid=c0b5558e336243b3&vjs=3	Data Developer	2ef0d527757decfa	Toronto, ON	COMPANY OVERVIEW:
Success stories like this, don’t happen every day. From humble beginnings as a courier industry solutions provider in Canada, Fleet Complete quickly grew to be one of the world’s leaders in telematics and connected mobility solutions for a wide variety of industries with fleets, assets and mobile workers.

Today, with 20 years in the industry, Fleet Complete is one of the fastest-growing IoT (Internet of Things) companies across the globe, operating in 17 countries with offices in Canada, Netherlands, Denmark, Belgium, Estonia, Latvia, Lithuania and Australia. Fleet Complete continues to win employer, innovation thanks to our relentless customer-centric approach and commitment to company values of Innovation, Quality, Customers, Productivity, People and Community.

Thanks to strong partnerships and sound investments, our trusted Fleet and Mobile workforce platform provides real-time insights, visibility, employee safety and overall operational efficiency. This helps organizations, municipalities and businesses of all sizes to modernize their operations with ease. Fleet Complete is known for hiring, growing and empowering talented people who develop innovative products, build powerful relationships and provide personalized support that is unparalleled in our industry. Join Fleet Complete on our next chapter and we can work together to "help fleets thrive".

Proud to be named one of Greater Toronto’s Top Employers for 2020: http://content.eluta.ca/top-employer-fleet-complete

ESSENTIAL DUTIES & RESPONSIBILITIES:

Create and maintain optimal data pipeline architecture for legacy and the new architecture of IoT streaming data (Telematics and other automotive sensors)
Assemble large, complex data sets that meet functional / non-functional business requirements for data and application products
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics
Work with the Product team and other stakeholders to assist with data-related technical issues and support their data infrastructure needs
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader

QUALIFICATIONS:
All applicants must possess the following:

5+ years of experience in a Data Engineer role
A degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field (Graduate degree would be a plus)
Experience with cloud technologies. Specifically, AWS technologies such as S3, Glacier, Lambda, Athena, Redshift
Experience with object-oriented & functional scripting languages including Python and Java
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Strong analytic skills related to working with unstructured datasets
Build processes supporting data transformation, data structures, metadata, dependency and workload management
A successful history of manipulating, processing and extracting value from large disconnected datasets
Strong project management and organizational skills
Experience supporting and working with cross-functional teams in a dynamic environment
Understands and helps drive business impact via data systems and their resulting output

Fleet Complete will provide support in its recruitment processes to applicants with disabilities, including accommodation that takes into account an applicant's accessibility needs. If you require accommodation during the interview process, please contact the Recruitment Team, 866-649-7949.

Fleet Complete is an equal opportunity employer committed to diversity and inclusion. We are pleased to consider all qualified applicants for employment without regard to race, color, religion, sex, national origin, age, disability, protected veterans’ status or any other legally protected factors.	2020-08-18T11:04:20.000Z
20200818102	2020-08-18T11:04:21.000Z	https://ca.indeed.com/rc/clk?jk=82b02bab68b5d127&fccid=a4e4e2eaf26690c9&vjs=3	ARE YOU READY to step up and take your technology expertise to the next level?

There is never a typical day at Accenture, but that’s why we love it here! This is an extraordinary chance to begin a rewarding career at Accenture Technology. Immersed in a digitally compassionate and innovation-led environment, here is where you can help top clients shift to the New using leading-edge technologies on the most ground-breaking projects imaginable.
Interested in building end-to-end marketing solutions for clients? Bring your talent and join Data which operates in the Interactive, Mobility and Analytics space. You will have opportunities to get involved in digital marketing, eCommerce and end-to-end mobility capabilities to help clients to improve productivity and more!

WORK YOU’LL DO
Work across the Service Delivery Lifecycle to analyze, design, build, test, implement and/or maintain multiple system components or applications for Accenture or our clients
Adapts existing methods and procedures to create possible alternative solutions to moderately complex problems
Uses considerable judgment to determine solution and seeks guidance on complex problems
Determines methods and procedures on new assignments with guidance
Manages small teams and/or work efforts (if in an individual contributor role) at a client or within Accenture
Experience mentoring and managing other Data Engineers, ensuring modern data engineering best practices are being followed

WHO WE´RE LOOKING FOR?
Minimum 8 years of experience as a Data Engineer with Data Warehouse Experience with Azure DW, Synapse, Oracle, Redshift, PostgreSQL etc.
Demonstrated strength in SQL, data modeling, ETL development, and data warehousing
Must have experience with one of the Cloud Technologies (Azure or AWS)
Azure cloud includes Spark, Python, Databricks, Synapse, Snowflake, Data Factory and ADLS
AWS cloud includes Glue, EC2, EMR, Athena, redshift, Snowflake, S3, Spark, Python and Databricks
Experience with Big Data technologies like MapReduce, Pig, Hive, HBase, Sqoop, Flume, YARN, Kafka, Storm and etc.
3+ years of experience with at least one SQL language such as T-SQL or PL/SQL
3+ years of work experience with ETL and data modeling
Experience in real-time analytics application
Experience in both batch and stream processing technologies
Coding proficiency in at least one modern programming language (Python, Ruby, Scala, Java, etc.)
Machine learning experience with Spark or similar
Must be eligible for security clearance
Professional Skills Qualifications:
Proven success in contributing to a team-oriented environment.
Proven ability to work creatively in a problem-solving environment.
Desire to work in an information systems environment.
Demonstrated teamwork and collaboration in professional setting; either military or civilian.
WHAT´S IN IT FOR YOU?
Competitive benefits, including a fair and balanced parental leave policy.
Fantastic opportunities to develop your career across industries with local and global clients.
Performance achievement and career mentorship: our performance management process focuses on your strengths, progress and career possibilities.
Opportunities to get involved in corporate citizenship initiatives, from volunteering to charity work.
To learn more about Accenture, and how you will be challenged and inspired from Day 1, please visit our website at accenture.ca/careers.
It is currently our objective to assign our people to work near where they live. However, given the nature of our business and our need to serve our clients, our employees must be available to travel when needed.

Accenture does not discriminate on the basis of race, religion, color, sex, age, non-disqualifying physical or mental disability, national origin, sexual orientation, gender identity or expression, or any other basis covered by local law. Accenture is committed to providing employment opportunities to current or former members of the armed forces.

We are committed to employment equity. We encourage all people, including women, visible minorities, persons with disabilities and persons of aboriginal descent to apply.

Accenture is a leading global professional services company, providing a broad range of services and solutions in strategy, consulting, digital, technology and operations. Combining unmatched experience and specialized skills across more than 40 industries and all business functions — underpinned by the world’s largest delivery network — Accenture works at the intersection of business and technology to help clients improve their performance and create sustainable value for their stakeholders. With 469,000 people serving clients in more than 120 countries, Accenture drives innovation to improve the way the world works and lives. Visit us at www.accenture.com.	82b02bab68b5d127	Toronto, ON	Azure/AWS Data Engineering Manager	Accenture
20200818103	bcb75a8b3d33578b	Markham, ON	BGIS	Senior Data Engineer	https://ca.indeed.com/rc/clk?jk=bcb75a8b3d33578b&fccid=84c23fbcabc14d59&vjs=3	Job Field:
Information Technology
Job Type:
Full-time
Building Location:
Length of Assignment:

SUMMARY

The Senior Data Engineer is a deep technical expert in building complex data warehousing and business intelligence applications. At this level, the incumbent demonstrates a passion and in-depth knowledge of large, complex application development methodologies. They motivate themselves and the team to refine their skills and adopt best practices for developing pragmatic software solutions for the organization. Leading the charge, they continue to raise the bar on mastery of business intelligence application development within the team and the organization.

KEY DUTIES & RESPONSIBILITIES

Programming

Uses in-depth knowledge of advanced programming techniques, design patterns and hardware/software interfaces to develop business intelligence and data warehouse applications.
Designs, tests and integrates data warehouse and BI modules and resolves programming errors using various debugging tools and techniques.
Provides guidance/mentors on programming practices and techniques to individuals and cross-functional teams.
Provides support, guidance and production assurance for very complex or urgent problems.
Performs work with minimum supervision, and work is assigned in terms of technical objectives.
Prepares technical documentation (e.g., user guides, technical specifications).
Assists in the design of business solutions.

Analysis

Conducts impact analysis for proposed changes to or problems across the system.
Leads team discussions in the analysis and collaboration to clarify and improve specifications or to identify alternative programming solutions.

Continuous Improvement

Makes recommendations or decisions on architecture, application design, standards and process improvements.
Enforces team and organizational standards and practices (e.g. at walkthroughs and peer code reviews).
Engages in continuous learning by developing and executing on a learning plan.
Takes responsibility for individual and the team's results.
Advocates for quality in all aspects of development efforts based on the team's definition of quality.

Risk Management

Estimates and prioritizes work to maximize value while taking into account risk, effort and dependencies.
Raises impediments, risks, and issues as early as possible and work with the team to mitigate as needed.

KNOWLEDGE & SKILLS

University graduation and minimum 5-10 years of relevant experience
Demonstrates in-depth knowledge of Microsoft BI architecture, established data warehouse development methodologies, multi-dimensional data modelling, OLAP, metadata management, data security, predictive analysis and big data processing. Extensive experience in one of these cloud data warehouses (Snowflake, bigtable, Redshift), Data Vault 2.0 methodology, steaming data processing, BI components in SQL Server 2016+, TSQL, and DAX, Power BI.
A good working knowledge of application security, C#, python, PowerShell, metadata management, NoSQL, and data security.
Experience in programming and debugging complex data warehouse and BI applications as part of a multi-disciplinary team environment (following an agile framework such as SCRUM preferred) based on Microsoft Team Foundation Server and git.
Experience in writing unit tests to support production code using a unit test framework.
Experience with database management (i.e. database design, schema creation, concurrency and performance considerations).
Takes ownership and initiative and collaborates well with a team of peers.
Demonstrates a commitment to continuous learning (e.g. user groups, blogs, conferences, community awareness, and next generation tooling).
Able to clearly communicate in both a verbal and written form within a predominantly English working environment.
Has a positive, passionate, idea generating attitude when faced with challenges.

Licenses and/or Professional Accreditation

Certification in Microsoft technologies preferred	2020-08-18T11:04:22.000Z
20200818104	https://ca.indeed.com/rc/clk?jk=fb3fb7f802f5a80a&fccid=d76467c879ed211c&vjs=3	About Etraveli Group
Etraveli is one of the leading global flight centric Online Travel Agencies (OTAs) with €4bn+in annual gross sales. We also operate flygresor.se, the #1 metasearcher in Sweden and Tripstack, the independent B2B arm of the group offering a variety of complex technology solutions.
Our diverse, dynamically growing team of 1000+ talented professionals is always on the lookout for more members to join our ranks and explore unlimited business opportunities together! Our 110 websites in 70+ countries across the globe include (but are not limited to) gotogate.com, pamediakopes.gr, mytrip.com, flightnetwork, supersavertravel.se, trip.ru & flygresor.se.
About Tripstack
TripStack is a brand under the Etraveli Group, It is revolutionizing the travel & technology field with a mission to challenge the status quo by solving difficult problems and changing the way millions of people travel.
We offer unique flight content, consisting of Virtual Interline & Low-Cost carriers in one search - this enables our partners to offer the end consumer the most relevant flight options at the lowest price.
Our company’s core values are what form our foundation. They inform how we work, how we execute, how we choose our future teammates and how we present ourselves to the world as TripStack employees.


Playing to Win.
#1 within flights
At TripStack we play as a team, we win as a team and we constantly aim to become better. We challenge convention, ourself and our teammates. We dare to think big and we value those who think differently.

Drive for Excellence

Good is not good enough.
We show a never give up attitude and we move fast, knowing we sometimes make mistakes, but that is needed to keep a high pace and to solve complicated stuff. And what we do is complicated, but by driving for high quality & excellence we simplify the product offering, enabling the end user more and cheaper ways to travel.


Act with Ownership

Have pride in your work
At TripStack we expect all to take accountability for your work but also accountability for the team work. We value proactivity, employees who take initiative to get things done- also when it is out of normal scope - it help us achieve our goals.
TripStack is part of the Etraveli Group, a global online travel agency and the fastest growing in Europe whose presence across the web spans 50 countries.
The Role
We are looking for an experienced data engineer to join our Virtual Interlining team for TripStack. Virtual Interlining is a technology we provide that combines flights from different carriers that don’t traditionally work together to go from point A to B via C. These unique fares provide significantly lower prices to the end consumer and much higher flight margins to our partners. Did you know that there are nearly 45 million flights operated worldwide on an annual basis? Indexing that data to provide customers with better flight options is a daunting and exciting mission.

You will use various methods to transform raw data into useful data systems. For example, you will create algorithms and conduct statistical analysis. Overall, you will strive for efficiency by aligning data systems with business goals.
To succeed in this role, you should have strong analytical and programming skills and the ability to combine data from different sources. Data engineer skills also include familiarity with several programming languages (preferably golang, python) and knowledge of machine learning methods would be considered an asset. You should have a good understanding of data warehousing concepts, having worked with large datasets and designed star/snowflake schemas. Experience with data processing tools like Apache Hadoop, Spark, Apache Druid( real-time OLAP) would be considered an asset.

Responsibilities:

Analyze and organize raw data
Build data systems and pipelines
Evaluate business needs and objectives
Interpret trends and patterns
Conduct data analysis and report on results
Prepare data for prescriptive and predictive modeling
Build prototypes
Combine raw information from different sources- flat files, databases, NoSQL caches
Explore ways to enhance data quality and reliability
Identify opportunities for data acquisition and prototype them using machine learning
Collaborate with Data Scientists and Architects on several projects
Requirements
Bachelor’s degree in Computer Science or technical discipline required. Master’s degree not mandatory but would be considered an asset.
4 years previous experience as a data engineer or in a similar role
Technical expertise with data models, data mining, and segmentation techniques
Knowledge of programming languages (Golang, Java, and Python)
Hands-on experience with SQL database design and proficient at writing SQL queries specifically Postgres
Knowledge of Apache Hadoop, Spark an asset
Knowledge of real-time OLAP like Apache Druid is an asset
Exceptional numerical and analytical skills	Etraveli Group	Toronto, ON	2020-08-18T11:04:23.000Z	Data Engineer	fb3fb7f802f5a80a
20200818105	https://ca.indeed.com/rc/clk?jk=1aca9ca80ec23fec&fccid=1f9d0530a51ff611&vjs=3	1aca9ca80ec23fec	2020-08-18T11:04:24.000Z	Expert BI/ETL Engineer (Tech Lead Cloud Data Engineer)	Mississauga, ON	What will you contribute?
Reporting to the Senior Manager, Development, the role of the Expert BI Developer is to ensure the effective design and delivery of the Student Lending reporting solution. This hands-on role serves as a Technical Lead for the Business Analytics and Reporting team providing development, technical guidance, review and support.
Responsibilities & Deliverables:

Your deliverables as an Expert BI Developer will include, but are not limited to, the following:
Develop and deliver a robust Reporting and Business Analytics framework, well aligned with the company’s long-term strategic goals for data architecture vision.
Ensure the solution supports Student Lending client data needs and can be easily extended to newly acquired clients and their standards.
Liaise with vendors and service providers to select the products or services that best meet company cost and performance goals related to data architecture and analytics
Working closely with both enterprise level and project level team - data owners, stewards, users, business analysts, developers, quality analysts, department managers, architects and other stakeholders to understand reporting requirements and ensure strategic goals and tactical implementation are in alignment.
Translate project requirements into functional and non-functional specifications for BI reports and applications.
Lead conceptual and physical design, development and implementation of enterprise level BI and ETL framework, conforming to well defined business, technical rules and SLAs, preserving reusability of artefacts, single version of truth, centralization of logic, testability and well-designed error handling.
Prepare all necessary documentation that clearly describes solution and Meta data.
Monitor, tune up and administer BI Environments for quality and optimal performance purpose. Debug, monitor and troubleshoot BI solutions.
Be aware of and comply with all corporate and department policies, procedures and standards that apply to your work area.
Ensure that Reporting and Business Analytics strategies and architectures are in regulatory compliance .
Required Skills & Experience:
8+ years' of hands-on experience developing BI and Reporting Solutions.
Experience with business requirements analysis, entity relationship planning, data modeling, database design, reporting structures.
Direct experience in implementing enterprise data management processes, procedures, and support on data monitoring.
Understanding of large scale DB and reporting solution design, Source to Target Mappings, distributed DB design, multi environment structures, logical DB partitioning strategy, data archiving and retention, design and development of reporting semantic layer and view objects and logic
Expert knowledge of MS SQL
Expert hands-on experience with Azure Cloud Data Engineering suite: ADF, Databricks, Azure Data Lake, Spark, Azure SQL and Azure SQL Data Warehouse
Experience with DAX, Tabular and Power BI.
Experience with data processing flowcharting techniques.
Experience developing and maintaining ETL tools and platforms such as SSIS, Azure ADF
Experience with data architecting, large-scale data modeling, and business requirements gathering/analysis.
Strong understanding of BI Reporting & ETL technologies, relational and dimensional data structures, Big Data hands-on experience, principles, and best practices.
Strong familiarity with metadata management and associated processes.
Demonstrated expertise with repository creation, and data and information system life cycle methodologies.
Understanding of Web services (SOAP, XML, REST, JSON, UDDI).
Good knowledge of applicable data privacy practices and laws.
#LI-MG1
*************************************************************************************************************
The above statements describe the general nature and level of work being performed by people assigned to this job. They are not intended to be an exhaustive list of all responsibilities, duties, and skills required. Reasonable accommodations may be made to enable qualified individuals with disabilities to perform the essential job functions. If you need assistance or an accommodation due to disability please contact your recruitment partner.
*************************************************************************************************************	Finastra
20200818106	https://ca.indeed.com/rc/clk?jk=38794e819475c388&fccid=3002307a9e5b4706&vjs=3	Senior Manager, Customer Insights - Analytics	Scotiabank	38794e819475c388	Requisition ID: 88492

Join the Global Community of Scotiabankers to help customers become better off.

Position:
The Senior Manager role in the Advanced Analytics team is designed for individuals with a curiosity for deriving insights out of data and applying them to address business opportunities, in partnership with global teams and business lines driving innovation and advanced analytics across the Enterprise.

The Senior Manager contributes to the development and refinement of business strategies and programs of enterprise banking by providing data-driven insights to the decision makers. This individual is responsible for compiling, aggregating, testing and validating different hypotheses from business lines and analytics teams and converting them into practical business intelligence and recommendations to identify and seize new business opportunities. She/he also manages analytical tools with both high-level dashboard and detailed drilldown analysis functionalities for various groups of internal users.

The Senior Manager will work closely with key internal stakeholders as well as various Strategy, Finance and Analytics departments on all matters related to customer, product and functions portfolio to drive growth aligned to the data and analytics strategic priorities. They will require a combination of business focus, strong analytical and problem-solving skills and programming knowledge to quickly draw insights from large, disparate data sources to support Bank strategies.
WHAT’S IN IT FOR YOU?
Opportunity to make an impact in the transformation of Scotiabank
Exposure to global teams and business lines where analytics techniques are being applied
Hands-on and end to end practical projects which provide an opportunity to gain new knowledge and develop skills
A compensation program with competitive salary, opportunities for annual performance incentives based on performance thresholds, a competitive benefits program and continuing education programs

Requirements & Qualifications:
5+ years of related experience in strategy, data analytics, financial services or related field
Strong knowledge of the banking industry is a must
Post-secondary degree in Business, Computer Science, Engineering or equivalent. MBA will be an asset
Experience cleaning, transforming and visualizing large data sets working with various data formats (e.g. unstructured logs, XML, JSON, flat files)
Hands-on experience with Big Data ecosystem tools (e.g. Hive, Pig, Sqoop and Spark) and strong skills for querying relational databases (e.g. SQL Server, DB2, MySQL, SAS)
Production experience with experimental design, statistical analysis, machine learning and predictive modeling (e.g. cross-sell, upsell, attrition, acquisition and lookalike models)
Experience with common machine Learning libraries in Python, R, Spark
Experience building, using and implementing visualization tools like PowerBI and Tableau
Strong collaboration skills with ability to translate technical knowledge into business value
Exceptional written and communication skills with ability to prepare presentations for Senior Management
Data Engineer and/or Dev Ops experience is a Plus

Accountabilities:
Work in an Agile environment to deploy new solutions
Perform self-initiated data mining and analysis to identify previously unknown trends, cause-effect relationships, and insights to support the development and refining of business strategies and programs
Will report newly-found business insights to Directors with recommendations on how to leverage these insights for the benefit of the Bank and driving growth strategy
Will Interview and work with Business Lines, Analytics and Enterprise Strategy and decision makers to gather BI reporting and analytics requirements; act as a subject matter expert for analytics and BI initiatives
Collaborate with business lines and other stakeholders and identify opportunities to drive business value by leveraging analytics
Develop models with Decision Sciences to validate business hypotheses
Acquire new data needed for new analytics initiatives working with Enterprise Data Management Office
Create and apply model and algorithm testing strategies to measure conduct multi-variate testing and A/B testing to measure effectiveness of models and make ongoing changes
Design, implement, and maintain dynamic dashboards as a tool for Executives and decision makers to get up-to-date information on the situation and changes in the business line and the industry
Will build analytics tools, create training materials, communicate and launch the results to key stakeholders
Continuously improve and manage the analytics / BI platform(s) by collecting and analyzing feedback and making recommendations
Liaise with Analytics, Enterprise Data Management Office, Enterprise Strategy to work on cross-departmental analytics initiatives
Support analytical use-case delivery, as well as customer/financial benefits tracking and communication
Coach a team of data scientist for analytics. This includes providing them guidance on how to work in agile environment, infuse analytics in building solutions and transforming the Bank’s digital capabilities, determine the analytics tools and technology that should be applied in each solution and coaching the team as they work with colleagues from across the organization.
Location(s): Canada : Ontario : Toronto
As Canada's International Bank, we are a diverse and global team. We speak more than 100 languages with backgrounds from more than 120 countries. Our employees are committed to a superior customer experience and use the Bank’s six guiding sales practice principles to ensure they act with honesty and integrity.

At Scotiabank, we value the unique skills and experiences each individual brings to the Bank, and are committed to creating and maintaining an inclusive and accessible environment for everyone. If you require accommodation (including, but not limited to, an accessible interview site, alternate format documents, ASL Interpreter, or Assistive Technology) during the recruitment and selection process, please let our Recruitment team know. If you require technical assistance, please click here. Candidates must apply directly online to be considered for this role. We thank all applicants for their interest in a career at Scotiabank; however, only those candidates who are selected for an interview will be contacted.	2020-08-18T11:04:25.000Z	Toronto, ON
20200818107	SADA	2020-08-18T11:04:26.000Z	Regional Cloud Engineering Manager	Toronto, ON	f9ab2bbaf19601e6	https://ca.indeed.com/rc/clk?jk=f9ab2bbaf19601e6&fccid=b704562e07a2a03f&vjs=3	Join SADA as a Regional Cloud Engineering Manager!

Your Mission

As a Regional Cloud Engineering Manager, you will manage a team of Cloud Engineers and work collaboratively with other architects and engineers to design, prototype and lead the deployment of scalable Google Cloud Platform (GCP) architectures. You will work with engineering teams, customers and sales teams to qualify potential engagements, craft robust architectural proposals, and help craft Statements of Work (SOWs) that engineering teams can successfully execute. You're also hands-on, able to conduct experiments and build functioning prototypes that prove out ideas and build confidence in the solutions you advocate.

You will be a recognized expert within SADA and will develop a reputation with customers as well as the Google Cloud sales and professional services organizations for the quality of your work. You will also lead early-stage opportunity technical qualification calls, as well as lead client-facing technical discussions. You'll enable your team to unblock clients from adopting GCP in accordance with best practices, ensuring client satisfaction via technical excellence.

You will also be the face of SADA sales engineering for the region. This means building relationships with our clients, engineers, sellers, and partners to ensure a proactive approach to serving their needs. You'll be expected to provide guidance to each of these groups as well as to present strategies and overviews to our internal audience. You'll be the senior most technical resource during the sales process for our most important clients and will be expected to present complex technical solutions to our clients' business stakeholders.

In addition, you'll be presenting the work you and your team accomplish via case studies, webinars, speaking roles at conferences, etc.


Pathway to Success

#BeAChangeAgent: You are a rainmaker! You are way out in front of our delivery organization, meeting with the spectrum of corporate and enterprise customers that need our consultative services. You have your finger on the pulse of their technical needs and take pride in helping them solve their real-world problems on GCP.

You will be measured quarterly by a combination of (a) the volume of GCP revenue your team generates and (b) the satisfaction of our clients and sales teams.


Expectations

Required Travel - 30% travel to customer sites, conferences, and other related events. Due to the COVID-19 pandemic, travel has been temporarily restricted.

Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives

Training - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.


Job Requirements

Required Credentials:

Google Professional Cloud Architect Certified and/or Google Professional Data Engineer Certified, or able to complete one of the above within the first 45 days of employment.

Required Qualifications:

Mastery in at least one of the following domain areas as well as general expertise across all:
Infrastructure Modernization: migrating n-tiered workloads from on-prem and other clouds to GCP with near-zero-downtime (includes full spectrum of lift and shift to complete re-platforming scenarios), or building hybrid-cloud solutions based on Anthos and containerization technologies, such as Docker, Kubernetes, and Istio
Application Development: building custom web and mobile applications on top of the GCP stack
Data Engineering: data warehouse modernization (including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines and reporting/analytic tools), OLTP/OLAP data migrations, or backup, restore and disaster recovery solutions.
Experience providing oversight and direction of cloud projects
Experience leading technical design sessions, architecting and documenting technical solutions that are aligned with client business objectives, and identifying gaps between the client's current and desired end states
Experience strategizing, designing, architecting and leading the deployment of scalable solutions on GCP
Experience across multiple cloud platforms: GCP, AWS, Azure
Experience with container engines: Kubernetes, Docker, AWS Elastic Container Service
Experience with automation technologies including Terraform, Google Cloud Deployment Manager, AWS Cloud Formation or Microsoft Azure Automation
Experience working with engineering and sales teams to elicit customer requirements
Ability to communicate across business units and the ability to interface with and communicate complex technical concepts to a broad range of internal and external stakeholders
Time management skills with the ability to manage multiple streams and lead less experienced architects
Experience as a technical consultant or another customer-facing technical role

Useful Qualifications:

Experience managing teams of engineers
Hands-on experience designing and recommending elegant solutions that drive business outcomes
Experience building, designing and migrating complex cloud architectures
Strong aptitude for learning new technologies and techniques with a willingness and capability to skill up the team
A constant desire to learn
Ability to lead an in-depth client meeting/workshop across a broad range of topics including discovery, cloud compliance, and security
Deep understanding of best practices, design patterns, reference and compliance architectures with an uncanny ability to build and recommend these as needed
Knowledge and understanding of industry trends, new technologies and the ability to apply these to customer architectures to drive outcomes
Highly self-motivated and able to work independently as well as leading a team to success.

About SADA

Values: We built our core values on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA's values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.
Make them rave
Be data driven
Be one step ahead
Be a change agent
Do the right thing
Work with the best: SADA has been the largest partner in North America for GCP since 2016 and has been named the 2019 and 2018 Google Cloud Global Partner of the Year. SADA has also been awarded Best Place to Work by Inc. as well as LA Business Journal!

Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match, professional development reimbursement program as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.
20200818108	SADA	https://ca.indeed.com/rc/clk?jk=2fef127f6d4b3e92&fccid=b704562e07a2a03f&vjs=3	Senior Pre-Sales Cloud Engineer	2020-08-18T11:04:26.000Z	Join SADA as a Senior Pre-Sales Cloud Engineer!

Your Mission

As a Senior Pre-Sales Cloud Engineer at SADA, you will work collaboratively with other architects and engineers to design, prototype and lead the deployment of scalable Google Cloud Platform (GCP) architectures. You will work with engineering teams, customers and sales teams to qualify potential engagements, craft robust architectural proposals, and deliver Statements of Work (SOWs) that engineering teams can successfully execute. You're also hands-on, able to conduct experiments and build functioning prototypes that prove out ideas and build confidence in the solutions you advocate.

You will be a recognized expert within SADA and will develop a reputation with customers as well as the Google Cloud sales and professional services organizations for the quality of your work. You will demonstrate repeated delivery of project architectures that other engineers and architects demur to you for lack of expertise. You will also lead early-stage opportunity technical qualification calls, as well as lead client-facing technical discussions.

Pathway to Success

#BeAChangeAgent: You are a rainmaker! You are way out in front of our delivery organization, meeting with the spectrum of corporate and enterprise customers that need our consultative services. You have your finger on the pulse of their technical needs and take pride in helping them solve their real-world problems on GCP.

You will be measured quarterly by a combination of (a) the volume of signed SOWs that you shepherd through the sales funnel, and (b) the level of customer satisfaction measured at the end of each engagement.

As you continue to execute successfully, we will build a customized development plan together that leads you through the solution architecture or management growth tracks.

Expectations

Required Travel - 30% travel to customer sites, conferences, and other related events. Due to the COVID-19 pandemic, travel has been temporarily restricted.
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives
Training - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:
Google Professional Cloud Architect Certified and/or Google Professional Data Engineer Certified, or able to complete one of the above within the first 45 days of employment.
Required Qualifications:
Mastery in at least one of the following domain areas:
Infrastructure Modernization: migrating n-tiered workloads from on-prem and other clouds to GCP with near-zero-downtime (includes full spectrum of lift and shift to complete re-platforming scenarios), or building hybrid-cloud solutions based on Anthos and containerization technologies, such as Docker, Kubernetes, and Istio
Application Development: building custom web and mobile applications on top of the GCP stack
Data Engineering: data warehouse modernization (including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines and reporting/analytic tools), OLTP/OLAP data migrations, or backup, restore and disaster recovery solutions.
Experience providing oversight and direction of cloud projects
Experience leading technical design sessions, architecting and documenting technical solutions that are aligned with client business objectives, and identifying gaps between the client's current and desired end states
Experience strategizing, designing, architecting and leading the deployment of scalable solutions on GCP
Experience across multiple cloud platforms: GCP, AWS, Azure
Experience with container engines: Kubernetes, Docker, AWS Elastic Container Service
Experience with automation technologies including Terraform, Google Cloud Deployment Manager, AWS Cloud Formation or Microsoft Azure Automation
Experience working with engineering and sales teams to elicit customer requirements
Ability to communicate across business units and the ability to interface with and communicate complex technical concepts to a broad range of internal and external stakeholders
Time management skills with the ability to manage multiple streams and lead less experienced architects
Experience as a technical consultant or another customer-facing technical role
Useful Qualifications:
Hands-on experience designing and recommending elegant solutions that drive business outcomes
Experience building, designing and migrating complex cloud architectures
Strong aptitude for learning new technologies and techniques with a willingness and capability to skill up the team
Ability to lead an in-depth client meeting/workshop across a broad range of topics including discovery, cloud compliance, and security
Deep understanding of best practices, design patterns, reference and compliance architectures with an uncanny ability to build and recommend these as needed
Knowledge and understanding of industry trends, new technologies and the ability to apply these to customer architectures to drive outcomes
Highly self-motivated and able to work independently as well as in a team environment

About SADA

Values: We built our core values on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA's values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

Make them rave
Be data driven
Be one step ahead
Be a change agent
Do the right thing

Work with the best: SADA has been the largest partner in North America for GCP since 2016 and has been named the 2019 and 2018 Google Cloud Global Partner of the Year. SADA has also been awarded Best Place to Work by Inc. as well as LA Business Journal!

Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match, professional development reimbursement program as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.	2fef127f6d4b3e92	Toronto, ON
20200818109	SADA	40aaab8c4c4a4804	https://ca.indeed.com/rc/clk?jk=40aaab8c4c4a4804&fccid=b704562e07a2a03f&vjs=3	Technical Account Manager, Google Cloud Platform	2020-08-18T11:04:27.000Z	Toronto, ON	Join SADA as a Technical Account Manager (TAM)!

Our technical competency is what sets SADA apart from other partners. We are transparent in our approach, celebrate our diverse workforce, and strengthen our competencies through training and development. As a TAM at SADA, you will collaborate with some of the most innovative organizations in their pursuit of a cloud-first approach.

This position requires a collaborative stakeholder with experience in infrastructure design, implementation, data engineering, and support. They must be able to architect solutions which include multi-cloud and hybrid-cloud scenarios. (S)he must be consultative, while anticipating potential needs and liabilities around growth and diversification. The ideal candidate is able to understand objectives/watchpoints and apply Google Cloud technology to solve for such outcomes.
If you like the idea of thinking strategically to help clients succeed with Google's cutting-edge solutions, apply now!

Accountabilities:
Provide robust and scalable technology solutions to enable our customers to scale their operations into GCP.
Advise customers on technology standards, methodologies and processes as they relate to infrastructure, application architecture, and data engineering.
Design and develop infrastructure blueprints for the implementation of new solutions that bring customers storage and compute workloads from cloud and non-cloud environments to GCP.
Participate in proof of concept development to assist in defining technology direction for our customers.
Conduct regular touchpoints with clients to review their cloud strategy and provide updates on best practices and new products.
Build solutions which leverage novel approaches to existing business and technology challenges.
Qualifications & Previous Experience

Must have:
Systems Engineering, System Administration, or Systems Architecture experience
Strong working knowledge of cloud offerings and solutions (Google Cloud Platform, Microsoft Azure, Amazon AWS)
Deep understanding of TCP, IP and other network protocols
Familiarity with DNS, DHCP and other network services
Experience administering a variety of Linux distributions
Experience with information security practices and procedures
Strong working knowledge of VMware, Hyper-V, KVM, or other virtual software
Ability to define infrastructure as code using tools like Google Deployment Manager, Terraform, Chef, etc
Strong scripting abilities via BASH, Python, etc
Knowledge of network topology and associated technologies
Mature understanding of DevOps best practices for cloud-native build and release pipelines
Strong technical aptitude and the ability to digest advanced technical topologies and concepts
Preferable:
A Bachelor of Science Degree in Computer Science or equivalent experience
Ability to write architectural design documents or review design documents provided by others
Knowledge of monitoring systems, capacity planning, and performance optimization across a variety of technologies (such as traditional compute, serverless, and data systems)
Knowledge and understanding of industry trends and new technologies and ability to apply trends to architectural needs
Full understanding of compute theory from hardware through serverless abstraction
Experience working with containerization technologies (Kubernetes, Docker, etc)
Certifications (strongly preferred, any of the below):
Google Certified Professional Cloud Architect
Google Certified Professional Data Engineer
AWS Certified Solutions Architect - Professional
AWS Certified DevOps Engineer - Professional


About SADA Systems, Inc

Values: We built our core values on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA's values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

Make them rave
Be data driven
Be one step ahead
Be a change agent
Do the right thing

Work with the best: SADA has been the largest partner in North America for GCP since 2016 and has been named the 2019 and 2018 Google Cloud Global Partner of the Year. SADA has also been awarded Best Place to Work by Inc. as well as LA Business Journal!

Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match, professional development reimbursement program as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.
20200818110	North York, ON	Role: Data Engineer (Intermediate 4-6 years)Structure: Contract (6 months initially)Location: North York, ONHours: Monday - Friday 40 hours (remote during COVID-19)Pay: $50.00 p/h inc.The role: We are looking for Intermediate Data Engineers who have roughly between 4-6 years experience who ideally has experience moving data from Google cloud over to Big query.However, if you are someone who has good experience with ETL and Data Pipeline Design & Data Validation experience and can script in Python and create strong SQL Queries then this will also be a potential match for this role.Ideal Qualifications to be successful in this role: - Degree in Computer Science, Engineering, Information Systems (or equivalent combination of skill and experience)- 4-6 years of experience working with data architecture projects- Excellent SQL coding and experience with a broad array of development tools and platforms including exposure to a big data environment tools/languages, such as SQL, R, SAS, Python, etc.- Experience creating relational database design and data models- Experience with various analytical data platforms and technologies- Experience with using REST API, Cloud (GCP Ideally)- Experience in custom or structured data integration design, implementation, and maintenance- Experience with business intelligence tools such as Qlikview, Tableau and Microstrategy (Nice to have for Data Visualizations)- Previous experience working in retail or eCommerce is highly preferred- Ability to curate data to tell stories and provide business insightsPlease apply with an updated resume and ensure the required skills you are able to speak to for this position are included.For more roles like this please go to www.corgta.com/find-a-job/Job Type: Full-timeSalary: $45.00-$50.00 per hour	Data Engineer (Intermediate 4-6 years) - Contract - Remote during COVID	997673b8effa647f	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Dkh866153otbRJ8nVMuEXnXPd315JS377__3VyD6HnNQkmcaOHMemmy5pLrNV6j0WaVWEWWabnp4PP6-2u77DLfT_FCIGvCW8CQmQW140Z7EVU32Jnwl42FALsxnbc4n5M6FOTfoW2OoTMOcGG21HZGOoXL7fWT8eaT67VlcClh6Pj9F1SyGchle5ICtqwTYSdKyHxhQgtM9EZ1NlBCEXxuNtDMbEcW5pFX6GEmjc0t-3hagShZExVVemnGN-vWpY6qxXaXMeeBg03R2LLKalMTuJXBENypCToPsbYexCk7b4jFGQcwqks93GdavcNLBiP5Q3nARiVzCnptwgtr-Aoh4M2FNRnbh_arLMcMTkXZhbtO0Y0wJ_BYlwgPjWd3Iv7FIs0xrPGkgkbOEQiQNx9vwHssRFAjSTt0LI9M7i13129tM52t0Xp3YTWiKm-r8hLPzLii0BgkfCGJmK58ulJ88gbWujHOF09KEvTaDm5qg==&p=10&fvj=1&vjs=3	2020-08-18T11:04:28.000Z	CorGTA
20200818111	Myticas Consulting	2020-08-18T11:04:29.000Z	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Bt274jo0r_e0cCCqhskoQvohXG3bfDAo8rIdE-Msu3yuVfyF2oiDFKHFOU-vxVulAfe8kPL7riFwoXa1bsfN-szjxO_8qUQWbPsJOnTg0EkutIoK2S4NNLL24JQh9e4DsE9hv907k2_zDbvj0a8WAj5oPSV5J0YhjPRMFQVzYj6ezLKZO7ci7_YeinvrxJc0IeBbkM5HvO72VyZLSnoCZUzlQlFSltftTTwjwMa68RqlUS9XZyHpw8Jk-wR4C08S5XV4Rs0xvDlODsUh3csnqN20-g-mA7tuAeWnfrczHtx4R_Ze7moQ7d2ZvTkJT6QBpf4PMVZebu72o6KXXXxozzwKGitwP51FEZbOWSCabdrif3DnAhE2CB8eht5jx7du_RP5uVzKKORembNORj2QRs__yIs8mTfXIC1MZKPHP421-Sf_a-P-PjhSZ_zBFdk5DZ5KmW3LZgSQ==&p=11&fvj=0&vjs=3	BHJOB15656_15145 - Data Engineer w/ DevOps - Azure Cloud	Toronto, ON	Our recruiting team at Myticas Consulting is looking for an experienced Data Engineer who would be interested in a remote contract opportunity offered within the Ottawa, Toronto and Montreal regions.

Responsibilities:

Defining and reviewing security design requirements for cloud infrastructure and application components.
Evaluating architecture patterns from security perspective.
Building and implementing security controls to enable enforcement of compliance with Cloud Control Objectives, using custom Azure policies and integrated controls in DevOps processes

Qualifications:

Strong Data Engineer w/ DevOps expertise + Azure Cloud Experience
Must know how to code and stand up scripts.
Experience with Data Digestions
Experience writing scripts to automate (infrastructure)
ARM Templating Expertise
Azure Synapse Expertise
Support developing automated DevOps processes and procedures for the following Azure components:
Azure Synapse (Azure DW) & Studio (private preview)
Azure Data Catalog Gen 2 (Babylon – private preview)
Azure Data Lake Storage Gen 2
Azure ML
ML Flow
Azure SQL Analysis Service
Azure Databricks
ADF data pipelines for data loading to AzSQL/Synapse
ADF data pipelines for connecting to on-prem data sources for data

Candidates looking to apply for this role are to send us an updated version of their resume in confidence. Our team will be sure to review all applicants and follow up accordingly at the conclusion of the review process.

Job is also known as: Data Engineer, DevOps Engineer, Azure Cloud Engineer, Azure Engineer, Cloud Engineer

INDMY	a32b7c599156a218
20200818112	2020-08-18T11:04:29.000Z	Our client, a boutique-size consulting firm, prides itself on being at the forefront of innovation in the Big Data space. Founded in 2010, they provide thought leadership and implementation excellence within the ever-growing data and analytics world.
They take pride in having some of the most highly trained and experienced consultants in the industry which translates into optimal value for their clients. They were one of the first companies to provide analytics and data as a service, via the cloud, as early as 2010. They strive to make sure their customers are well-positioned with the best technologies/tools in the industry, constantly evaluating new and existing technology partnerships. Some of the more prominent companies they have partnered with include; Snowflake, DataRobot, MicroStrategy, Informatica, Amazon AWS & Microsoft. They continue to invest in their most valuable resource, their people. They do this through extensive training both on the job and through various educational programs.
Due to growth, they are seeking to hire an Intermediate - Senior Integration Consultant.

Desired Skills and Experience
: University/College degree in Computer Science, Mathematics, Data Science and/or Relevant Degree
: 5+ years hands-on development, configuration, scripting and administration experience with Data Integration platforms. (i.e. Informatica, Talend, DataStage, SSIS)
: BI Experience (MicroStrategy, Looker, Tableau, PowerBI) considered a nice to have
: Extensive theoretical and practical knowledge of data warehousing principles/concepts and practical development experience in all areas of the data warehousing life cycle
: Experience with Data Management, ETL, Cloud Data (AWS), Data Integration
: Knowledge of OLAP-related principles and concepts
: Strong grasp of data modeling techniques and concepts (Normalized/Denormalized, Conceptual/Logical/Physical, Star, Snowflake, Data Vault)
: Strong knowledge and experience with relational databases such as Snowflake, SQL Server, Oracle (Advanced knowledge of reading and writing SQL, Performance analysis and tuning)
: Knowledge and experience with key Big Data technologies (Hive, Presto, Spark, Kafka, NoSQL databases, Semi-structured data access patterns (Json, Parquet, XML, etc.))
: Strong Python scripting skills
: Excellent communication skills
: Great problem-solving skills
: Leadership and good client management skills

Day to Day Activities Would Include
: Conduct relevant customer interviews to determine key business requirements and objectives
: Build appropriate analytical data models based on outcomes of user interviews
: Analyze and profile data systems to build source to target data mappings
: Review ETL performance and conducts performance tuning as required on mappings/workflows or SQL
: Administration and support of data integration infrastructure
: 2nd level on-call support of ETL services as required

You will be responsible for attaining the following goals:
: Attaining a minimum of 1 new accreditation/certification per year
: Spending 80% or more of their time on billable work
: Completing 90% or more of their agile delivery tasks on time
: Demonstrating competency in 1 new relevant technology every year	Copperstone Connect	Toronto, ON	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Ceb7pqR8iGmLuXIZQklF6pqed3xddONNM6Jjumj-ndyGK0kULQUmQ2REqfJ5KAzFVeK_es1WVg0s1sLgKW09UsvVRp0-dSizFQr1oNyK_VIqdjla00VU9GNVvu2x4v9p3hsL7NeF-6hBksbAaSgKazsaYTODGMKQQeVuYcxoOuxvFKuD0HRLD9ZozEtlooKFM2VG90cBIaAqO4coWSGBCyOJ3ZquM-A7XoeDFuHVFoIUhyUCQjVnG3xG62yhTGvEx5CAddPn6juGM18AHVBWS8U9A4SErBI4WmJTGzCHMZT9khWBGB7KYA8DJwS4KKOCF0vE-rKNekMbnLvHjm4Q_zVt_r-pMGu18NZXmBxLL9LxGfACbRh1yX4gDJ2WmqPG2WZ9oOHZJDyUXm1f3yT9CMSpXMZmBcyypmZaQFSbsDQyYai4rxzAfdkt3dhn0XFgDc1BYJpVJSf7RALWiPPHm-1BxbngjszUgIayXjVXUXyzW1ogKJZQfb1GuH2CWTnmij5DKgV1myL9jfe17qY4rzlIRx4QHbNgW3zjL_LJVcGWGqp0TLTGsv1bxYhrYbxDTB2yCfG-qv2pZNzt_gX-32PE8AIgTo9ss-tLfscyQ09b3iOFyP_vYUU5273Kh0czkhktMTQ5ZTBmIjh_N_ZL3dSn1zBb9rbUw_snUJrTIgDA==&p=12&fvj=0&vjs=3	Data Engineer/Integration Consultant	54e0fcc38385e3a2
20200818113	05f7bf2a7458c7fc	2020-08-18T11:04:30.000Z	Job Title- Data Engineer
Location- Downtown, Toronto
Type - Full Time Permanent
Salary - Negotiable + Benefits



Focus on data architecture, best practices, reliability, security, and complianceImprove and extend ETL, data processing, and analytics processesFacility with PowerBI, including creating dashboards and data sourcesDeveloping high complexity, fast performing SELECT queries.Developing T-SQL procedures, functions, triggers, jobs, scripts, etc.Development of Advanced T-SQL such as temporal tables, PIVOTs, recursive table expressions and more.Modeling and implementing Data Mart solution for Power BI analytics
Managing indexes, statistics, query plans alerts, database activity, and overall performance activity.In-depth experience working with relational databases, such as Microsoft SQL Server or PostgreSQLEnthusiasm for applying good data design, testing, documentation, and support practicesExperience building and optimizing data pipelines, architectures, and data setsKnowledge of message queueing, stream processing, and data stores/warehousesWorking knowledge of AWS products related to data engineeringBachelor's degree in Computer Science, Software Engineering or an equivalent
Excellent communication skills - both written and verbal; ability to speak in Spanish is a bonus

To apply please send an email to sheetalk@tes.net	Toronto, ON	https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DQQnUBuQBSuyaIQhpC59TW7hrTbBg8v-nGtzzV8dunbQHh9VTp-k89gJ8q3B9UEXLHcYbAG67d8jpOK4E3ok8olcOhfyyiChWMI3BPvUFdra3ljwgLS_vFsJXlW6jHt2NpQMzd2p9F6BXGm9ND3feUN42yGy4tVXBhZi8rYpSJy9B-h7N6tcMfjMGLfleo1qOYU9Aq4mMhBU1prMQgtLkynqyi8LSUwUYa31DlsABxlFTbZxbOJmZ4Gvbxm75VBYFPMK719YWzIvAr6iCwSfbbGUv2SyhxwnZsc2VDHTCAJ1J6A4riV7Q30AWHPmtFn1pyFAv7OFDbW-1wi04SXfF6985Dns2Bsfbxkwk3RNfa8-n-3KxLaUwOQ9bvHW06oKu9N0RLfvlz9Uf0VNdJpbyndnR56N9tZ74-l_tr-qNzU0mhbU5-ziBN8a78dMN7xm_xl0x6FDvq8q12t4BAfsHAKubteXixFOD9QpeT27bQVevYoSv_XWZuAe81Q0VEMXwhdLSBeQcegd-n0MFw20wK69WBfT47RVVI8VCGJqCxMlEvfGHncwvZaKYX4WuuFgISWAQDwCE5vtch5fjKXafyzNcz0xHbGzed8OXKuUk0c-CVOV6BUJN2fjcFwiZo8WYUUetUD4pLYQAQGyHxLZDYUYQkVU92ZBReR7Oz3bOggDXmRK_xvRwaZdnd_S-cw-CNI4jwD-Mw35Gpk6TRQ8CxE50iGQ96vXUwvBEgyimv6E2J-nOboJmU61MHUgvqCqHO9ZCzB0X6pfqRXUuHfmsHTecjrHHvHQLZzPvYQXry-bbzIBmShgjd7vp7DPsFJJNqT0e9i9tYG5guivxXdq-UyYfbKV037MpaU81_KmPAdg==&p=13&fvj=0&vjs=3	Data Engineer	TES - The Employment Solution